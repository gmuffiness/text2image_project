{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataset_stage1_generator_discriminator_stage2_generator.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"0KV9OIEe9DXn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1592461891686,"user_tz":-540,"elapsed":2948134,"user":{"displayName":"ᄏᄏᄏ","photoUrl":"","userId":"10582150059672851107"}},"outputId":"73f18cb8-37d0-4f9b-9fab-95cf4de2d7c9"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zY6dWr4XdtAe","colab_type":"code","colab":{}},"source":["import os\n","import tarfile\n","\n","fname = '/content/gdrive/My Drive/딥러닝/CUB_200_2011.tgz'  # 압축 파일을 지정해주고   \n","ap = tarfile.open(fname)      # 열어줍니다. \n","\n","ap.extractall('/content/gdrive/My Drive/딥러닝')         # 그리고는 압축을 풀어줍니다. \n","# () 안에는 풀고 싶은 경로를 넣어주면 되요. 비워둘 경우 현재 경로에 압축 풉니다. \n"," \n","ap.close()  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3hZFWeb77ztn","colab_type":"code","colab":{}},"source":["# unzip\n","import zipfile, os, shutil\n","\n","dataset = '/content/gdrive/My Drive/딥러닝/birds.zip'\n","dst_path = '/content/birds' # 코랩 인스턴스의 로컬 스토리지\n","dst_file = os.path.join(dst_path, 'birds.zip')\n","\n","if not os.path.exists(dst_path):\n","  os.makedirs(dst_path)\n","\n","# copy zip file\n","shutil.copy(dataset, dst_file)\n","  \n","with zipfile.ZipFile(dst_file, 'r') as file:\n","  file.extractall(dst_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KIegGWV5aON0","colab_type":"text"},"source":["# Importing Libraries"]},{"cell_type":"code","metadata":{"id":"0TjLKTwFcHZ1","colab_type":"code","colab":{}},"source":["import os\n","import pickle\n","import random\n","import time\n","\n","import PIL\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from PIL import Image\n","from keras import Input, Model\n","from keras import backend as K\n","from keras.callbacks import TensorBoard\n","from keras.layers import Dense, LeakyReLU, BatchNormalization, ReLU, Reshape, UpSampling2D, Conv2D, Activation, \\\n","    concatenate, Flatten, Lambda, Concatenate, ZeroPadding2D\n","from keras.optimizers import Adam\n","from matplotlib import pyplot as plt\n","from keras.layers import add"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7pycIMXgaTtW","colab_type":"text"},"source":["# Loading of Dataset"]},{"cell_type":"code","metadata":{"id":"PHs4UNm2eTWA","colab_type":"code","colab":{}},"source":["#pickle(텍스트가 아닌 객체 자체인 파일 ex.list) 불러오기 위한 함수 (labels저장된 파일)\n","def load_class_ids(class_info_file_path):\n","    with open(class_info_file_path, 'rb') as f:\n","        class_ids = pickle.load(f, encoding='latin1')\n","        return class_ids\n","       \n","#임베딩     \n","def load_embeddings(embeddings_file_path):\n","    with open(embeddings_file_path, 'rb') as f:\n","        embeddings = pickle.load(f, encoding='latin1')\n","        embeddings = np.array(embeddings)\n","        print('embeddings: ', embeddings.shape)\n","    return embeddings\n","\n","#pickle 파일이름 불러오기\n","def load_filenames(filenames_file_path):\n","    with open(filenames_file_path, 'rb') as f:\n","        filenames = pickle.load(f, encoding='latin1')\n","    return filenames\n","\n","#image detection위한 bounding box(바운딩 박스와 일치하는 파일의 dictionary 불러오기) \n","def load_bounding_boxes(dataset_dir):\n","    # Paths\n","    bounding_boxes_path = os.path.join(dataset_dir, 'bounding_boxes.txt')\n","    file_paths_path = os.path.join(dataset_dir, 'images.txt')\n","\n","    # Read bounding_boxes.txt and images.txt file\n","    df_bounding_boxes = pd.read_csv(bounding_boxes_path,\n","                                    delim_whitespace=True, header=None).astype(int) #delim_whitespace : 공백으로 구분된 값 파일 읽기\n","    df_file_names = pd.read_csv(file_paths_path, delim_whitespace=True, header=None)\n","\n","    # Create a list of file names\n","    file_names = df_file_names[1].tolist()\n","\n","    # Create a dictionary of file_names and bounding boxes\n","    filename_boundingbox_dict = {img_file[:-4]: [] for img_file in file_names[:2]}\n","\n","    # Assign a bounding box to the corresponding image\n","    for i in range(0, len(file_names)):\n","        # Get the bounding box\n","        bounding_box = df_bounding_boxes.iloc[i][1:].tolist()\n","        key = file_names[i][:-4]\n","        filename_boundingbox_dict[key] = bounding_box\n","\n","    return filename_boundingbox_dict\n","\n","#image 바운딩 박스로 자르고, 주어진 사이즈로 이미지 resize\n","def get_img(img_path, bbox, image_size):\n","    img = Image.open(img_path).convert('RGB')\n","    width, height = img.size\n","    if bbox is not None:\n","        R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n","        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n","        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n","        y1 = np.maximum(0, center_y - R)\n","        y2 = np.minimum(height, center_y + R)\n","        x1 = np.maximum(0, center_x - R)\n","        x2 = np.minimum(width, center_x + R)\n","        img = img.crop([x1, y1, x2, y2])\n","    img = img.resize(image_size, PIL.Image.BILINEAR)\n","    return img\n"," \n"," #트레이닝하기 위한 데이터셋 로드 : image, labels, 일치하는 embedding return\n","def load_dataset(filenames_file_path, class_info_file_path, cub_dataset_dir, embeddings_file_path, image_size):\n","    filenames = load_filenames(filenames_file_path)\n","    class_ids = load_class_ids(class_info_file_path)\n","    bounding_boxes = load_bounding_boxes(cub_dataset_dir)\n","    all_embeddings = load_embeddings(embeddings_file_path)\n","\n","    X, y, embeddings = [], [], []\n","    print(\"Embeddings shape:\", all_embeddings.shape)\n","\n","    for index, filename in enumerate(filenames):\n","        bounding_box = bounding_boxes[filename]\n","        try:\n","            img_name = '{}/images/{}.jpg'.format(cub_dataset_dir, filename)\n","            img = get_img(img_name, bounding_box, image_size)\n","\n","            all_embeddings1 = all_embeddings[index, :, :]\n","\n","            embedding_ix = random.randint(0, all_embeddings1.shape[0] - 1)#0과 임베딩크기 사이 정수 랜덤반환\n","            embedding = all_embeddings1[embedding_ix, :]\n","\n","            X.append(np.array(img))\n","            y.append(class_ids[index])\n","            embeddings.append(embedding)\n","        except Exception as e:\n","            print(e)\n","\n","    X = np.array(X)\n","    y = np.array(y)\n","    embeddings = np.array(embeddings)\n","    return X, y, embeddings"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SHG799svaZA9","colab_type":"text"},"source":["#  Model Creation"]},{"cell_type":"code","metadata":{"id":"Fnamu5fjeXv8","colab_type":"code","colab":{}},"source":["def generate_c(x):\n","    mean = x[:, :128] #(batch,128)dims의 tensor생성\n","    log_sigma = x[:, 128:]\n","    stddev = K.exp(log_sigma) #from keras import backend as K | exp = exponential\n","    epsilon = K.random_normal(shape=K.constant((mean.shape[1],), dtype='int32'))  # random normal vector with mean=0 and std=1.0\n","    c = stddev * epsilon + mean #text conditioning variable 계산 | 모델 아키텍쳐 그림 중에서 c0 햇 부분\n","    return c\n","\n","#conditioning augmentation: text embedding vector를 conditioning latent variables로 변환  \n","def build_ca_model():\n","    input_layer = Input(shape=(1024,))\n","    x = Dense(256)(input_layer)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    model = Model(inputs=[input_layer], outputs=[x])\n","    return model  # Takes an embedding of shape (1024,) and returns a tensor of shape (256,)\n","  \n","def build_embedding_compressor_model():\n","    input_layer = Input(shape=(1024,))\n","    x = Dense(128)(input_layer)\n","    x = ReLU()(x)\n","    model = Model(inputs=[input_layer], outputs=[x])\n","    return model\n","\n","\n","def build_stage1_generator():\n","    input_layer = Input(shape=(1024,)) #noise variable\n","    x = Dense(256)(input_layer)\n","    mean_logsigma = LeakyReLU(alpha=0.2)(x)\n","\n","    c = Lambda(generate_c)(mean_logsigma)\n","\n","    input_layer2 = Input(shape=(100,))\n","\n","    gen_input = Concatenate(axis=1)([c, input_layer2]) #text-conditioning variable/noise variable\n","\n","    x = Dense(128 * 8 * 4 * 4, use_bias=False)(gen_input)\n","    x = ReLU()(x)\n","\n","    x = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(x) #2d tensor->4d tensor로 변환\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x) #bn 사용 - > bias=False\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = Activation(activation='tanh')(x) #저해상도 이미지 생성할 generator\n","\n","    stage1_gen = Model(inputs=[input_layer, input_layer2], outputs=[x, mean_logsigma])\n","    stage1_gen.summary()\n","\n","    return stage1_gen"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iHvK2VzuebDk","colab_type":"code","colab":{}},"source":["# 이미지 저장\n","def save_rgb_img(img, path):\n","    fig = plt.figure()\n","    ax = fig.add_subplot(1, 1, 1)\n","    ax.imshow(img)\n","    ax.axis(\"off\")\n","    ax.set_title(\"Image\")\n","\n","    plt.savefig(path)\n","    plt.close()\n","\n","#텐서보드에 summary기록   \n","def write_log(callback, name, loss, batch_no):\n","  \"\"\"\n","    summary = tf.Summary()\n","    summary_value = summary.value.add()\n","    summary_value.simple_value = loss\n","    summary_value.tag = name\n","    \n","\n","    callback.writer.add_summary(summary, batch_no)\n","    callback.writer.flush()\n","    \"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X6wFBKBgefMD","colab_type":"code","colab":{}},"source":["def build_stage1_discriminator():\n","    \"\"\"\n","    discriminator는 모델 아키텍쳐 그림에서처럼 2개의 input을 받는다 \n","    1) generator거쳐서 upsampling된 네트워크를 다시 downsampling해서 만든 3차원의 4x4x512의 네트워크\n","    2) 3번에서 concatenate하기 위해 embedding layer를 같은 shape으로 만들어준다. 4x4x128 \n","    3. Concatenate 시키고, 마지막 로짓값(0~1)을 얻기 위해 마지막 모듈(merged_input ~ x2)로 넣어준다.\n","    \"\"\"\n","    input_layer = Input(shape=(64, 64, 3))\n","\n","    x = Conv2D(64, (4, 4),\n","               padding='same', strides=2,\n","               input_shape=(64, 64, 3), use_bias=False)(input_layer)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    input_layer2 = Input(shape=(4, 4, 128))\n","\n","    merged_input = concatenate([x, input_layer2])\n","\n","    x2 = Conv2D(64 * 8, kernel_size=1,\n","                padding=\"same\", strides=1)(merged_input)\n","    x2 = BatchNormalization()(x2)\n","    x2 = LeakyReLU(alpha=0.2)(x2)\n","    x2 = Flatten()(x2)\n","    x2 = Dense(1)(x2)\n","    x2 = Activation('sigmoid')(x2)\n","\n","    stage1_dis = Model(inputs=[input_layer, input_layer2], outputs=[x2])\n","    return stage1_dis"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awq_9gAP_EGV","colab_type":"code","colab":{}},"source":["def build_adversarial_model(gen_model, dis_model):\n","    input_layer = Input(shape=(1024,)) # 1024 = stage1_generator에 들어갈 input size\n","    input_layer2 = Input(shape=(100,)) # 100 = noise 변수의 input size\n","    input_layer3 = Input(shape=(4, 4, 128)) \n","\n","    x, mean_logsigma = gen_model([input_layer, input_layer2]) # stage1_gen 처럼 나온 output\n","\n","    dis_model.trainable = False\n","    valid = dis_model([x, input_layer3]) # stage1_gen 처럼 나온 output과 임베딩 logit값?\n","\n","    model = Model(inputs=[input_layer, input_layer2, input_layer3], outputs=[valid, mean_logsigma])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T3BNs5n__P94","colab_type":"text"},"source":["# Defining Loss"]},{"cell_type":"code","metadata":{"id":"2c7a_PbH_Vwu","colab_type":"code","colab":{}},"source":["def KL_loss(y_true, y_pred):\n","    mean = y_pred[:, :128]\n","    logsigma = y_pred[:, :128]\n","    loss = -logsigma + .5 * (-1 + K.exp(2. * logsigma) + K.square(mean))\n","    loss = K.mean(loss)\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5LDXcjx4-19Z","colab_type":"text"},"source":["# Main File"]},{"cell_type":"code","metadata":{"id":"ss5bkjvYUUWv","colab_type":"code","colab":{}},"source":["data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3JUeRDH4RXfN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592466659447,"user_tz":-540,"elapsed":706959,"user":{"displayName":"ᄏᄏᄏ","photoUrl":"","userId":"10582150059672851107"}},"outputId":"fd9771b1-9987-4e43-b62f-b258e1f0be0c"},"source":["if __name__ == '__main__':\n","\n","    # 폴더 경로 설정\n","    data_dir = \"/content/birds/birds\"\n","    train_dir = data_dir + \"/train\"\n","    test_dir = data_dir + \"/test\"\n","\n","    # 하이퍼 파라미터 설정\n","    image_size = 64\n","    batch_size = 64\n","    z_dim = 100\n","    stage1_generator_lr = 0.0002\n","    stage1_discriminator_lr = 0.0002\n","    stage1_lr_decay_step = 600\n","    epochs = 2\n","    condition_dim = 128\n","\n","    #폴더 경로 설정\n","    embeddings_file_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n","    embeddings_file_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n","\n","    filenames_file_path_train = train_dir + \"/filenames.pickle\"\n","    filenames_file_path_test = test_dir + \"/filenames.pickle\"\n","\n","    class_info_file_path_train = train_dir + \"/class_info.pickle\"\n","    class_info_file_path_test = test_dir + \"/class_info.pickle\"\n","\n","    cub_dataset_dir = \"/content/gdrive/My Drive/딥러닝/CUB_200_2011\"\n","    \n","    # optimizer 정의\n","    dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n","    gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n","\n","    \"\"\"\"\n","    데이터 불러오기\n","    \"\"\"\n","    X_train, y_train, embeddings_train = load_dataset(filenames_file_path=filenames_file_path_train,\n","                                                      class_info_file_path=class_info_file_path_train,\n","                                                      cub_dataset_dir=cub_dataset_dir,\n","                                                      embeddings_file_path=embeddings_file_path_train,\n","                                                      image_size=(64, 64))\n","\n","    X_test, y_test, embeddings_test = load_dataset(filenames_file_path=filenames_file_path_test,\n","                                                   class_info_file_path=class_info_file_path_test,\n","                                                   cub_dataset_dir=cub_dataset_dir,\n","                                                   embeddings_file_path=embeddings_file_path_test,\n","                                                   image_size=(64, 64))\n","\n","    \"\"\"\n","    네트워크 만들고 컴파일\n","    \"\"\"\n","    ca_model = build_ca_model()\n","    ca_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n","\n","    stage1_dis = build_stage1_discriminator()\n","    stage1_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n","\n","    stage1_gen = build_stage1_generator()\n","    stage1_gen.compile(loss=\"mse\", optimizer=gen_optimizer)\n","\n","    embedding_compressor_model = build_embedding_compressor_model()\n","    embedding_compressor_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n","\n","    adversarial_model = build_adversarial_model(gen_model=stage1_gen, dis_model=stage1_dis)\n","    adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1, 2.0],\n","                              optimizer=gen_optimizer, metrics=None)\n","\n","    tensorboard = TensorBoard(log_dir=\"logs/\".format(time.time()))\n","    tensorboard.set_model(stage1_gen)\n","    tensorboard.set_model(stage1_dis)\n","    tensorboard.set_model(ca_model)\n","    tensorboard.set_model(embedding_compressor_model)\n","\n","    # 진짜와 가짜 값들이 담긴 배열 생성\n","    # label smoothing 적용 (discriminator가 부드러운 형태로 확률을 예측하도록 하기 위해 실제 데이터에 대한 target 값을 1보다 약간 작은 값, 이를테면 0.9로 해준다는 것)\n","    # 정확한 건 (https://kangbk0120.github.io/articles/2017-08/tips-from-goodfellow)\n","    real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n","    fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n","\n","    for epoch in range(epochs):\n","        print(\"========================================\")\n","        print(\"Epoch is:\", epoch+1)\n","        print(\"Number of batches\", int(X_train.shape[0] / batch_size))\n","\n","        gen_losses = []\n","        dis_losses = []\n","\n","        # 데이터와 train 모델 불러오기\n","        number_of_batches = int(X_train.shape[0] / batch_size)\n","        for index in range(number_of_batches):\n","            print(\"Batch:{}\".format(index+1))\n","            \n","            \"\"\"\n","            Discriminator network 학습\n","            \"\"\"\n","            # 배치 사이즈 만큼의 데이터를 샘플링한다 Sample a batch of data\n","            z_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n","            image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n","            embedding_batch = embeddings_train[index * batch_size:(index + 1) * batch_size]\n","            image_batch = (image_batch - 127.5) / 127.5\n","\n","            # 가짜 이미지 생성\n","            fake_images, _ = stage1_gen.predict([embedding_batch, z_noise], verbose=3)\n","\n","            # compressed된 embedding 생성\n","            compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n","            compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n","            # 배열을 반복하면서 새로운 축(axis)을 추가하는 np.tile\n","            compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n","\n","            # 진짜 이미지에 진짜라는 라벨(1)을 주고 discriminator 학습시킨 loss \n","            dis_loss_real = stage1_dis.train_on_batch([image_batch, compressed_embedding],\n","                                                      np.reshape(real_labels, (batch_size, 1)))\n","            # generator가 생성한 가짜 이미지에 가짜라는 라벨(0)을 주고 discriminator 학습시킨 loss \n","            dis_loss_fake = stage1_dis.train_on_batch([fake_images, compressed_embedding],\n","                                                      np.reshape(fake_labels, (batch_size, 1)))\n","            # 진짜 이미지에 가짜라는 라벨(0)주고 discriminator 학습시킨 loss \n","            dis_loss_wrong = stage1_dis.train_on_batch([image_batch[:(batch_size - 1)], compressed_embedding[1:]],\n","                                                       np.reshape(fake_labels[1:], (batch_size-1, 1)))\n","            # 총 discriminator의 loss = 0.5 * (loss_real + 0.5 * (loss_wrong + loss_fake)) \n","            d_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong, dis_loss_fake))\n","\n","            print(\"d_loss_real:{}\".format(dis_loss_real))\n","            print(\"d_loss_fake:{}\".format(dis_loss_fake))\n","            print(\"d_loss_wrong:{}\".format(dis_loss_wrong))\n","            print(\"d_loss:{}\".format(d_loss))\n","\n","            \"\"\"\n","            Generator network 학습\n","            \"\"\"\n","            g_loss = adversarial_model.train_on_batch([embedding_batch, z_noise, compressed_embedding],[K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n","            print(\"g_loss:{}\".format(g_loss))\n","\n","            dis_losses.append(d_loss)\n","            gen_losses.append(g_loss)\n","\n","        \"\"\"\n","        각 에폭 끝나고 tensorboard에 loss값들 저장하는 부분\n","        \"\"\"\n","       # write_log(tensorboard, 'discriminator_loss', np.mean(dis_losses), epoch)\n","       # write_log(tensorboard, 'generator_loss', np.mean(gen_losses[0]), epoch)\n","        \n","        # 짝수번째 에폭 학습 끝날때마다 이미지 생성하고 저장하는 부분\n","        if epoch % 2 == 0:\n","            # z_noise2 = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n","            z_noise2 = np.random.normal(0, 1, size=(batch_size, z_dim))\n","            embedding_batch = embeddings_test[0:batch_size]\n","            fake_images, _ = stage1_gen.predict_on_batch([embedding_batch, z_noise2])\n","\n","            # Save images\n","            for i, img in enumerate(fake_images[:10]):\n","                save_rgb_img(img, \"/content/gdrive/My Drive/딥러닝/results/gen_{}_{}.png\".format(epoch, i))\n","\n","    # Save models\n","    stage1_gen.save_weights(\"stage1_gen.h5\")\n","    stage1_dis.save_weights(\"stage1_dis.h5\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["embeddings:  (8855, 10, 1024)\n","Embeddings shape: (8855, 10, 1024)\n","embeddings:  (2933, 10, 1024)\n","Embeddings shape: (2933, 10, 1024)\n","Model: \"model_13\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_22 (InputLayer)           (None, 1024)         0                                            \n","__________________________________________________________________________________________________\n","dense_13 (Dense)                (None, 256)          262400      input_22[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_21 (LeakyReLU)      (None, 256)          0           dense_13[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 128)          0           leaky_re_lu_21[0][0]             \n","__________________________________________________________________________________________________\n","input_23 (InputLayer)           (None, 100)          0                                            \n","__________________________________________________________________________________________________\n","concatenate_6 (Concatenate)     (None, 228)          0           lambda_3[0][0]                   \n","                                                                 input_23[0][0]                   \n","__________________________________________________________________________________________________\n","dense_14 (Dense)                (None, 16384)        3735552     concatenate_6[0][0]              \n","__________________________________________________________________________________________________\n","re_lu_13 (ReLU)                 (None, 16384)        0           dense_14[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_3 (Reshape)             (None, 4, 4, 1024)   0           re_lu_13[0][0]                   \n","__________________________________________________________________________________________________\n","up_sampling2d_9 (UpSampling2D)  (None, 8, 8, 1024)   0           reshape_3[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 8, 8, 512)    4718592     up_sampling2d_9[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_21 (BatchNo (None, 8, 8, 512)    2048        conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","re_lu_14 (ReLU)                 (None, 8, 8, 512)    0           batch_normalization_21[0][0]     \n","__________________________________________________________________________________________________\n","up_sampling2d_10 (UpSampling2D) (None, 16, 16, 512)  0           re_lu_14[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 16, 16, 256)  1179648     up_sampling2d_10[0][0]           \n","__________________________________________________________________________________________________\n","batch_normalization_22 (BatchNo (None, 16, 16, 256)  1024        conv2d_27[0][0]                  \n","__________________________________________________________________________________________________\n","re_lu_15 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_22[0][0]     \n","__________________________________________________________________________________________________\n","up_sampling2d_11 (UpSampling2D) (None, 32, 32, 256)  0           re_lu_15[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 32, 32, 128)  294912      up_sampling2d_11[0][0]           \n","__________________________________________________________________________________________________\n","batch_normalization_23 (BatchNo (None, 32, 32, 128)  512         conv2d_28[0][0]                  \n","__________________________________________________________________________________________________\n","re_lu_16 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_23[0][0]     \n","__________________________________________________________________________________________________\n","up_sampling2d_12 (UpSampling2D) (None, 64, 64, 128)  0           re_lu_16[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 64, 64, 64)   73728       up_sampling2d_12[0][0]           \n","__________________________________________________________________________________________________\n","batch_normalization_24 (BatchNo (None, 64, 64, 64)   256         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","re_lu_17 (ReLU)                 (None, 64, 64, 64)   0           batch_normalization_24[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 64, 64, 3)    1728        re_lu_17[0][0]                   \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 64, 64, 3)    0           conv2d_30[0][0]                  \n","==================================================================================================\n","Total params: 10,270,400\n","Trainable params: 10,268,480\n","Non-trainable params: 1,920\n","__________________________________________________________________________________________________\n","========================================\n","Epoch is: 0\n","Number of batches 138\n","Batch:1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":["d_loss_real:0.7221051454544067\n","d_loss_fake:2.611485004425049\n","d_loss_wrong:5.176578521728516\n","d_loss:2.3080684542655945\n","g_loss:[2.1057172, 2.0707545, 0.017481381]\n","Batch:2\n","d_loss_real:1.1742351055145264\n","d_loss_fake:0.0005834209150634706\n","d_loss_wrong:1.2217262983322144\n","d_loss:0.8926949799060822\n","g_loss:[0.74918437, 0.70775646, 0.02071397]\n","Batch:3\n","d_loss_real:2.859447479248047\n","d_loss_fake:0.0033343390095978975\n","d_loss_wrong:2.280238151550293\n","d_loss:2.0006168484687805\n","g_loss:[0.5815766, 0.54811263, 0.016731992]\n","Batch:4\n","d_loss_real:1.7981653213500977\n","d_loss_fake:0.02203897386789322\n","d_loss_wrong:2.9273548126220703\n","d_loss:1.636431097984314\n","g_loss:[0.37404943, 0.33737567, 0.018336885]\n","Batch:5\n","d_loss_real:1.793898344039917\n","d_loss_fake:0.02958407998085022\n","d_loss_wrong:2.481085777282715\n","d_loss:1.5246166586875916\n","g_loss:[0.4652051, 0.43085602, 0.017174544]\n","Batch:6\n","d_loss_real:1.8009436130523682\n","d_loss_fake:0.010700283572077751\n","d_loss_wrong:1.8966797590255737\n","d_loss:1.377316802740097\n","g_loss:[0.3737816, 0.33011466, 0.021833466]\n","Batch:7\n","d_loss_real:2.3108837604522705\n","d_loss_fake:0.007631085347384214\n","d_loss_wrong:0.9976966977119446\n","d_loss:1.4067738354206085\n","g_loss:[0.36474922, 0.33619952, 0.014274844]\n","Batch:8\n","d_loss_real:2.0290491580963135\n","d_loss_fake:0.007216807454824448\n","d_loss_wrong:0.9168365597724915\n","d_loss:1.2455379217863083\n","g_loss:[0.38310823, 0.33271006, 0.02519909]\n","Batch:9\n","d_loss_real:1.519161581993103\n","d_loss_fake:0.01695774681866169\n","d_loss_wrong:1.4843392372131348\n","d_loss:1.1349050402641296\n","g_loss:[0.39523983, 0.32795942, 0.033640202]\n","Batch:10\n","d_loss_real:1.2787609100341797\n","d_loss_fake:0.016293860971927643\n","d_loss_wrong:1.0589238405227661\n","d_loss:0.9081848859786987\n","g_loss:[0.3736098, 0.33998698, 0.016811414]\n","Batch:11\n","d_loss_real:1.3736988306045532\n","d_loss_fake:0.006683544255793095\n","d_loss_wrong:1.091261625289917\n","d_loss:0.9613357186317444\n","g_loss:[0.36830705, 0.33643913, 0.01593396]\n","Batch:12\n","d_loss_real:1.5181400775909424\n","d_loss_fake:0.012467194348573685\n","d_loss_wrong:0.9013494849205017\n","d_loss:0.9875242114067078\n","g_loss:[0.3696556, 0.3402721, 0.014691749]\n","Batch:13\n","d_loss_real:1.4022581577301025\n","d_loss_fake:0.012007280252873898\n","d_loss_wrong:0.9841474890708923\n","d_loss:0.9501677751541138\n","g_loss:[0.3594904, 0.33518505, 0.012152678]\n","Batch:14\n","d_loss_real:1.2659128904342651\n","d_loss_fake:0.018689963966608047\n","d_loss_wrong:1.0050393342971802\n","d_loss:0.8888887763023376\n","g_loss:[0.34893483, 0.32839274, 0.010271042]\n","Batch:15\n","d_loss_real:1.233752965927124\n","d_loss_fake:0.01253877766430378\n","d_loss_wrong:1.2356582880020142\n","d_loss:0.9289257526397705\n","g_loss:[0.36285144, 0.33139104, 0.015730195]\n","Batch:16\n","d_loss_real:1.1342370510101318\n","d_loss_fake:0.007271491922438145\n","d_loss_wrong:1.036194086074829\n","d_loss:0.8279849290847778\n","g_loss:[0.36568254, 0.3407464, 0.012468071]\n","Batch:17\n","d_loss_real:1.259971261024475\n","d_loss_fake:0.011356770992279053\n","d_loss_wrong:1.4678590297698975\n","d_loss:0.9997895956039429\n","g_loss:[0.35239288, 0.33391196, 0.00924047]\n","Batch:18\n","d_loss_real:1.3269678354263306\n","d_loss_fake:0.010247686877846718\n","d_loss_wrong:0.7038386464118958\n","d_loss:0.8420055061578751\n","g_loss:[0.34174687, 0.32659194, 0.007577471]\n","Batch:19\n","d_loss_real:1.3428044319152832\n","d_loss_fake:0.008793830871582031\n","d_loss_wrong:0.8896583914756775\n","d_loss:0.8960152715444565\n","g_loss:[0.34899986, 0.33193952, 0.008530174]\n","Batch:20\n","d_loss_real:1.1665278673171997\n","d_loss_fake:0.011445923708379269\n","d_loss_wrong:1.0205543041229248\n","d_loss:0.8412639796733856\n","g_loss:[0.34384924, 0.32772538, 0.008061938]\n","Batch:21\n","d_loss_real:1.27231764793396\n","d_loss_fake:0.006028527393937111\n","d_loss_wrong:0.7361515760421753\n","d_loss:0.8217038512229919\n","g_loss:[0.36580265, 0.34719032, 0.009306159]\n","Batch:22\n","d_loss_real:1.1033159494400024\n","d_loss_fake:0.00769024807959795\n","d_loss_wrong:0.9539188146591187\n","d_loss:0.7920602411031723\n","g_loss:[0.34358332, 0.32716396, 0.00820968]\n","Batch:23\n","d_loss_real:1.1204490661621094\n","d_loss_fake:0.005418369546532631\n","d_loss_wrong:0.8197043538093567\n","d_loss:0.7665052115917206\n","g_loss:[0.3436564, 0.32757667, 0.008039867]\n","Batch:24\n","d_loss_real:1.057456612586975\n","d_loss_fake:0.007907536812126637\n","d_loss_wrong:0.8758255243301392\n","d_loss:0.7496615648269653\n","g_loss:[0.35393637, 0.33273518, 0.010600604]\n","Batch:25\n","d_loss_real:1.1132732629776\n","d_loss_fake:0.006276573985815048\n","d_loss_wrong:0.9164637327194214\n","d_loss:0.7873217016458511\n","g_loss:[0.3455948, 0.32661146, 0.009491661]\n","Batch:26\n","d_loss_real:1.1355726718902588\n","d_loss_fake:0.004132868722081184\n","d_loss_wrong:0.9202401638031006\n","d_loss:0.7988795936107635\n","g_loss:[0.36267397, 0.3444058, 0.009134083]\n","Batch:27\n","d_loss_real:1.1264132261276245\n","d_loss_fake:0.007767681498080492\n","d_loss_wrong:1.0308678150177002\n","d_loss:0.8228654861450195\n","g_loss:[0.35011396, 0.33192173, 0.009096109]\n","Batch:28\n","d_loss_real:1.021571159362793\n","d_loss_fake:0.005829460918903351\n","d_loss_wrong:0.8575162887573242\n","d_loss:0.7266220152378082\n","g_loss:[0.35087678, 0.32631102, 0.012282885]\n","Batch:29\n","d_loss_real:1.0522936582565308\n","d_loss_fake:0.005840415600687265\n","d_loss_wrong:0.75369793176651\n","d_loss:0.7160314172506332\n","g_loss:[0.34792536, 0.32724938, 0.010337989]\n","Batch:30\n","d_loss_real:1.1316158771514893\n","d_loss_fake:0.006234345026314259\n","d_loss_wrong:1.064692735671997\n","d_loss:0.8335396945476532\n","g_loss:[0.34187788, 0.3281437, 0.0068670968]\n","Batch:31\n","d_loss_real:0.9716513156890869\n","d_loss_fake:0.003968693315982819\n","d_loss_wrong:0.7803945541381836\n","d_loss:0.6819164752960205\n","g_loss:[0.33974913, 0.32621875, 0.006765194]\n","Batch:32\n","d_loss_real:1.0371813774108887\n","d_loss_fake:0.003461658488959074\n","d_loss_wrong:0.7243129014968872\n","d_loss:0.7005343288183212\n","g_loss:[0.34046307, 0.3265652, 0.006948933]\n","Batch:33\n","d_loss_real:1.0545430183410645\n","d_loss_fake:0.006552158854901791\n","d_loss_wrong:0.7594864964485168\n","d_loss:0.7187811732292175\n","g_loss:[0.34544376, 0.32590118, 0.009771281]\n","Batch:34\n","d_loss_real:1.0165886878967285\n","d_loss_fake:0.004822686314582825\n","d_loss_wrong:0.769057035446167\n","d_loss:0.7017642706632614\n","g_loss:[0.3423221, 0.32700306, 0.0076595256]\n","Batch:35\n","d_loss_real:0.9508652687072754\n","d_loss_fake:0.004493703134357929\n","d_loss_wrong:0.8432593941688538\n","d_loss:0.6873709112405777\n","g_loss:[0.33730683, 0.3263961, 0.0054553594]\n","Batch:36\n","d_loss_real:1.0162322521209717\n","d_loss_fake:0.005688583478331566\n","d_loss_wrong:0.7443788647651672\n","d_loss:0.6956329941749573\n","g_loss:[0.34309912, 0.32616094, 0.008469096]\n","Batch:37\n","d_loss_real:1.0248668193817139\n","d_loss_fake:0.0064794281497597694\n","d_loss_wrong:0.9103991389274597\n","d_loss:0.7416530549526215\n","g_loss:[0.34848204, 0.3308144, 0.008833829]\n","Batch:38\n","d_loss_real:1.0117665529251099\n","d_loss_fake:0.004185528494417667\n","d_loss_wrong:0.844721257686615\n","d_loss:0.718109980225563\n","g_loss:[0.34767967, 0.33481574, 0.006431964]\n","Batch:39\n","d_loss_real:1.0433536767959595\n","d_loss_fake:0.003910803701728582\n","d_loss_wrong:0.7624022364616394\n","d_loss:0.7132550925016403\n","g_loss:[0.34260148, 0.32761773, 0.007491879]\n","Batch:40\n","d_loss_real:1.054647445678711\n","d_loss_fake:0.00416776817291975\n","d_loss_wrong:1.0007719993591309\n","d_loss:0.7785586714744568\n","g_loss:[0.35178718, 0.33562422, 0.00808148]\n","Batch:41\n","d_loss_real:0.9597516059875488\n","d_loss_fake:0.004240857437252998\n","d_loss_wrong:0.7081454992294312\n","d_loss:0.6579723954200745\n","g_loss:[0.34062266, 0.3274045, 0.006609087]\n","Batch:42\n","d_loss_real:1.0795061588287354\n","d_loss_fake:0.005220120772719383\n","d_loss_wrong:0.7060992121696472\n","d_loss:0.7175829112529755\n","g_loss:[0.35406986, 0.34197325, 0.0060483064]\n","Batch:43\n","d_loss_real:0.8841414451599121\n","d_loss_fake:0.0042595695704221725\n","d_loss_wrong:0.7642526626586914\n","d_loss:0.634198784828186\n","g_loss:[0.3400636, 0.3259662, 0.007048693]\n","Batch:44\n","d_loss_real:0.9459071159362793\n","d_loss_fake:0.003984675742685795\n","d_loss_wrong:0.7522674798965454\n","d_loss:0.6620166003704071\n","g_loss:[0.3458715, 0.3333987, 0.006236397]\n","Batch:45\n","d_loss_real:0.9425712823867798\n","d_loss_fake:0.0037101195193827152\n","d_loss_wrong:0.7115980386734009\n","d_loss:0.6501126736402512\n","g_loss:[0.340828, 0.3270958, 0.006866104]\n","Batch:46\n","d_loss_real:0.9570907354354858\n","d_loss_fake:0.0033776136115193367\n","d_loss_wrong:0.7248108983039856\n","d_loss:0.6605924963951111\n","g_loss:[0.3407673, 0.32787615, 0.006445571]\n","Batch:47\n","d_loss_real:0.9354610443115234\n","d_loss_fake:0.0028661363758146763\n","d_loss_wrong:0.7679976224899292\n","d_loss:0.6604464650154114\n","g_loss:[0.33815843, 0.32694706, 0.0056056865]\n","Batch:48\n","d_loss_real:0.9870864152908325\n","d_loss_fake:0.003726133145391941\n","d_loss_wrong:0.7090798616409302\n","d_loss:0.671744704246521\n","g_loss:[0.3352205, 0.3264532, 0.0043836343]\n","Batch:49\n","d_loss_real:0.9143853187561035\n","d_loss_fake:0.003257239004597068\n","d_loss_wrong:0.7368943691253662\n","d_loss:0.6422305554151535\n","g_loss:[0.34067833, 0.32714468, 0.0067668217]\n","Batch:50\n","d_loss_real:0.9679746627807617\n","d_loss_fake:0.0028842103201895952\n","d_loss_wrong:0.6936497688293457\n","d_loss:0.6581208258867264\n","g_loss:[0.3346484, 0.32759142, 0.003528496]\n","Batch:51\n","d_loss_real:0.9409098625183105\n","d_loss_fake:0.0042937700636684895\n","d_loss_wrong:0.7400201559066772\n","d_loss:0.656533420085907\n","g_loss:[0.3369974, 0.3276809, 0.0046582525]\n","Batch:52\n","d_loss_real:0.9458266496658325\n","d_loss_fake:0.004187550395727158\n","d_loss_wrong:0.7594611048698425\n","d_loss:0.6638254821300507\n","g_loss:[0.33558223, 0.3294788, 0.003051706]\n","Batch:53\n","d_loss_real:0.9182342290878296\n","d_loss_fake:0.0029161374550312757\n","d_loss_wrong:0.7544158697128296\n","d_loss:0.6484501212835312\n","g_loss:[0.33629778, 0.32841718, 0.0039403015]\n","Batch:54\n","d_loss_real:0.9381707906723022\n","d_loss_fake:0.0028112733270972967\n","d_loss_wrong:0.7193701863288879\n","d_loss:0.6496307551860809\n","g_loss:[0.33493626, 0.32695976, 0.003988253]\n","Batch:55\n","d_loss_real:0.9197487831115723\n","d_loss_fake:0.0035719622392207384\n","d_loss_wrong:0.7300668358802795\n","d_loss:0.6432840973138809\n","g_loss:[0.33449376, 0.32640743, 0.0040431544]\n","Batch:56\n","d_loss_real:0.9602437019348145\n","d_loss_fake:0.0036492806393653154\n","d_loss_wrong:0.7595449686050415\n","d_loss:0.6709204167127609\n","g_loss:[0.33646318, 0.32618982, 0.0051366836]\n","Batch:57\n","d_loss_real:0.9518336057662964\n","d_loss_fake:0.002649662783369422\n","d_loss_wrong:0.734634280204773\n","d_loss:0.6602377891540527\n","g_loss:[0.33527273, 0.32659444, 0.0043391413]\n","Batch:58\n","d_loss_real:0.9092657566070557\n","d_loss_fake:0.0034296696539968252\n","d_loss_wrong:0.7533826231956482\n","d_loss:0.6438359469175339\n","g_loss:[0.3350269, 0.32587016, 0.0045783687]\n","Batch:59\n","d_loss_real:0.9098004102706909\n","d_loss_fake:0.0037027080543339252\n","d_loss_wrong:0.7535008788108826\n","d_loss:0.6442010998725891\n","g_loss:[0.33286226, 0.32582802, 0.0035171253]\n","Batch:60\n","d_loss_real:0.9383788704872131\n","d_loss_fake:0.003366044256836176\n","d_loss_wrong:0.7486717104911804\n","d_loss:0.6571988761425018\n","g_loss:[0.334632, 0.32647383, 0.0040790886]\n","Batch:61\n","d_loss_real:0.9738514423370361\n","d_loss_fake:0.0028009754605591297\n","d_loss_wrong:0.7676376700401306\n","d_loss:0.6795353889465332\n","g_loss:[0.33240435, 0.3257709, 0.0033167112]\n","Batch:62\n","d_loss_real:0.9643926620483398\n","d_loss_fake:0.002644773107022047\n","d_loss_wrong:0.7201621532440186\n","d_loss:0.662898063659668\n","g_loss:[0.33172733, 0.3259859, 0.002870706]\n","Batch:63\n","d_loss_real:0.9842865467071533\n","d_loss_fake:0.00376666197553277\n","d_loss_wrong:0.7222369313240051\n","d_loss:0.673644170165062\n","g_loss:[0.33321604, 0.32763383, 0.0027911132]\n","Batch:64\n","d_loss_real:0.9017110466957092\n","d_loss_fake:0.002886692062020302\n","d_loss_wrong:0.7508634924888611\n","d_loss:0.6392930746078491\n","g_loss:[0.33220974, 0.32594895, 0.0031303954]\n","Batch:65\n","d_loss_real:0.9629716277122498\n","d_loss_fake:0.0034655057825148106\n","d_loss_wrong:0.7331558465957642\n","d_loss:0.6656411588191986\n","g_loss:[0.33335865, 0.32592183, 0.0037184134]\n","Batch:66\n","d_loss_real:0.8849660158157349\n","d_loss_fake:0.004338850267231464\n","d_loss_wrong:0.7630268335342407\n","d_loss:0.6343244314193726\n","g_loss:[0.33067614, 0.3259781, 0.0023490225]\n","Batch:67\n","d_loss_real:0.9516509771347046\n","d_loss_fake:0.0025528273545205593\n","d_loss_wrong:0.7063474059104919\n","d_loss:0.6530505418777466\n","g_loss:[0.33362415, 0.32805687, 0.0027836377]\n","Batch:68\n","d_loss_real:0.9206631183624268\n","d_loss_fake:0.0022083125077188015\n","d_loss_wrong:0.7828758358955383\n","d_loss:0.6566025912761688\n","g_loss:[0.33396918, 0.3275449, 0.0032121423]\n","Batch:69\n","d_loss_real:0.9732417464256287\n","d_loss_fake:0.0020313221029937267\n","d_loss_wrong:1.0169143676757812\n","d_loss:0.741357296705246\n","g_loss:[0.33587354, 0.3311321, 0.0023707112]\n","Batch:70\n","d_loss_real:0.9035274386405945\n","d_loss_fake:0.0023556111846119165\n","d_loss_wrong:0.7207497954368591\n","d_loss:0.6325400769710541\n","g_loss:[0.349464, 0.34463918, 0.002412412]\n","Batch:71\n","d_loss_real:0.9806996583938599\n","d_loss_fake:0.002652427414432168\n","d_loss_wrong:0.6409274935722351\n","d_loss:0.6512448042631149\n","g_loss:[0.33299378, 0.3279395, 0.0025271324]\n","Batch:72\n","d_loss_real:0.9600687026977539\n","d_loss_fake:0.0015617392491549253\n","d_loss_wrong:0.6789414286613464\n","d_loss:0.6501601487398148\n","g_loss:[0.3358929, 0.331677, 0.0021079553]\n","Batch:73\n","d_loss_real:0.9122443199157715\n","d_loss_fake:0.0021842466667294502\n","d_loss_wrong:0.7118757963180542\n","d_loss:0.634637176990509\n","g_loss:[0.33265483, 0.32784402, 0.0024054071]\n","Batch:74\n","d_loss_real:0.890022873878479\n","d_loss_fake:0.0024026064202189445\n","d_loss_wrong:0.7527313828468323\n","d_loss:0.6337949335575104\n","g_loss:[0.33091456, 0.32604873, 0.0024329086]\n","Batch:75\n","d_loss_real:0.9345706701278687\n","d_loss_fake:0.0027552207466214895\n","d_loss_wrong:0.7197151184082031\n","d_loss:0.6479029208421707\n","g_loss:[0.3347091, 0.32897753, 0.002865796]\n","Batch:76\n","d_loss_real:0.9300057291984558\n","d_loss_fake:0.0024140288587659597\n","d_loss_wrong:0.6929855346679688\n","d_loss:0.6388527601957321\n","g_loss:[0.33081642, 0.32623369, 0.0022913592]\n","Batch:77\n","d_loss_real:0.943211555480957\n","d_loss_fake:0.0030054557137191296\n","d_loss_wrong:0.7089731097221375\n","d_loss:0.6496004164218903\n","g_loss:[0.33064803, 0.32681954, 0.0019142518]\n","Batch:78\n","d_loss_real:0.8968733549118042\n","d_loss_fake:0.0018437104299664497\n","d_loss_wrong:0.7288445234298706\n","d_loss:0.6311087310314178\n","g_loss:[0.33021766, 0.32601494, 0.002101363]\n","Batch:79\n","d_loss_real:0.8879852294921875\n","d_loss_fake:0.0020690150558948517\n","d_loss_wrong:0.7008575797080994\n","d_loss:0.6197242587804794\n","g_loss:[0.3327523, 0.32825622, 0.0022480304]\n","Batch:80\n","d_loss_real:0.8963153958320618\n","d_loss_fake:0.002213127911090851\n","d_loss_wrong:0.6947314143180847\n","d_loss:0.6223938316106796\n","g_loss:[0.33124584, 0.32675588, 0.0022449733]\n","Batch:81\n","d_loss_real:0.8942506909370422\n","d_loss_fake:0.00254993699491024\n","d_loss_wrong:0.6769436597824097\n","d_loss:0.6169987469911575\n","g_loss:[0.3325726, 0.32794023, 0.002316192]\n","Batch:82\n","d_loss_real:0.953598141670227\n","d_loss_fake:0.002971080131828785\n","d_loss_wrong:0.727464497089386\n","d_loss:0.6594079583883286\n","g_loss:[0.33323234, 0.32932803, 0.0019521498]\n","Batch:83\n","d_loss_real:0.8555034399032593\n","d_loss_fake:0.0026769761461764574\n","d_loss_wrong:0.6948603987693787\n","d_loss:0.6021360605955124\n","g_loss:[0.33007938, 0.32612938, 0.001975002]\n","Batch:84\n","d_loss_real:0.9134812355041504\n","d_loss_fake:0.0024228976108133793\n","d_loss_wrong:0.7050102949142456\n","d_loss:0.6335989087820053\n","g_loss:[0.3308324, 0.32636762, 0.0022323823]\n","Batch:85\n","d_loss_real:0.9003994464874268\n","d_loss_fake:0.0022379630245268345\n","d_loss_wrong:0.7053216099739075\n","d_loss:0.6270896196365356\n","g_loss:[0.32943735, 0.32562798, 0.0019046796]\n","Batch:86\n","d_loss_real:0.8828955888748169\n","d_loss_fake:0.0027004890143871307\n","d_loss_wrong:0.6900531649589539\n","d_loss:0.6146362125873566\n","g_loss:[0.32928526, 0.32569605, 0.0017946007]\n","Batch:87\n","d_loss_real:0.8895703554153442\n","d_loss_fake:0.0017944083083420992\n","d_loss_wrong:0.6945369839668274\n","d_loss:0.6188680231571198\n","g_loss:[0.33259365, 0.32736933, 0.0026121552]\n","Batch:88\n","d_loss_real:0.9270245432853699\n","d_loss_fake:0.0019082583021372557\n","d_loss_wrong:0.7063687443733215\n","d_loss:0.6405815184116364\n","g_loss:[0.3314435, 0.3265867, 0.002428392]\n","Batch:89\n","d_loss_real:0.9078102111816406\n","d_loss_fake:0.001326654921285808\n","d_loss_wrong:0.8232889175415039\n","d_loss:0.660059005022049\n","g_loss:[0.33679485, 0.33021858, 0.0032881277]\n","Batch:90\n","d_loss_real:0.8845353722572327\n","d_loss_fake:0.0017297343583777547\n","d_loss_wrong:0.7477865219116211\n","d_loss:0.6296467483043671\n","g_loss:[0.33112285, 0.32663494, 0.0022439556]\n","Batch:91\n","d_loss_real:0.9218908548355103\n","d_loss_fake:0.001507583656348288\n","d_loss_wrong:0.7100632786750793\n","d_loss:0.638838142156601\n","g_loss:[0.33121485, 0.3272938, 0.0019605106]\n","Batch:92\n","d_loss_real:0.9314711093902588\n","d_loss_fake:0.0024817169178277254\n","d_loss_wrong:0.8576681017875671\n","d_loss:0.6807730048894882\n","g_loss:[0.33162293, 0.32555398, 0.0030344778]\n","Batch:93\n","d_loss_real:0.9398281574249268\n","d_loss_fake:0.002700126264244318\n","d_loss_wrong:0.6695596575737\n","d_loss:0.6379790306091309\n","g_loss:[0.33955508, 0.33301717, 0.0032689502]\n","Batch:94\n","d_loss_real:0.9281951189041138\n","d_loss_fake:0.0024177059531211853\n","d_loss_wrong:0.749792754650116\n","d_loss:0.6521501690149307\n","g_loss:[0.3374025, 0.32924902, 0.0040767323]\n","Batch:95\n","d_loss_real:0.9595133662223816\n","d_loss_fake:0.0019294683588668704\n","d_loss_wrong:0.6539762020111084\n","d_loss:0.6437330991029739\n","g_loss:[0.3475679, 0.34165263, 0.0029576344]\n","Batch:96\n","d_loss_real:0.8715329170227051\n","d_loss_fake:0.003155176527798176\n","d_loss_wrong:0.6809269785881042\n","d_loss:0.6067869961261749\n","g_loss:[0.33358744, 0.32598823, 0.0037996066]\n","Batch:97\n","d_loss_real:0.8830353617668152\n","d_loss_fake:0.0027925362810492516\n","d_loss_wrong:0.6891167759895325\n","d_loss:0.6144950091838837\n","g_loss:[0.33297965, 0.32607678, 0.0034514386]\n","Batch:98\n","d_loss_real:0.8986167907714844\n","d_loss_fake:0.0016969742719084024\n","d_loss_wrong:0.7164730429649353\n","d_loss:0.628850907087326\n","g_loss:[0.33166313, 0.3262369, 0.0027131212]\n","Batch:99\n","d_loss_real:0.9092899560928345\n","d_loss_fake:0.0012142841005697846\n","d_loss_wrong:0.6841784119606018\n","d_loss:0.6259931474924088\n","g_loss:[0.33104032, 0.3257493, 0.0026455084]\n","Batch:100\n","d_loss_real:0.872323751449585\n","d_loss_fake:0.0013835085555911064\n","d_loss_wrong:0.6950826048851013\n","d_loss:0.6102783977985382\n","g_loss:[0.33063194, 0.3260711, 0.0022804125]\n","Batch:101\n","d_loss_real:0.8927342891693115\n","d_loss_fake:0.0019780434668064117\n","d_loss_wrong:0.7128772139549255\n","d_loss:0.6250809580087662\n","g_loss:[0.32940656, 0.32624808, 0.001579241]\n","Batch:102\n","d_loss_real:0.9163776636123657\n","d_loss_fake:0.0017689738888293505\n","d_loss_wrong:0.7313336730003357\n","d_loss:0.6414644867181778\n","g_loss:[0.32933623, 0.32585847, 0.0017388734]\n","Batch:103\n","d_loss_real:0.9013675451278687\n","d_loss_fake:0.0017384439706802368\n","d_loss_wrong:0.6952061653137207\n","d_loss:0.6249199211597443\n","g_loss:[0.33012047, 0.32572734, 0.0021965688]\n","Batch:104\n","d_loss_real:0.8869283199310303\n","d_loss_fake:0.0018203412182629108\n","d_loss_wrong:0.7812773585319519\n","d_loss:0.6392385810613632\n","g_loss:[0.3308492, 0.32652354, 0.002162833]\n","Batch:105\n","d_loss_real:0.9196071624755859\n","d_loss_fake:0.0022239426616579294\n","d_loss_wrong:0.732125461101532\n","d_loss:0.6433909386396408\n","g_loss:[0.3295006, 0.32584643, 0.0018270833]\n","Batch:106\n","d_loss_real:0.8953241109848022\n","d_loss_fake:0.0018484201282262802\n","d_loss_wrong:0.6753062605857849\n","d_loss:0.6169507205486298\n","g_loss:[0.3294581, 0.32561806, 0.0019200117]\n","Batch:107\n","d_loss_real:0.9065741896629333\n","d_loss_fake:0.0016321077710017562\n","d_loss_wrong:0.7100560665130615\n","d_loss:0.631209135055542\n","g_loss:[0.33047983, 0.32619113, 0.00214435]\n","Batch:108\n","d_loss_real:0.8910287618637085\n","d_loss_fake:0.0011273003183305264\n","d_loss_wrong:0.7110029458999634\n","d_loss:0.6235469430685043\n","g_loss:[0.329336, 0.32627207, 0.0015319637]\n","Batch:109\n","d_loss_real:0.919110894203186\n","d_loss_fake:0.00117356120608747\n","d_loss_wrong:0.6876077651977539\n","d_loss:0.6317507773637772\n","g_loss:[0.32880536, 0.32575184, 0.0015267617]\n","Batch:110\n","d_loss_real:0.8913685083389282\n","d_loss_fake:0.0016519573982805014\n","d_loss_wrong:0.6807690858840942\n","d_loss:0.6162895113229752\n","g_loss:[0.33017677, 0.3256936, 0.0022415842]\n","Batch:111\n","d_loss_real:0.8701512813568115\n","d_loss_fake:0.0013983957469463348\n","d_loss_wrong:0.6983572244644165\n","d_loss:0.6100145429372787\n","g_loss:[0.33023962, 0.3257023, 0.0022686573]\n","Batch:112\n","d_loss_real:0.9108731746673584\n","d_loss_fake:0.001651796163059771\n","d_loss_wrong:0.753503143787384\n","d_loss:0.6442253291606903\n","g_loss:[0.33245218, 0.32864448, 0.0019038448]\n","Batch:113\n","d_loss_real:0.871704638004303\n","d_loss_fake:0.0021393834613263607\n","d_loss_wrong:0.6981266736984253\n","d_loss:0.6109188348054886\n","g_loss:[0.3319031, 0.32701692, 0.00244309]\n","Batch:114\n","d_loss_real:0.8515661954879761\n","d_loss_fake:0.0021741401869803667\n","d_loss_wrong:0.6804811954498291\n","d_loss:0.5964469313621521\n","g_loss:[0.3304805, 0.32618105, 0.0021497095]\n","Batch:115\n","d_loss_real:0.9032524824142456\n","d_loss_fake:0.0017061498947441578\n","d_loss_wrong:0.6774188280105591\n","d_loss:0.6214074790477753\n","g_loss:[0.32957637, 0.3257035, 0.001936443]\n","Batch:116\n","d_loss_real:0.9253732562065125\n","d_loss_fake:0.0018415431259199977\n","d_loss_wrong:0.7024182081222534\n","d_loss:0.6387515664100647\n","g_loss:[0.32950354, 0.325442, 0.0020307787]\n","Batch:117\n","d_loss_real:0.8926735520362854\n","d_loss_fake:0.0016189099987968802\n","d_loss_wrong:0.7048491835594177\n","d_loss:0.6229538023471832\n","g_loss:[0.3286149, 0.32550603, 0.001554436]\n","Batch:118\n","d_loss_real:0.8495965600013733\n","d_loss_fake:0.0018543553305789828\n","d_loss_wrong:0.7020649909973145\n","d_loss:0.600778117775917\n","g_loss:[0.32994518, 0.32676116, 0.00159201]\n","Batch:119\n","d_loss_real:0.8823829889297485\n","d_loss_fake:0.0019464795477688313\n","d_loss_wrong:0.6947075128555298\n","d_loss:0.6153549998998642\n","g_loss:[0.32890987, 0.32565677, 0.0016265503]\n","Batch:120\n","d_loss_real:0.8921433687210083\n","d_loss_fake:0.0018252915469929576\n","d_loss_wrong:0.674653947353363\n","d_loss:0.6151914894580841\n","g_loss:[0.3297225, 0.3263939, 0.0016642996]\n","Batch:121\n","d_loss_real:0.8857656717300415\n","d_loss_fake:0.001900843228213489\n","d_loss_wrong:0.7297205924987793\n","d_loss:0.6257881969213486\n","g_loss:[0.32962632, 0.32556874, 0.0020287896]\n","Batch:122\n","d_loss_real:0.8558027148246765\n","d_loss_fake:0.0019645891152322292\n","d_loss_wrong:0.6858016848564148\n","d_loss:0.5998429208993912\n","g_loss:[0.32965362, 0.32582057, 0.0019165275]\n","Batch:123\n","d_loss_real:0.937807559967041\n","d_loss_fake:0.0018232375150546432\n","d_loss_wrong:0.7535354495048523\n","d_loss:0.6577434539794922\n","g_loss:[0.33103636, 0.3279979, 0.0015192294]\n","Batch:124\n","d_loss_real:0.9034867286682129\n","d_loss_fake:0.0020205946639180183\n","d_loss_wrong:0.6639811396598816\n","d_loss:0.6182437986135483\n","g_loss:[0.3286216, 0.3260076, 0.0013069957]\n","Batch:125\n","d_loss_real:0.8659621477127075\n","d_loss_fake:0.0014996457612141967\n","d_loss_wrong:0.6909395456314087\n","d_loss:0.6060908734798431\n","g_loss:[0.32909688, 0.3261779, 0.0014594923]\n","Batch:126\n","d_loss_real:0.8712781071662903\n","d_loss_fake:0.001653960905969143\n","d_loss_wrong:0.6829046010971069\n","d_loss:0.60677869617939\n","g_loss:[0.3297834, 0.32634887, 0.0017172622]\n","Batch:127\n","d_loss_real:0.9348810911178589\n","d_loss_fake:0.0016925260424613953\n","d_loss_wrong:0.6695631742477417\n","d_loss:0.6352544724941254\n","g_loss:[0.32896456, 0.32551914, 0.0017227142]\n","Batch:128\n","d_loss_real:0.8767181634902954\n","d_loss_fake:0.0016651141922920942\n","d_loss_wrong:0.7736137509346008\n","d_loss:0.6321787983179092\n","g_loss:[0.33355567, 0.32928056, 0.0021375585]\n","Batch:129\n","d_loss_real:0.8891090154647827\n","d_loss_fake:0.0017670136876404285\n","d_loss_wrong:0.6973203420639038\n","d_loss:0.6193263530731201\n","g_loss:[0.33499694, 0.33026028, 0.0023683282]\n","Batch:130\n","d_loss_real:0.8901711702346802\n","d_loss_fake:0.0014248497318476439\n","d_loss_wrong:0.6885573267936707\n","d_loss:0.6175811290740967\n","g_loss:[0.32870436, 0.32550877, 0.0015977985]\n","Batch:131\n","d_loss_real:0.9131655693054199\n","d_loss_fake:0.0012345953145995736\n","d_loss_wrong:0.6959505677223206\n","d_loss:0.6308790743350983\n","g_loss:[0.33007503, 0.32599437, 0.0020403212]\n","Batch:132\n","d_loss_real:0.8634781837463379\n","d_loss_fake:0.0012519409647211432\n","d_loss_wrong:0.6939984560012817\n","d_loss:0.605551689863205\n","g_loss:[0.3287176, 0.32539046, 0.0016635695]\n","Batch:133\n","d_loss_real:0.8776696920394897\n","d_loss_fake:0.001520431600511074\n","d_loss_wrong:0.687113344669342\n","d_loss:0.6109932959079742\n","g_loss:[0.33033273, 0.3272743, 0.001529212]\n","Batch:134\n","d_loss_real:0.8621073961257935\n","d_loss_fake:0.0014722628984600306\n","d_loss_wrong:0.6867818832397461\n","d_loss:0.6031172275543213\n","g_loss:[0.32861722, 0.32545015, 0.001583539]\n","Batch:135\n","d_loss_real:0.8722995519638062\n","d_loss_fake:0.0014674215344712138\n","d_loss_wrong:0.6910243034362793\n","d_loss:0.6092727035284042\n","g_loss:[0.32764065, 0.3253733, 0.0011336786]\n","Batch:136\n","d_loss_real:0.8729275465011597\n","d_loss_fake:0.0016921267379075289\n","d_loss_wrong:0.6906667947769165\n","d_loss:0.6095535010099411\n","g_loss:[0.32815924, 0.32565212, 0.0012535648]\n","Batch:137\n","d_loss_real:0.8943643569946289\n","d_loss_fake:0.0013723192969337106\n","d_loss_wrong:0.6741285920143127\n","d_loss:0.6160574108362198\n","g_loss:[0.32844517, 0.32629433, 0.0010754179]\n","Batch:138\n","d_loss_real:0.8533820509910583\n","d_loss_fake:0.001655218075029552\n","d_loss_wrong:0.6883420944213867\n","d_loss:0.599190354347229\n"],"name":"stdout"},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["g_loss:[0.32832158, 0.32562223, 0.0013496797]\n"],"name":"stdout"},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["========================================\n","Epoch is: 1\n","Number of batches 138\n","Batch:1\n","d_loss_real:0.9109621047973633\n","d_loss_fake:0.0014479310484603047\n","d_loss_wrong:0.6969915628433228\n","d_loss:0.6300909221172333\n","g_loss:[0.33054292, 0.32678849, 0.0018772195]\n","Batch:2\n","d_loss_real:0.8685667514801025\n","d_loss_fake:0.00128651550039649\n","d_loss_wrong:0.694867730140686\n","d_loss:0.6083219349384308\n","g_loss:[0.33074906, 0.3265526, 0.0020982397]\n","Batch:3\n","d_loss_real:0.8682432174682617\n","d_loss_fake:0.0013006443623453379\n","d_loss_wrong:0.6966918706893921\n","d_loss:0.6086197346448898\n","g_loss:[0.32897323, 0.32554966, 0.0017117932]\n","Batch:4\n","d_loss_real:0.9126355051994324\n","d_loss_fake:0.0010545803233981133\n","d_loss_wrong:0.8126509189605713\n","d_loss:0.6597441285848618\n","g_loss:[0.3319525, 0.328766, 0.0015932659]\n","Batch:5\n","d_loss_real:0.85288405418396\n","d_loss_fake:0.0013932462316006422\n","d_loss_wrong:0.8870696425437927\n","d_loss:0.6485577523708344\n","g_loss:[0.32852405, 0.32562765, 0.0014482001]\n","Batch:6\n","d_loss_real:0.9122664928436279\n","d_loss_fake:0.0013707572361454368\n","d_loss_wrong:0.8837961554527283\n","d_loss:0.677424967288971\n","g_loss:[0.33799016, 0.33501446, 0.0014878535]\n","Batch:7\n","d_loss_real:0.9736813902854919\n","d_loss_fake:0.0014034754130989313\n","d_loss_wrong:0.6642192602157593\n","d_loss:0.6532463729381561\n","g_loss:[0.33342534, 0.33103508, 0.0011951394]\n","Batch:8\n","d_loss_real:0.9391868710517883\n","d_loss_fake:0.0007125826086848974\n","d_loss_wrong:0.6127432584762573\n","d_loss:0.622957393527031\n","g_loss:[0.33200508, 0.32744062, 0.0022822265]\n","Batch:9\n","d_loss_real:0.9251904487609863\n","d_loss_fake:0.00109668483491987\n","d_loss_wrong:0.6472429633140564\n","d_loss:0.6246801316738129\n","g_loss:[0.34598002, 0.33974886, 0.0031155797]\n","Batch:10\n","d_loss_real:0.8752487301826477\n","d_loss_fake:0.0011731290724128485\n","d_loss_wrong:0.662592887878418\n","d_loss:0.6035658717155457\n","g_loss:[0.32911864, 0.325718, 0.0017003331]\n","Batch:11\n","d_loss_real:0.8517462015151978\n","d_loss_fake:0.001211926806718111\n","d_loss_wrong:0.7057176232337952\n","d_loss:0.6026054918766022\n","g_loss:[0.33545232, 0.33247668, 0.0014878233]\n","Batch:12\n","d_loss_real:0.9133259057998657\n","d_loss_fake:0.0018623914802446961\n","d_loss_wrong:0.6657718420028687\n","d_loss:0.623571515083313\n","g_loss:[0.33027115, 0.32759497, 0.0013380956]\n","Batch:13\n","d_loss_real:0.9044358730316162\n","d_loss_fake:0.0011865360429510474\n","d_loss_wrong:0.6721669435501099\n","d_loss:0.6205563098192215\n","g_loss:[0.32850942, 0.3258592, 0.0013251216]\n","Batch:14\n","d_loss_real:0.9103225469589233\n","d_loss_fake:0.0013512157602235675\n","d_loss_wrong:0.681852400302887\n","d_loss:0.6259621828794479\n","g_loss:[0.3282179, 0.3259685, 0.0011246935]\n","Batch:15\n","d_loss_real:0.8947762250900269\n","d_loss_fake:0.0011276460718363523\n","d_loss_wrong:0.7672803401947021\n","d_loss:0.6394901126623154\n","g_loss:[0.33020857, 0.32624692, 0.0019808298]\n","Batch:16\n","d_loss_real:0.8697563409805298\n","d_loss_fake:0.0010025871451944113\n","d_loss_wrong:0.7625243067741394\n","d_loss:0.6257598996162415\n","g_loss:[0.3297175, 0.32631177, 0.0017028664]\n","Batch:17\n","d_loss_real:0.912832498550415\n","d_loss_fake:0.0010294895619153976\n","d_loss_wrong:0.9560049176216125\n","d_loss:0.6956748515367508\n","g_loss:[0.328333, 0.32548684, 0.0014230713]\n","Batch:18\n","d_loss_real:0.9037420153617859\n","d_loss_fake:0.0006806969176977873\n","d_loss_wrong:0.6510091423988342\n","d_loss:0.6147934645414352\n","g_loss:[0.33456057, 0.33249822, 0.0010311808]\n","Batch:19\n","d_loss_real:0.981624960899353\n","d_loss_fake:0.0015419531846418977\n","d_loss_wrong:0.6715075373649597\n","d_loss:0.6590748578310013\n","g_loss:[0.33784857, 0.33560538, 0.0011215959]\n","Batch:20\n","d_loss_real:0.9557923078536987\n","d_loss_fake:0.0015633502043783665\n","d_loss_wrong:0.7411211729049683\n","d_loss:0.6635672897100449\n","g_loss:[0.33002815, 0.3277971, 0.0011155132]\n","Batch:21\n","d_loss_real:0.8841389417648315\n","d_loss_fake:0.0013130559818819165\n","d_loss_wrong:0.6558793187141418\n","d_loss:0.6063675582408905\n","g_loss:[0.33839434, 0.33591664, 0.0012388537]\n","Batch:22\n","d_loss_real:0.8891681432723999\n","d_loss_fake:0.0010626146104186773\n","d_loss_wrong:0.6883057355880737\n","d_loss:0.6169261634349823\n","g_loss:[0.32774034, 0.3255722, 0.0010840708]\n","Batch:23\n","d_loss_real:0.9013157486915588\n","d_loss_fake:0.0017156375106424093\n","d_loss_wrong:0.6660588383674622\n","d_loss:0.6176014989614487\n","g_loss:[0.32980248, 0.32754537, 0.0011285604]\n","Batch:24\n","d_loss_real:0.8867968320846558\n","d_loss_fake:0.0013422181364148855\n","d_loss_wrong:0.6757050156593323\n","d_loss:0.6126602292060852\n","g_loss:[0.33489528, 0.3319988, 0.0014482465]\n","Batch:25\n","d_loss_real:0.8923168182373047\n","d_loss_fake:0.0016580892261117697\n","d_loss_wrong:0.7462214827537537\n","d_loss:0.6331283003091812\n","g_loss:[0.32987645, 0.3266195, 0.001628469]\n","Batch:26\n","d_loss_real:0.8729363083839417\n","d_loss_fake:0.0014929722528904676\n","d_loss_wrong:0.7047328352928162\n","d_loss:0.6130246073007584\n","g_loss:[0.33574814, 0.3329965, 0.0013758222]\n","Batch:27\n","d_loss_real:0.9128868579864502\n","d_loss_fake:0.0013127722777426243\n","d_loss_wrong:0.7223418354988098\n","d_loss:0.637357085943222\n","g_loss:[0.33290124, 0.33034337, 0.0012789323]\n","Batch:28\n","d_loss_real:0.8479843139648438\n","d_loss_fake:0.0013427118537947536\n","d_loss_wrong:0.6773892045021057\n","d_loss:0.5936751365661621\n","g_loss:[0.3288313, 0.3256816, 0.0015748432]\n","Batch:29\n","d_loss_real:0.8715459108352661\n","d_loss_fake:0.0012410372728481889\n","d_loss_wrong:0.67180997133255\n","d_loss:0.6040357053279877\n","g_loss:[0.32966805, 0.3268667, 0.0014006761]\n","Batch:30\n","d_loss_real:0.9257495403289795\n","d_loss_fake:0.0013645654544234276\n","d_loss_wrong:0.756605863571167\n","d_loss:0.6523673832416534\n","g_loss:[0.3293305, 0.32676798, 0.0012812541]\n","Batch:31\n","d_loss_real:0.8492380380630493\n","d_loss_fake:0.0011684776982292533\n","d_loss_wrong:0.6949902772903442\n","d_loss:0.5986587107181549\n","g_loss:[0.32858056, 0.32603905, 0.0012707586]\n","Batch:32\n","d_loss_real:0.868243932723999\n","d_loss_fake:0.0008817757479846478\n","d_loss_wrong:0.6602091193199158\n","d_loss:0.5993946939706802\n","g_loss:[0.32842097, 0.32557458, 0.0014231905]\n","Batch:33\n","d_loss_real:0.8881102800369263\n","d_loss_fake:0.0013276950921863317\n","d_loss_wrong:0.6613567471504211\n","d_loss:0.6097262501716614\n","g_loss:[0.32869807, 0.3254491, 0.0016244816]\n","Batch:34\n","d_loss_real:0.872883677482605\n","d_loss_fake:0.0010618758387863636\n","d_loss_wrong:0.671924352645874\n","d_loss:0.6046883910894394\n","g_loss:[0.32865244, 0.3257252, 0.0014636173]\n","Batch:35\n","d_loss_real:0.8692260980606079\n","d_loss_fake:0.0007894734153524041\n","d_loss_wrong:0.7116424441337585\n","d_loss:0.6127210259437561\n","g_loss:[0.32810098, 0.32564086, 0.0012300675]\n","Batch:36\n","d_loss_real:0.8670374155044556\n","d_loss_fake:0.0013609881279990077\n","d_loss_wrong:0.6721997261047363\n","d_loss:0.6019088923931122\n","g_loss:[0.32873592, 0.32559827, 0.0015688248]\n","Batch:37\n","d_loss_real:0.887338399887085\n","d_loss_fake:0.0013736231485381722\n","d_loss_wrong:0.7162541151046753\n","d_loss:0.6230761408805847\n","g_loss:[0.32937166, 0.32618463, 0.0015935142]\n","Batch:38\n","d_loss_real:0.8835101127624512\n","d_loss_fake:0.0012792914640158415\n","d_loss_wrong:0.7172845005989075\n","d_loss:0.621396005153656\n","g_loss:[0.33232316, 0.329801, 0.0012610807]\n","Batch:39\n","d_loss_real:0.8918543457984924\n","d_loss_fake:0.001252238405868411\n","d_loss_wrong:0.6904262900352478\n","d_loss:0.6188468039035797\n","g_loss:[0.32852772, 0.32567286, 0.0014274218]\n","Batch:40\n","d_loss_real:0.9100244641304016\n","d_loss_fake:0.001275746151804924\n","d_loss_wrong:0.7698994278907776\n","d_loss:0.6478060185909271\n","g_loss:[0.32970217, 0.32684666, 0.0014277493]\n","Batch:41\n","d_loss_real:0.8455850481987\n","d_loss_fake:0.0009847094770520926\n","d_loss_wrong:0.6816190481185913\n","d_loss:0.5934434682130814\n","g_loss:[0.32832178, 0.3255859, 0.0013679453]\n","Batch:42\n","d_loss_real:0.9247127175331116\n","d_loss_fake:0.0009730130550451577\n","d_loss_wrong:0.6535269618034363\n","d_loss:0.6259813457727432\n","g_loss:[0.33166802, 0.3291104, 0.0012788066]\n","Batch:43\n","d_loss_real:0.827713131904602\n","d_loss_fake:0.000874770455993712\n","d_loss_wrong:0.6871116161346436\n","d_loss:0.5858531594276428\n","g_loss:[0.32836404, 0.32543182, 0.0014661034]\n","Batch:44\n","d_loss_real:0.8531891107559204\n","d_loss_fake:0.0009584765648469329\n","d_loss_wrong:0.6879189610481262\n","d_loss:0.59881392121315\n","g_loss:[0.33017263, 0.3273924, 0.001390117]\n","Batch:45\n","d_loss_real:0.8422337174415588\n","d_loss_fake:0.0008852619794197381\n","d_loss_wrong:0.6691984534263611\n","d_loss:0.5886377841234207\n","g_loss:[0.32939956, 0.32615894, 0.0016203002]\n","Batch:46\n","d_loss_real:0.8599579334259033\n","d_loss_fake:0.0007296722033061087\n","d_loss_wrong:0.6675878763198853\n","d_loss:0.5970583558082581\n","g_loss:[0.3297493, 0.3263879, 0.0016806854]\n","Batch:47\n","d_loss_real:0.8716285228729248\n","d_loss_fake:0.0005664839409291744\n","d_loss_wrong:0.6810977458953857\n","d_loss:0.6062303185462952\n","g_loss:[0.32916957, 0.32635933, 0.0014051163]\n","Batch:48\n","d_loss_real:0.880998432636261\n","d_loss_fake:0.0007009942200966179\n","d_loss_wrong:0.6561098098754883\n","d_loss:0.6047019213438034\n","g_loss:[0.32812977, 0.32561582, 0.0012569698]\n","Batch:49\n","d_loss_real:0.8371131420135498\n","d_loss_fake:0.0007447509560734034\n","d_loss_wrong:0.6796499490737915\n","d_loss:0.5886552482843399\n","g_loss:[0.3292014, 0.32545796, 0.0018717134]\n","Batch:50\n","d_loss_real:0.8752298355102539\n","d_loss_fake:0.0009446275653317571\n","d_loss_wrong:0.6473050117492676\n","d_loss:0.599677324295044\n","g_loss:[0.32761315, 0.3254245, 0.0010943222]\n","Batch:51\n","d_loss_real:0.8621410131454468\n","d_loss_fake:0.0010606528958305717\n","d_loss_wrong:0.6694443821907043\n","d_loss:0.598696768283844\n","g_loss:[0.32838985, 0.3259708, 0.0012095243]\n","Batch:52\n","d_loss_real:0.8682467937469482\n","d_loss_fake:0.0007970748702064157\n","d_loss_wrong:0.6856717467308044\n","d_loss:0.6057406067848206\n","g_loss:[0.3272099, 0.3254687, 0.00087060215]\n","Batch:53\n","d_loss_real:0.8538657426834106\n","d_loss_fake:0.0008692153496667743\n","d_loss_wrong:0.6859849095344543\n","d_loss:0.5986464023590088\n","g_loss:[0.32819593, 0.32590133, 0.0011472944]\n","Batch:54\n","d_loss_real:0.8522026538848877\n","d_loss_fake:0.0007119234651327133\n","d_loss_wrong:0.6723516583442688\n","d_loss:0.5943672209978104\n","g_loss:[0.32750908, 0.32530898, 0.001100049]\n","Batch:55\n","d_loss_real:0.858723521232605\n","d_loss_fake:0.0009625331149436533\n","d_loss_wrong:0.6738283038139343\n","d_loss:0.5980594754219055\n","g_loss:[0.32818675, 0.32565418, 0.0012662901]\n","Batch:56\n","d_loss_real:0.8795968294143677\n","d_loss_fake:0.0010776461567729712\n","d_loss_wrong:0.690194845199585\n","d_loss:0.6126165390014648\n","g_loss:[0.3284752, 0.32570696, 0.0013841196]\n","Batch:57\n","d_loss_real:0.8696175217628479\n","d_loss_fake:0.0009967946680262685\n","d_loss_wrong:0.6759417653083801\n","d_loss:0.6040433943271637\n","g_loss:[0.328162, 0.32546696, 0.0013475268]\n","Batch:58\n","d_loss_real:0.850536584854126\n","d_loss_fake:0.000819587439764291\n","d_loss_wrong:0.6825518608093262\n","d_loss:0.5961111485958099\n","g_loss:[0.32800227, 0.3253342, 0.0013340353]\n","Batch:59\n","d_loss_real:0.8608808517456055\n","d_loss_fake:0.0008305555675178766\n","d_loss_wrong:0.6876142621040344\n","d_loss:0.6025516241788864\n","g_loss:[0.3274942, 0.32543856, 0.0010278152]\n","Batch:60\n","d_loss_real:0.8629617691040039\n","d_loss_fake:0.0007596518844366074\n","d_loss_wrong:0.6992900371551514\n","d_loss:0.6064933091402054\n","g_loss:[0.32809278, 0.32570404, 0.0011943757]\n","Batch:61\n","d_loss_real:0.8776508569717407\n","d_loss_fake:0.0008811748120933771\n","d_loss_wrong:0.6985768675804138\n","d_loss:0.6136899441480637\n","g_loss:[0.32751238, 0.325481, 0.0010156989]\n","Batch:62\n","d_loss_real:0.8849167227745056\n","d_loss_fake:0.0008070167968980968\n","d_loss_wrong:0.6749793887138367\n","d_loss:0.6114049553871155\n","g_loss:[0.32733575, 0.32536697, 0.0009843876]\n","Batch:63\n","d_loss_real:0.8866753578186035\n","d_loss_fake:0.0008682172046974301\n","d_loss_wrong:0.6770136952400208\n","d_loss:0.6128081530332565\n","g_loss:[0.32762223, 0.32563818, 0.0009920368]\n","Batch:64\n","d_loss_real:0.8446444272994995\n","d_loss_fake:0.0005731515120714903\n","d_loss_wrong:0.6875836849212646\n","d_loss:0.594361424446106\n","g_loss:[0.32750788, 0.32533604, 0.0010859162]\n","Batch:65\n","d_loss_real:0.8929203748703003\n","d_loss_fake:0.0008238860173150897\n","d_loss_wrong:0.6902877688407898\n","d_loss:0.6192381083965302\n","g_loss:[0.3278195, 0.32538453, 0.0012174897]\n","Batch:66\n","d_loss_real:0.8406338095664978\n","d_loss_fake:0.0010210792534053326\n","d_loss_wrong:0.6967201828956604\n","d_loss:0.5947522222995758\n","g_loss:[0.32727233, 0.32556894, 0.0008516888]\n","Batch:67\n","d_loss_real:0.8772420883178711\n","d_loss_fake:0.0011636314447969198\n","d_loss_wrong:0.6678937673568726\n","d_loss:0.60588538646698\n","g_loss:[0.32800126, 0.32632262, 0.00083931856]\n","Batch:68\n","d_loss_real:0.8615947365760803\n","d_loss_fake:0.0009696728084236383\n","d_loss_wrong:0.7121430039405823\n","d_loss:0.6090755313634872\n","g_loss:[0.32793444, 0.32585377, 0.0010403349]\n","Batch:69\n","d_loss_real:0.9061772227287292\n","d_loss_fake:0.0010425698710605502\n","d_loss_wrong:0.8319185972213745\n","d_loss:0.6613288968801498\n","g_loss:[0.32856318, 0.32672122, 0.0009209882]\n","Batch:70\n","d_loss_real:0.8449225425720215\n","d_loss_fake:0.0007445879164151847\n","d_loss_wrong:0.6961911916732788\n","d_loss:0.596695214509964\n","g_loss:[0.33021533, 0.3283775, 0.00091893045]\n","Batch:71\n","d_loss_real:0.8853561878204346\n","d_loss_fake:0.0008713623974472284\n","d_loss_wrong:0.636110782623291\n","d_loss:0.6019236296415329\n","g_loss:[0.3299068, 0.3278983, 0.0010042515]\n","Batch:72\n","d_loss_real:0.8892686367034912\n","d_loss_fake:0.0011069958563894033\n","d_loss_wrong:0.6515125632286072\n","d_loss:0.6077892035245895\n","g_loss:[0.32694167, 0.32530707, 0.00081729563]\n","Batch:73\n","d_loss_real:0.8639411330223083\n","d_loss_fake:0.000970019435044378\n","d_loss_wrong:0.6659114956855774\n","d_loss:0.5986909419298172\n","g_loss:[0.32727817, 0.32556283, 0.0008576721]\n","Batch:74\n","d_loss_real:0.8600414991378784\n","d_loss_fake:0.0009275506599806249\n","d_loss_wrong:0.6998933553695679\n","d_loss:0.6052259802818298\n","g_loss:[0.32761955, 0.3258203, 0.0008996246]\n","Batch:75\n","d_loss_real:0.8761180639266968\n","d_loss_fake:0.0008952913922257721\n","d_loss_wrong:0.68073970079422\n","d_loss:0.608467772603035\n","g_loss:[0.32805395, 0.32601446, 0.0010197512]\n","Batch:76\n","d_loss_real:0.8695496320724487\n","d_loss_fake:0.0009827258763834834\n","d_loss_wrong:0.6661806702613831\n","d_loss:0.6015656590461731\n","g_loss:[0.32716498, 0.32541668, 0.00087414036]\n","Batch:77\n","d_loss_real:0.8831067085266113\n","d_loss_fake:0.0014895563945174217\n","d_loss_wrong:0.6734207272529602\n","d_loss:0.6102809309959412\n","g_loss:[0.3272053, 0.32551652, 0.0008443921]\n","Batch:78\n","d_loss_real:0.8632746934890747\n","d_loss_fake:0.0013301409780979156\n","d_loss_wrong:0.6737332940101624\n","d_loss:0.6004032045602798\n","g_loss:[0.32765472, 0.3258704, 0.00089216226]\n","Batch:79\n","d_loss_real:0.846924364566803\n","d_loss_fake:0.0013006937224417925\n","d_loss_wrong:0.671790361404419\n","d_loss:0.5917349457740784\n","g_loss:[0.32737982, 0.32558542, 0.00089719787]\n","Batch:80\n","d_loss_real:0.8492226004600525\n","d_loss_fake:0.000980596523731947\n","d_loss_wrong:0.664966881275177\n","d_loss:0.5910981744527817\n","g_loss:[0.32828107, 0.32653192, 0.0008745723]\n","Batch:81\n","d_loss_real:0.8526367545127869\n","d_loss_fake:0.0010329822544008493\n","d_loss_wrong:0.6447765231132507\n","d_loss:0.5877707600593567\n","g_loss:[0.32721025, 0.3254334, 0.00088841893]\n","Batch:82\n","d_loss_real:0.8994432687759399\n","d_loss_fake:0.0007816294091753662\n","d_loss_wrong:0.6793110370635986\n","d_loss:0.6197448074817657\n","g_loss:[0.32952747, 0.32775375, 0.00088686415]\n","Batch:83\n","d_loss_real:0.8302059173583984\n","d_loss_fake:0.0008698242600075901\n","d_loss_wrong:0.6624696850776672\n","d_loss:0.5809378325939178\n","g_loss:[0.32702547, 0.32533708, 0.00084418856]\n","Batch:84\n","d_loss_real:0.8654587268829346\n","d_loss_fake:0.0008567121112719178\n","d_loss_wrong:0.6771901845932007\n","d_loss:0.6022410839796066\n","g_loss:[0.32729015, 0.32544088, 0.0009246272]\n","Batch:85\n","d_loss_real:0.8562871217727661\n","d_loss_fake:0.0007760812295600772\n","d_loss_wrong:0.6721035838127136\n","d_loss:0.5963634699583054\n","g_loss:[0.32706672, 0.32545704, 0.0008048414]\n","Batch:86\n","d_loss_real:0.849148154258728\n","d_loss_fake:0.0010722442530095577\n","d_loss_wrong:0.6594983339309692\n","d_loss:0.5897167176008224\n","g_loss:[0.3268865, 0.32524496, 0.00082077074]\n","Batch:87\n","d_loss_real:0.8423646688461304\n","d_loss_fake:0.0006917549180798233\n","d_loss_wrong:0.6719876527786255\n","d_loss:0.5893521904945374\n","g_loss:[0.32776174, 0.3256711, 0.0010453103]\n","Batch:88\n","d_loss_real:0.8748399615287781\n","d_loss_fake:0.0007335161790251732\n","d_loss_wrong:0.6748936772346497\n","d_loss:0.6063267737627029\n","g_loss:[0.3273922, 0.3254323, 0.0009799515]\n","Batch:89\n","d_loss_real:0.873266339302063\n","d_loss_fake:0.0006274274783208966\n","d_loss_wrong:0.7255493402481079\n","d_loss:0.6181773543357849\n","g_loss:[0.32980585, 0.32703006, 0.0013878952]\n","Batch:90\n","d_loss_real:0.8472727537155151\n","d_loss_fake:0.0007643864955753088\n","d_loss_wrong:0.7068410515785217\n","d_loss:0.6005377322435379\n","g_loss:[0.32736814, 0.32529402, 0.0010370555]\n","Batch:91\n","d_loss_real:0.8583536148071289\n","d_loss_fake:0.000659644661936909\n","d_loss_wrong:0.6822009682655334\n","d_loss:0.5998919606208801\n","g_loss:[0.32746404, 0.32572544, 0.00086930394]\n","Batch:92\n","d_loss_real:0.8936423063278198\n","d_loss_fake:0.0008412150200456381\n","d_loss_wrong:0.7530695199966431\n","d_loss:0.6352988332509995\n","g_loss:[0.32746863, 0.32528725, 0.0010906914]\n","Batch:93\n","d_loss_real:0.869730532169342\n","d_loss_fake:0.000918571138754487\n","d_loss_wrong:0.6622278094291687\n","d_loss:0.6006518602371216\n","g_loss:[0.33019468, 0.32780132, 0.0011966851]\n","Batch:94\n","d_loss_real:0.8761711716651917\n","d_loss_fake:0.0008372932206839323\n","d_loss_wrong:0.6898412704467773\n","d_loss:0.6107552200555801\n","g_loss:[0.32925248, 0.32686967, 0.001191406]\n","Batch:95\n","d_loss_real:0.8952587842941284\n","d_loss_fake:0.0005577575066126883\n","d_loss_wrong:0.6469383835792542\n","d_loss:0.6095034331083298\n","g_loss:[0.32969058, 0.32750225, 0.0010941596]\n","Batch:96\n","d_loss_real:0.8296823501586914\n","d_loss_fake:0.00067833939101547\n","d_loss_wrong:0.6618629097938538\n","d_loss:0.5804764926433563\n","g_loss:[0.3281356, 0.3255958, 0.0012699114]\n","Batch:97\n","d_loss_real:0.8467561602592468\n","d_loss_fake:0.0006990628316998482\n","d_loss_wrong:0.6640176177024841\n","d_loss:0.5895572453737259\n","g_loss:[0.32793716, 0.3253864, 0.0012753791]\n","Batch:98\n","d_loss_real:0.8599294424057007\n","d_loss_fake:0.0006768663879483938\n","d_loss_wrong:0.6905127763748169\n","d_loss:0.6027621328830719\n","g_loss:[0.32747006, 0.32532156, 0.0010742618]\n","Batch:99\n","d_loss_real:0.8700166940689087\n","d_loss_fake:0.0006576521554961801\n","d_loss_wrong:0.6607314944267273\n","d_loss:0.6003556400537491\n","g_loss:[0.32757723, 0.3253372, 0.0011200206]\n","Batch:100\n","d_loss_real:0.8392752408981323\n","d_loss_fake:0.0007077896734699607\n","d_loss_wrong:0.6666967868804932\n","d_loss:0.5864887684583664\n","g_loss:[0.32737172, 0.32533664, 0.0010175337]\n","Batch:101\n","d_loss_real:0.8649110198020935\n","d_loss_fake:0.0007638692623004317\n","d_loss_wrong:0.674326479434967\n","d_loss:0.6012281030416489\n","g_loss:[0.3269388, 0.32536745, 0.00078567717]\n","Batch:102\n","d_loss_real:0.8701598644256592\n","d_loss_fake:0.00060190015938133\n","d_loss_wrong:0.6984900236129761\n","d_loss:0.6098529100418091\n","g_loss:[0.32699838, 0.325301, 0.0008486924]\n","Batch:103\n","d_loss_real:0.8616265058517456\n","d_loss_fake:0.0007150272140279412\n","d_loss_wrong:0.6654952168464661\n","d_loss:0.5973658114671707\n","g_loss:[0.32747957, 0.32553464, 0.00097245775]\n","Batch:104\n","d_loss_real:0.852048397064209\n","d_loss_fake:0.0008207028149627149\n","d_loss_wrong:0.7247377634048462\n","d_loss:0.6074138134717941\n","g_loss:[0.3272628, 0.32538167, 0.0009405571]\n","Batch:105\n","d_loss_real:0.8821316957473755\n","d_loss_fake:0.0009141392074525356\n","d_loss_wrong:0.6942576169967651\n","d_loss:0.6148587912321091\n","g_loss:[0.32713586, 0.32546574, 0.0008350648]\n","Batch:106\n","d_loss_real:0.8520410060882568\n","d_loss_fake:0.0006742316763848066\n","d_loss_wrong:0.6549661755561829\n","d_loss:0.5899306088685989\n","g_loss:[0.3275517, 0.32583126, 0.0008602151]\n","Batch:107\n","d_loss_real:0.8780975937843323\n","d_loss_fake:0.0007941887015476823\n","d_loss_wrong:0.6778779029846191\n","d_loss:0.6087168157100677\n","g_loss:[0.32779193, 0.32589644, 0.00094774313]\n","Batch:108\n","d_loss_real:0.8576350212097168\n","d_loss_fake:0.0006080251187086105\n","d_loss_wrong:0.682779848575592\n","d_loss:0.5996644794940948\n","g_loss:[0.32705873, 0.3255062, 0.0007762565]\n","Batch:109\n","d_loss_real:0.8745874166488647\n","d_loss_fake:0.0007236828096210957\n","d_loss_wrong:0.6633971929550171\n","d_loss:0.6033239215612411\n","g_loss:[0.32696638, 0.3253838, 0.0007912805]\n","Batch:110\n","d_loss_real:0.84824538230896\n","d_loss_fake:0.0005022305413149297\n","d_loss_wrong:0.6680576801300049\n","d_loss:0.5912626683712006\n","g_loss:[0.327451, 0.32547507, 0.000987962]\n","Batch:111\n","d_loss_real:0.840295672416687\n","d_loss_fake:0.0009565339423716068\n","d_loss_wrong:0.6673867702484131\n","d_loss:0.5872336626052856\n","g_loss:[0.32737413, 0.3252887, 0.0010427075]\n","Batch:112\n","d_loss_real:0.8603315353393555\n","d_loss_fake:0.000593271222896874\n","d_loss_wrong:0.6994726061820984\n","d_loss:0.6051822304725647\n","g_loss:[0.3279421, 0.32600638, 0.0009678643]\n","Batch:113\n","d_loss_real:0.8457298278808594\n","d_loss_fake:0.0006612170254811645\n","d_loss_wrong:0.6745842099189758\n","d_loss:0.591676265001297\n","g_loss:[0.3275935, 0.32533264, 0.0011304294]\n","Batch:114\n","d_loss_real:0.8288940191268921\n","d_loss_fake:0.0009176243329420686\n","d_loss_wrong:0.6539439558982849\n","d_loss:0.5781624019145966\n","g_loss:[0.32738832, 0.32528317, 0.0010525679]\n","Batch:115\n","d_loss_real:0.8636844754219055\n","d_loss_fake:0.0006099541205912828\n","d_loss_wrong:0.6592157483100891\n","d_loss:0.5967986583709717\n","g_loss:[0.32729995, 0.32529956, 0.0010001884]\n","Batch:116\n","d_loss_real:0.8761913776397705\n","d_loss_fake:0.0005142546724528074\n","d_loss_wrong:0.6736939549446106\n","d_loss:0.6066477447748184\n","g_loss:[0.3274187, 0.32533073, 0.001043989]\n","Batch:117\n","d_loss_real:0.8767774701118469\n","d_loss_fake:0.0010954838944599032\n","d_loss_wrong:0.6704599857330322\n","d_loss:0.6062775999307632\n","g_loss:[0.32699847, 0.32538098, 0.00080874603]\n","Batch:118\n","d_loss_real:0.8313677310943604\n","d_loss_fake:0.0010671403724700212\n","d_loss_wrong:0.6730384230613708\n","d_loss:0.5842102617025375\n","g_loss:[0.32731357, 0.3256898, 0.00081188424]\n","Batch:119\n","d_loss_real:0.8506017327308655\n","d_loss_fake:0.0010613856138661504\n","d_loss_wrong:0.6763097047805786\n","d_loss:0.5946436375379562\n","g_loss:[0.32727575, 0.32558507, 0.0008453474]\n","Batch:120\n","d_loss_real:0.8588542342185974\n","d_loss_fake:0.000694593763910234\n","d_loss_wrong:0.6559870839118958\n","d_loss:0.5935975313186646\n","g_loss:[0.3276377, 0.32583255, 0.0009025722]\n","Batch:121\n","d_loss_real:0.8591018915176392\n","d_loss_fake:0.0009288600995205343\n","d_loss_wrong:0.6929627060890198\n","d_loss:0.6030238419771194\n","g_loss:[0.32729423, 0.32529432, 0.0009999637]\n","Batch:122\n","d_loss_real:0.8333778381347656\n","d_loss_fake:0.0007179105305112898\n","d_loss_wrong:0.6661199927330017\n","d_loss:0.5833984017372131\n","g_loss:[0.32794225, 0.32604134, 0.0009504491]\n","Batch:123\n","d_loss_real:0.8900082111358643\n","d_loss_fake:0.0009042687015607953\n","d_loss_wrong:0.7236819863319397\n","d_loss:0.6261506676673889\n","g_loss:[0.3282867, 0.32669675, 0.00079497637]\n","Batch:124\n","d_loss_real:0.8601831197738647\n","d_loss_fake:0.0007789066294208169\n","d_loss_wrong:0.6495915651321411\n","d_loss:0.5926841795444489\n","g_loss:[0.32679778, 0.3253321, 0.0007328368]\n","Batch:125\n","d_loss_real:0.846383810043335\n","d_loss_fake:0.0011144938180223107\n","d_loss_wrong:0.6642841100692749\n","d_loss:0.5895415544509888\n","g_loss:[0.3270678, 0.32546818, 0.00079980784]\n","Batch:126\n","d_loss_real:0.8504387140274048\n","d_loss_fake:0.0007434157887473702\n","d_loss_wrong:0.660304844379425\n","d_loss:0.5904814153909683\n","g_loss:[0.32747254, 0.32561317, 0.0009296797]\n","Batch:127\n","d_loss_real:0.886466920375824\n","d_loss_fake:0.0007225093431770802\n","d_loss_wrong:0.6530653834342957\n","d_loss:0.6066804379224777\n","g_loss:[0.32721245, 0.3253489, 0.0009317748]\n","Batch:128\n","d_loss_real:0.8564276695251465\n","d_loss_fake:0.0007655539666302502\n","d_loss_wrong:0.7250500917434692\n","d_loss:0.6096677482128143\n","g_loss:[0.3292271, 0.32711232, 0.0010573874]\n","Batch:129\n","d_loss_real:0.8533251285552979\n","d_loss_fake:0.0006519075250253081\n","d_loss_wrong:0.6789244413375854\n","d_loss:0.5965566486120224\n","g_loss:[0.32863548, 0.32637218, 0.001131653]\n","Batch:130\n","d_loss_real:0.8562685251235962\n","d_loss_fake:0.0007925690733827651\n","d_loss_wrong:0.667384147644043\n","d_loss:0.5951784402132034\n","g_loss:[0.32744944, 0.32578763, 0.00083089806]\n","Batch:131\n","d_loss_real:0.8865980505943298\n","d_loss_fake:0.0008845289121381938\n","d_loss_wrong:0.6710469126701355\n","d_loss:0.6112818866968155\n","g_loss:[0.32730353, 0.32529855, 0.0010024895]\n","Batch:132\n","d_loss_real:0.8408204317092896\n","d_loss_fake:0.0008379047503694892\n","d_loss_wrong:0.6692463755607605\n","d_loss:0.587931290268898\n","g_loss:[0.32740727, 0.32569236, 0.00085745286]\n","Batch:133\n","d_loss_real:0.85422682762146\n","d_loss_fake:0.0007449286058545113\n","d_loss_wrong:0.6646203398704529\n","d_loss:0.5934547334909439\n","g_loss:[0.32705644, 0.32537752, 0.0008394536]\n","Batch:134\n","d_loss_real:0.844266951084137\n","d_loss_fake:0.0007988273864611983\n","d_loss_wrong:0.6626172661781311\n","d_loss:0.5879874974489212\n","g_loss:[0.32701948, 0.3253759, 0.0008217827]\n","Batch:135\n","d_loss_real:0.8534741401672363\n","d_loss_fake:0.0008848183788359165\n","d_loss_wrong:0.6738856434822083\n","d_loss:0.5954296886920929\n","g_loss:[0.32657668, 0.32529423, 0.00064121955]\n","Batch:136\n","d_loss_real:0.8473687171936035\n","d_loss_fake:0.0007910350104793906\n","d_loss_wrong:0.6727414131164551\n","d_loss:0.5920674651861191\n","g_loss:[0.32662913, 0.32525417, 0.0006874828]\n","Batch:137\n","d_loss_real:0.8630189895629883\n","d_loss_fake:0.0006421937141567469\n","d_loss_wrong:0.6549426913261414\n","d_loss:0.595405712723732\n","g_loss:[0.32717094, 0.32595545, 0.00060773926]\n","Batch:138\n","d_loss_real:0.8392971158027649\n","d_loss_fake:0.001007488346658647\n","d_loss_wrong:0.670404314994812\n","d_loss:0.5875015109777451\n","g_loss:[0.32681164, 0.32527813, 0.00076675514]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BuQPCc-eVTKp","colab_type":"code","colab":{}},"source":["def residual_block(input):\n","    \"\"\"\n","    Residual block in the generator network\n","    \"\"\"\n","    x = Conv2D(128 * 4, kernel_size=(3, 3), padding='same', strides=1)(input)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = Conv2D(128 * 4, kernel_size=(3, 3), strides=1, padding='same')(x)\n","    x = BatchNormalization()(x)\n","\n","    x = add([x, input])\n","    x = ReLU()(x)\n","\n","    return x\n","  \n","def joint_block(inputs):\n","    c = inputs[0]\n","    x = inputs[1]\n","\n","    c = K.expand_dims(c, axis=1)\n","    c = K.expand_dims(c, axis=1)\n","    c = K.tile(c, [1, 16, 16, 1])\n","    return K.concatenate([c, x], axis=3)\n","  \n","def build_stage2_generator():\n","    \"\"\"\n","    Create Stage-II generator containing the CA Augmentation Network,\n","    the image encoder and the generator network\n","    \"\"\"\n","\n","    # 1. CA Augmentation Network\n","    input_layer = Input(shape=(1024,))\n","    input_lr_images = Input(shape=(64, 64, 3))\n","\n","    ca = Dense(256)(input_layer)\n","    mean_logsigma = LeakyReLU(alpha=0.2)(ca)\n","    c = Lambda(generate_c)(mean_logsigma)\n","\n","    # 2. Image Encoder\n","    x = ZeroPadding2D(padding=(1, 1))(input_lr_images)\n","    x = Conv2D(128, kernel_size=(3, 3), strides=1, use_bias=False)(x)\n","    x = ReLU()(x)\n","\n","    x = ZeroPadding2D(padding=(1, 1))(x)\n","    x = Conv2D(256, kernel_size=(4, 4), strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = ZeroPadding2D(padding=(1, 1))(x)\n","    x = Conv2D(512, kernel_size=(4, 4), strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    # 3. Joint\n","    c_code = Lambda(joint_block)([c, x])\n","\n","    x = ZeroPadding2D(padding=(1, 1))(c_code)\n","    x = Conv2D(512, kernel_size=(3, 3), strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    # 4. Residual blocks\n","    x = residual_block(x)\n","    x = residual_block(x)\n","    x = residual_block(x)\n","    x = residual_block(x)\n","\n","    # 5. Upsampling blocks\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = Activation('tanh')(x)\n","\n","    model = Model(inputs=[input_layer, input_lr_images], outputs=[x, mean_logsigma])\n","    return model\n","  \n","def build_stage2_discriminator():\n","    \"\"\"\n","    Create Stage-II discriminator network\n","    \"\"\"\n","    input_layer = Input(shape=(256, 256, 3))\n","\n","    x = Conv2D(64, (4, 4), padding='same', strides=2, input_shape=(256, 256, 3), use_bias=False)(input_layer)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(1024, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(2048, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(1024, (1, 1), padding='same', strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(512, (1, 1), padding='same', strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","\n","    x2 = Conv2D(128, (1, 1), padding='same', strides=1, use_bias=False)(x)\n","    x2 = BatchNormalization()(x2)\n","    x2 = LeakyReLU(alpha=0.2)(x2)\n","\n","    x2 = Conv2D(128, (3, 3), padding='same', strides=1, use_bias=False)(x2)\n","    x2 = BatchNormalization()(x2)\n","    x2 = LeakyReLU(alpha=0.2)(x2)\n","\n","    x2 = Conv2D(512, (3, 3), padding='same', strides=1, use_bias=False)(x2)\n","    x2 = BatchNormalization()(x2)\n","\n","    added_x = add([x, x2])\n","    added_x = LeakyReLU(alpha=0.2)(added_x)\n","\n","    input_layer2 = Input(shape=(4, 4, 128))\n","\n","    merged_input = concatenate([added_x, input_layer2])\n","\n","    x3 = Conv2D(64 * 8, kernel_size=1, padding=\"same\", strides=1)(merged_input)\n","    x3 = BatchNormalization()(x3)\n","    x3 = LeakyReLU(alpha=0.2)(x3)\n","    x3 = Flatten()(x3)\n","    x3 = Dense(1)(x3)\n","    x3 = Activation('sigmoid')(x3)\n","\n","    stage2_dis = Model(inputs=[input_layer, input_layer2], outputs=[x3])\n","    return stage2_dis\n","  \n","def build_adversarial_model(gen_model2, dis_model, gen_model1):\n","    \"\"\"\n","    Create adversarial model\n","    \"\"\"\n","    embeddings_input_layer = Input(shape=(1024, ))\n","    noise_input_layer = Input(shape=(100, ))\n","    compressed_embedding_input_layer = Input(shape=(4, 4, 128))\n","\n","    gen_model1.trainable = False\n","    dis_model.trainable = False\n","\n","    lr_images, mean_logsigma1 = gen_model1([embeddings_input_layer, noise_input_layer])\n","    hr_images, mean_logsigma2 = gen_model2([embeddings_input_layer, lr_images])\n","    valid = dis_model([hr_images, compressed_embedding_input_layer])\n","\n","    model = Model(inputs=[embeddings_input_layer, noise_input_layer, compressed_embedding_input_layer], outputs=[valid, mean_logsigma2])\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1WfenJ4lAi9g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1592474347171,"user_tz":-540,"elapsed":6393964,"user":{"displayName":"ᄏᄏᄏ","photoUrl":"","userId":"10582150059672851107"}},"outputId":"8fcf1653-e08d-4289-f62f-c2d39a29ce62"},"source":["if __name__ == '__main__':\n","    data_dir = \"/content/birds/birds/\"\n","    train_dir = data_dir + \"/train\"\n","    test_dir = data_dir + \"/test\"\n","    hr_image_size = (256, 256)\n","    lr_image_size = (64, 64)\n","    batch_size = 32\n","    z_dim = 100\n","    stage1_generator_lr = 0.0002\n","    stage1_discriminator_lr = 0.0002\n","    stage1_lr_decay_step = 600\n","    epochs = 2\n","    condition_dim = 128\n","\n","    embeddings_file_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n","    embeddings_file_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n","\n","    filenames_file_path_train = train_dir + \"/filenames.pickle\"\n","    filenames_file_path_test = test_dir + \"/filenames.pickle\"\n","\n","    class_info_file_path_train = train_dir + \"/class_info.pickle\"\n","    class_info_file_path_test = test_dir + \"/class_info.pickle\"\n","\n","    cub_dataset_dir = \"/content/gdrive/My Drive/딥러닝/CUB_200_2011\"\n","\n","    # Define optimizers\n","    dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n","    gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n","\n","    \"\"\"\n","    Load datasets\n","    \"\"\"\n","    X_hr_train, y_hr_train, embeddings_train = load_dataset(filenames_file_path=filenames_file_path_train,\n","                                                            class_info_file_path=class_info_file_path_train,\n","                                                            cub_dataset_dir=cub_dataset_dir,\n","                                                            embeddings_file_path=embeddings_file_path_train,\n","                                                            image_size=(256, 256))\n","\n","    X_hr_test, y_hr_test, embeddings_test = load_dataset(filenames_file_path=filenames_file_path_test,\n","                                                         class_info_file_path=class_info_file_path_test,\n","                                                         cub_dataset_dir=cub_dataset_dir,\n","                                                         embeddings_file_path=embeddings_file_path_test,\n","                                                         image_size=(256, 256))\n","\n","    X_lr_train, y_lr_train, _ = load_dataset(filenames_file_path=filenames_file_path_train,\n","                                             class_info_file_path=class_info_file_path_train,\n","                                             cub_dataset_dir=cub_dataset_dir,\n","                                             embeddings_file_path=embeddings_file_path_train,\n","                                             image_size=(64, 64))\n","\n","    X_lr_test, y_lr_test, _ = load_dataset(filenames_file_path=filenames_file_path_test,\n","                                           class_info_file_path=class_info_file_path_test,\n","                                           cub_dataset_dir=cub_dataset_dir,\n","                                           embeddings_file_path=embeddings_file_path_test,\n","                                           image_size=(64, 64))\n","\n","    \"\"\"\n","    Build and compile models\n","    \"\"\"\n","    stage2_dis = build_stage2_discriminator()\n","    stage2_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n","\n","    stage1_gen = build_stage1_generator()\n","    stage1_gen.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n","\n","    stage1_gen.load_weights(\"stage1_gen.h5\")\n","\n","    stage2_gen = build_stage2_generator()\n","    stage2_gen.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n","\n","    embedding_compressor_model = build_embedding_compressor_model()\n","    embedding_compressor_model.compile(loss='binary_crossentropy', optimizer='adam')\n","\n","    adversarial_model = build_adversarial_model(stage2_gen, stage2_dis, stage1_gen)\n","    adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1.0, 2.0],\n","                              optimizer=gen_optimizer, metrics=None)\n","\n","    tensorboard = TensorBoard(log_dir=\"logs/\".format(time.time()))\n","    tensorboard.set_model(stage2_gen)\n","    tensorboard.set_model(stage2_dis)\n","\n","    # Generate an array containing real and fake values\n","    # Apply label smoothing\n","    real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n","    fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n","\n","    for epoch in range(epochs):\n","        print(\"========================================\")\n","        print(\"Epoch is:\", epoch)\n","\n","        gen_losses = []\n","        dis_losses = []\n","\n","        # Load data and train model\n","        number_of_batches = int(X_hr_train.shape[0] / batch_size)\n","        print(\"Number of batches:{}\".format(number_of_batches))\n","        for index in range(number_of_batches):\n","            print(\"Batch:{}\".format(index+1))\n","\n","            # Create a noise vector\n","            z_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n","            X_hr_train_batch = X_hr_train[index * batch_size:(index + 1) * batch_size]\n","            embedding_batch = embeddings_train[index * batch_size:(index + 1) * batch_size]\n","            X_hr_train_batch = (X_hr_train_batch - 127.5) / 127.5\n","\n","            # Generate fake images\n","            lr_fake_images, _ = stage1_gen.predict([embedding_batch, z_noise], verbose=3)\n","            hr_fake_images, _ = stage2_gen.predict([embedding_batch, lr_fake_images], verbose=3)\n","\n","            \"\"\"\n","            4. Generate compressed embeddings\n","            \"\"\"\n","            compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n","            compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n","            compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n","\n","            \"\"\"\n","            5. Train the discriminator model\n","            \"\"\"\n","            dis_loss_real = stage2_dis.train_on_batch([X_hr_train_batch, compressed_embedding],\n","                                                      np.reshape(real_labels, (batch_size, 1)))\n","            dis_loss_fake = stage2_dis.train_on_batch([hr_fake_images, compressed_embedding],\n","                                                      np.reshape(fake_labels, (batch_size, 1)))\n","            dis_loss_wrong = stage2_dis.train_on_batch([X_hr_train_batch[:(batch_size - 1)], compressed_embedding[1:]],\n","                                                       np.reshape(fake_labels[1:], (batch_size-1, 1)))\n","            d_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong,  dis_loss_fake))\n","            print(\"d_loss:{}\".format(d_loss))\n","\n","            \"\"\"\n","            Train the adversarial model\n","            \"\"\"\n","            g_loss = adversarial_model.train_on_batch([embedding_batch, z_noise, compressed_embedding],\n","                                                                [K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n","\n","            print(\"g_loss:{}\".format(g_loss))\n","\n","            dis_losses.append(d_loss)\n","            gen_losses.append(g_loss)\n","\n","        \"\"\"\n","        Save losses to Tensorboard after each epoch\n","        \"\"\"\n","      #  write_log(tensorboard, 'discriminator_loss', np.mean(dis_losses), epoch)\n","       # write_log(tensorboard, 'generator_loss', np.mean(gen_losses)[0], epoch)\n","\n","        # Generate and save images after every 2nd epoch\n","        if epoch % 2 == 0:\n","            # z_noise2 = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n","            z_noise2 = np.random.normal(0, 1, size=(batch_size, z_dim))\n","            embedding_batch = embeddings_test[0:batch_size]\n","\n","            lr_fake_images, _ = stage1_gen.predict([embedding_batch, z_noise2], verbose=3)\n","            hr_fake_images, _ = stage2_gen.predict([embedding_batch, lr_fake_images], verbose=3)\n","            # Save images\n","            for i, img in enumerate(lr_fake_images[:10]):\n","                save_rgb_img(img, \"/content/gdrive/My Drive/딥러닝/results/gen1_{}_{}.png\".format(epoch, i))\n","            for i, img in enumerate(hr_fake_images[:10]):\n","                save_rgb_img(img, \"/content/gdrive/My Drive/딥러닝/results/gen2_{}_{}.png\".format(epoch, i))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["embeddings:  (8855, 10, 1024)\n","Embeddings shape: (8855, 10, 1024)\n","embeddings:  (2933, 10, 1024)\n","Embeddings shape: (2933, 10, 1024)\n","embeddings:  (8855, 10, 1024)\n","Embeddings shape: (8855, 10, 1024)\n","embeddings:  (2933, 10, 1024)\n","Embeddings shape: (2933, 10, 1024)\n","Model: \"model_22\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_41 (InputLayer)           (None, 1024)         0                                            \n","__________________________________________________________________________________________________\n","dense_22 (Dense)                (None, 256)          262400      input_41[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_55 (LeakyReLU)      (None, 256)          0           dense_22[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (None, 128)          0           leaky_re_lu_55[0][0]             \n","__________________________________________________________________________________________________\n","input_42 (InputLayer)           (None, 100)          0                                            \n","__________________________________________________________________________________________________\n","concatenate_10 (Concatenate)    (None, 228)          0           lambda_7[0][0]                   \n","                                                                 input_42[0][0]                   \n","__________________________________________________________________________________________________\n","dense_23 (Dense)                (None, 16384)        3735552     concatenate_10[0][0]             \n","__________________________________________________________________________________________________\n","re_lu_41 (ReLU)                 (None, 16384)        0           dense_23[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_5 (Reshape)             (None, 4, 4, 1024)   0           re_lu_41[0][0]                   \n","__________________________________________________________________________________________________\n","up_sampling2d_21 (UpSampling2D) (None, 8, 8, 1024)   0           reshape_5[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_88 (Conv2D)              (None, 8, 8, 512)    4718592     up_sampling2d_21[0][0]           \n","__________________________________________________________________________________________________\n","batch_normalization_76 (BatchNo (None, 8, 8, 512)    2048        conv2d_88[0][0]                  \n","__________________________________________________________________________________________________\n","re_lu_42 (ReLU)                 (None, 8, 8, 512)    0           batch_normalization_76[0][0]     \n","__________________________________________________________________________________________________\n","up_sampling2d_22 (UpSampling2D) (None, 16, 16, 512)  0           re_lu_42[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_89 (Conv2D)              (None, 16, 16, 256)  1179648     up_sampling2d_22[0][0]           \n","__________________________________________________________________________________________________\n","batch_normalization_77 (BatchNo (None, 16, 16, 256)  1024        conv2d_89[0][0]                  \n","__________________________________________________________________________________________________\n","re_lu_43 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_77[0][0]     \n","__________________________________________________________________________________________________\n","up_sampling2d_23 (UpSampling2D) (None, 32, 32, 256)  0           re_lu_43[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_90 (Conv2D)              (None, 32, 32, 128)  294912      up_sampling2d_23[0][0]           \n","__________________________________________________________________________________________________\n","batch_normalization_78 (BatchNo (None, 32, 32, 128)  512         conv2d_90[0][0]                  \n","__________________________________________________________________________________________________\n","re_lu_44 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_78[0][0]     \n","__________________________________________________________________________________________________\n","up_sampling2d_24 (UpSampling2D) (None, 64, 64, 128)  0           re_lu_44[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_91 (Conv2D)              (None, 64, 64, 64)   73728       up_sampling2d_24[0][0]           \n","__________________________________________________________________________________________________\n","batch_normalization_79 (BatchNo (None, 64, 64, 64)   256         conv2d_91[0][0]                  \n","__________________________________________________________________________________________________\n","re_lu_45 (ReLU)                 (None, 64, 64, 64)   0           batch_normalization_79[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_92 (Conv2D)              (None, 64, 64, 3)    1728        re_lu_45[0][0]                   \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 64, 64, 3)    0           conv2d_92[0][0]                  \n","==================================================================================================\n","Total params: 10,270,400\n","Trainable params: 10,268,480\n","Non-trainable params: 1,920\n","__________________________________________________________________________________________________\n","========================================\n","Epoch is: 0\n","Number of batches:276\n","Batch:1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":["d_loss:5.30508217215538\n","g_loss:[1.0396787, 1.0094671, 0.015105804]\n","Batch:2\n","d_loss:2.3419058322906494\n","g_loss:[7.675961, 7.6480556, 0.013952805]\n","Batch:3\n","d_loss:2.1829627752304077\n","g_loss:[0.9077781, 0.8841495, 0.011814299]\n","Batch:4\n","d_loss:1.1956219375133514\n","g_loss:[1.1021438, 1.0727656, 0.014689059]\n","Batch:5\n","d_loss:1.5137208700180054\n","g_loss:[0.7946745, 0.76187897, 0.016397778]\n","Batch:6\n","d_loss:1.3732889592647552\n","g_loss:[0.79377794, 0.7534529, 0.020162534]\n","Batch:7\n","d_loss:1.2576802968978882\n","g_loss:[0.5367812, 0.49282724, 0.02197697]\n","Batch:8\n","d_loss:1.448770523071289\n","g_loss:[1.7797241, 1.7426362, 0.018543936]\n","Batch:9\n","d_loss:1.074418306350708\n","g_loss:[1.4381843, 1.4052811, 0.016451597]\n","Batch:10\n","d_loss:1.3030151277780533\n","g_loss:[0.84588116, 0.8116547, 0.017113231]\n","Batch:11\n","d_loss:1.2199333906173706\n","g_loss:[0.6830706, 0.6511285, 0.015971057]\n","Batch:12\n","d_loss:1.0173361897468567\n","g_loss:[0.6481281, 0.6168978, 0.015615143]\n","Batch:13\n","d_loss:1.0143731832504272\n","g_loss:[0.5229057, 0.5032468, 0.009829457]\n","Batch:14\n","d_loss:1.0421903431415558\n","g_loss:[0.46213827, 0.43968588, 0.011226194]\n","Batch:15\n","d_loss:0.9835126847028732\n","g_loss:[0.41086215, 0.36448756, 0.023187296]\n","Batch:16\n","d_loss:0.8090683817863464\n","g_loss:[0.42893708, 0.38239545, 0.023270817]\n","Batch:17\n","d_loss:0.8996340036392212\n","g_loss:[0.4099549, 0.36700666, 0.021474121]\n","Batch:18\n","d_loss:0.7849733382463455\n","g_loss:[0.44147354, 0.3987666, 0.021353468]\n","Batch:19\n","d_loss:0.8185520768165588\n","g_loss:[0.4658234, 0.4202568, 0.022783317]\n","Batch:20\n","d_loss:0.8313339799642563\n","g_loss:[0.40551025, 0.3766904, 0.0144099295]\n","Batch:21\n","d_loss:0.9772982001304626\n","g_loss:[0.4006487, 0.3665558, 0.017046444]\n","Batch:22\n","d_loss:0.8667296171188354\n","g_loss:[0.41753575, 0.39842856, 0.009553601]\n","Batch:23\n","d_loss:0.7938398122787476\n","g_loss:[0.45941794, 0.44211575, 0.0086511]\n","Batch:24\n","d_loss:0.817644476890564\n","g_loss:[0.45041996, 0.42594236, 0.012238802]\n","Batch:25\n","d_loss:0.710572212934494\n","g_loss:[0.37934068, 0.35640532, 0.011467686]\n","Batch:26\n","d_loss:0.747906357049942\n","g_loss:[0.3931736, 0.37278453, 0.010194544]\n","Batch:27\n","d_loss:0.7857045829296112\n","g_loss:[0.3572852, 0.33951253, 0.008886337]\n","Batch:28\n","d_loss:0.7398415803909302\n","g_loss:[0.3581475, 0.33877626, 0.009685621]\n","Batch:29\n","d_loss:0.7343313992023468\n","g_loss:[0.35498285, 0.33594018, 0.009521343]\n","Batch:30\n","d_loss:0.7323076575994492\n","g_loss:[0.36698824, 0.34886467, 0.009061784]\n","Batch:31\n","d_loss:0.7808625549077988\n","g_loss:[0.37141374, 0.35382143, 0.008796155]\n","Batch:32\n","d_loss:0.7481562793254852\n","g_loss:[0.35877904, 0.3465266, 0.0061262306]\n","Batch:33\n","d_loss:0.742742270231247\n","g_loss:[0.36680272, 0.35484377, 0.005979485]\n","Batch:34\n","d_loss:0.7957605868577957\n","g_loss:[0.39065194, 0.38066152, 0.0049952073]\n","Batch:35\n","d_loss:0.7593464553356171\n","g_loss:[0.35179448, 0.34279287, 0.0045008]\n","Batch:36\n","d_loss:0.7423744797706604\n","g_loss:[0.3869792, 0.36611795, 0.010430623]\n","Batch:37\n","d_loss:0.6973033398389816\n","g_loss:[0.40240967, 0.384883, 0.008763343]\n","Batch:38\n","d_loss:0.7651439309120178\n","g_loss:[0.43372613, 0.42332193, 0.005202101]\n","Batch:39\n","d_loss:0.8080029934644699\n","g_loss:[0.3718983, 0.36421475, 0.003841767]\n","Batch:40\n","d_loss:0.7120777666568756\n","g_loss:[0.3534574, 0.34050587, 0.006475756]\n","Batch:41\n","d_loss:0.6936679184436798\n","g_loss:[0.35979962, 0.3478037, 0.0059979567]\n","Batch:42\n","d_loss:0.7414759993553162\n","g_loss:[0.35154453, 0.33878648, 0.006379028]\n","Batch:43\n","d_loss:0.7077327519655228\n","g_loss:[0.3422023, 0.32953215, 0.006335072]\n","Batch:44\n","d_loss:0.708758533000946\n","g_loss:[0.34228116, 0.3299833, 0.0061489306]\n","Batch:45\n","d_loss:0.6511843204498291\n","g_loss:[0.34613532, 0.33300483, 0.00656524]\n","Batch:46\n","d_loss:0.6925736516714096\n","g_loss:[0.34596455, 0.3326315, 0.0066665197]\n","Batch:47\n","d_loss:0.6645967215299606\n","g_loss:[0.3542035, 0.34105235, 0.006575565]\n","Batch:48\n","d_loss:0.7060590237379074\n","g_loss:[0.34832168, 0.3359569, 0.006182389]\n","Batch:49\n","d_loss:0.6931576877832413\n","g_loss:[0.34440872, 0.33247024, 0.005969246]\n","Batch:50\n","d_loss:0.6625897884368896\n","g_loss:[0.3412529, 0.33075905, 0.0052469242]\n","Batch:51\n","d_loss:0.6401344686746597\n","g_loss:[0.33742025, 0.32944396, 0.0039881445]\n","Batch:52\n","d_loss:0.6562197506427765\n","g_loss:[0.33871764, 0.329472, 0.004622817]\n","Batch:53\n","d_loss:0.6920762062072754\n","g_loss:[0.3381147, 0.3276448, 0.005234953]\n","Batch:54\n","d_loss:0.6654409468173981\n","g_loss:[0.33908454, 0.32831192, 0.0053863116]\n","Batch:55\n","d_loss:0.6539964228868484\n","g_loss:[0.34975, 0.33349454, 0.008127741]\n","Batch:56\n","d_loss:0.7512919902801514\n","g_loss:[0.35150075, 0.3352778, 0.008111482]\n","Batch:57\n","d_loss:0.6400094628334045\n","g_loss:[0.34044215, 0.328104, 0.0061690826]\n","Batch:58\n","d_loss:0.6989428699016571\n","g_loss:[0.3427392, 0.33387125, 0.0044339816]\n","Batch:59\n","d_loss:0.6771435737609863\n","g_loss:[0.3427027, 0.33450207, 0.0041003036]\n","Batch:60\n","d_loss:0.6503562182188034\n","g_loss:[0.3444896, 0.33818695, 0.0031513344]\n","Batch:61\n","d_loss:0.672840803861618\n","g_loss:[0.3463458, 0.34035105, 0.0029973881]\n","Batch:62\n","d_loss:0.6803691536188126\n","g_loss:[0.33963165, 0.33126315, 0.0041842475]\n","Batch:63\n","d_loss:0.677944153547287\n","g_loss:[0.35004306, 0.34180975, 0.0041166614]\n","Batch:64\n","d_loss:0.6398421823978424\n","g_loss:[0.3602514, 0.3477605, 0.0062454445]\n","Batch:65\n","d_loss:0.6791692078113556\n","g_loss:[0.377118, 0.36563376, 0.00574212]\n","Batch:66\n","d_loss:0.6782482862472534\n","g_loss:[0.35457078, 0.3457079, 0.0044314344]\n","Batch:67\n","d_loss:0.6983074992895126\n","g_loss:[0.3695906, 0.36081344, 0.004388591]\n","Batch:68\n","d_loss:0.6581465005874634\n","g_loss:[0.35327137, 0.34126854, 0.0060014147]\n","Batch:69\n","d_loss:0.6480874121189117\n","g_loss:[0.3598654, 0.34936422, 0.005250587]\n","Batch:70\n","d_loss:0.668982744216919\n","g_loss:[0.35538045, 0.347542, 0.003919229]\n","Batch:71\n","d_loss:0.6325294673442841\n","g_loss:[0.3402536, 0.33243525, 0.0039091683]\n","Batch:72\n","d_loss:0.6795197576284409\n","g_loss:[0.34972784, 0.33824557, 0.005741137]\n","Batch:73\n","d_loss:0.6097742468118668\n","g_loss:[0.3485967, 0.33871126, 0.0049427142]\n","Batch:74\n","d_loss:0.6460479348897934\n","g_loss:[0.34162897, 0.3324535, 0.004587736]\n","Batch:75\n","d_loss:0.6179487556219101\n","g_loss:[0.34208533, 0.3346488, 0.0037182705]\n","Batch:76\n","d_loss:0.6345489323139191\n","g_loss:[0.3417917, 0.3333329, 0.0042293957]\n","Batch:77\n","d_loss:0.634083941578865\n","g_loss:[0.33842504, 0.33092207, 0.0037514898]\n","Batch:78\n","d_loss:0.6614224165678024\n","g_loss:[0.3376787, 0.32903686, 0.0043209265]\n","Batch:79\n","d_loss:0.6228879243135452\n","g_loss:[0.33639643, 0.32887143, 0.003762494]\n","Batch:80\n","d_loss:0.658165454864502\n","g_loss:[0.3343144, 0.32800642, 0.0031539886]\n","Batch:81\n","d_loss:0.6216355860233307\n","g_loss:[0.33379558, 0.3276334, 0.0030810814]\n","Batch:82\n","d_loss:0.6520468592643738\n","g_loss:[0.33803493, 0.33125097, 0.0033919862]\n","Batch:83\n","d_loss:0.6438168287277222\n","g_loss:[0.33383763, 0.32826868, 0.0027844778]\n","Batch:84\n","d_loss:0.6335209161043167\n","g_loss:[0.36090565, 0.35405967, 0.003422987]\n","Batch:85\n","d_loss:0.6126589179039001\n","g_loss:[0.5754485, 0.56800675, 0.003720887]\n","Batch:86\n","d_loss:0.6808283478021622\n","g_loss:[0.45576647, 0.445233, 0.005266738]\n","Batch:87\n","d_loss:0.6789060682058334\n","g_loss:[0.5471754, 0.5412395, 0.0029679425]\n","Batch:88\n","d_loss:0.6726806908845901\n","g_loss:[0.48565298, 0.47898647, 0.0033332622]\n","Batch:89\n","d_loss:0.6135221272706985\n","g_loss:[0.45300743, 0.44442225, 0.0042925887]\n","Batch:90\n","d_loss:0.6477619707584381\n","g_loss:[0.360283, 0.3520574, 0.004112792]\n","Batch:91\n","d_loss:0.6425613611936569\n","g_loss:[0.35014543, 0.34275544, 0.003694989]\n","Batch:92\n","d_loss:0.5989420861005783\n","g_loss:[0.34927112, 0.3444665, 0.0024023014]\n","Batch:93\n","d_loss:0.6117261797189713\n","g_loss:[0.33894134, 0.33339977, 0.0027707745]\n","Batch:94\n","d_loss:0.6179386973381042\n","g_loss:[0.34186256, 0.33409542, 0.0038835728]\n","Batch:95\n","d_loss:0.6673938482999802\n","g_loss:[0.3370689, 0.33065754, 0.0032056714]\n","Batch:96\n","d_loss:0.6376918703317642\n","g_loss:[0.34317562, 0.33798864, 0.0025934912]\n","Batch:97\n","d_loss:0.6665677428245544\n","g_loss:[0.3340971, 0.32783288, 0.0031321007]\n","Batch:98\n","d_loss:0.628465011715889\n","g_loss:[0.3447224, 0.33691627, 0.003903057]\n","Batch:99\n","d_loss:0.5879640579223633\n","g_loss:[0.36360872, 0.35814518, 0.0027317773]\n","Batch:100\n","d_loss:0.6305264830589294\n","g_loss:[0.3405033, 0.33592507, 0.0022891099]\n","Batch:101\n","d_loss:0.645993784070015\n","g_loss:[0.3467571, 0.34271228, 0.0020224103]\n","Batch:102\n","d_loss:0.6085629761219025\n","g_loss:[0.34077993, 0.33575276, 0.0025135835]\n","Batch:103\n","d_loss:0.6288454830646515\n","g_loss:[0.33500624, 0.33115926, 0.0019234836]\n","Batch:104\n","d_loss:0.6125827431678772\n","g_loss:[0.3386685, 0.33520105, 0.0017337134]\n","Batch:105\n","d_loss:0.6388263255357742\n","g_loss:[0.33291775, 0.3282631, 0.002327328]\n","Batch:106\n","d_loss:0.6300377249717712\n","g_loss:[0.3335383, 0.32835895, 0.0025896789]\n","Batch:107\n","d_loss:0.6217364519834518\n","g_loss:[0.3315533, 0.32724404, 0.0021546325]\n","Batch:108\n","d_loss:0.6081757992506027\n","g_loss:[0.3324851, 0.32773143, 0.0023768414]\n","Batch:109\n","d_loss:0.6342138200998306\n","g_loss:[0.3329052, 0.32755056, 0.002677319]\n","Batch:110\n","d_loss:0.6039244830608368\n","g_loss:[0.33113116, 0.3258599, 0.0026356257]\n","Batch:111\n","d_loss:0.6304775029420853\n","g_loss:[0.3343027, 0.32740813, 0.0034472786]\n","Batch:112\n","d_loss:0.6305553019046783\n","g_loss:[0.3391413, 0.33136117, 0.0038900715]\n","Batch:113\n","d_loss:0.616857036948204\n","g_loss:[0.3360893, 0.32884917, 0.0036200664]\n","Batch:114\n","d_loss:0.6090273708105087\n","g_loss:[0.33292416, 0.32726264, 0.002830755]\n","Batch:115\n","d_loss:0.6172353625297546\n","g_loss:[0.33316717, 0.32747227, 0.0028474452]\n","Batch:116\n","d_loss:0.5919051021337509\n","g_loss:[0.33090836, 0.32610074, 0.0024038064]\n","Batch:117\n","d_loss:0.6318067908287048\n","g_loss:[0.33186603, 0.32693297, 0.0024665315]\n","Batch:118\n","d_loss:0.5994260311126709\n","g_loss:[0.33288628, 0.3272525, 0.0028168927]\n","Batch:119\n","d_loss:0.6230458170175552\n","g_loss:[0.33122173, 0.32590055, 0.002660593]\n","Batch:120\n","d_loss:0.6265880912542343\n","g_loss:[0.3328888, 0.32812962, 0.002379593]\n","Batch:121\n","d_loss:0.6459560692310333\n","g_loss:[0.33184233, 0.32787856, 0.0019818863]\n","Batch:122\n","d_loss:0.6344418376684189\n","g_loss:[0.33019543, 0.32688823, 0.0016535978]\n","Batch:123\n","d_loss:0.62173131108284\n","g_loss:[0.3323192, 0.32866895, 0.0018251231]\n","Batch:124\n","d_loss:0.5921184718608856\n","g_loss:[0.33215815, 0.32901108, 0.0015735368]\n","Batch:125\n","d_loss:0.6252581030130386\n","g_loss:[0.33817935, 0.33427256, 0.001953396]\n","Batch:126\n","d_loss:0.5866344124078751\n","g_loss:[0.33174664, 0.32663184, 0.0025573988]\n","Batch:127\n","d_loss:0.606934517621994\n","g_loss:[0.3328029, 0.32656518, 0.0031188638]\n","Batch:128\n","d_loss:0.6284775137901306\n","g_loss:[0.3325415, 0.3271463, 0.002697601]\n","Batch:129\n","d_loss:0.6048660576343536\n","g_loss:[0.3363768, 0.3307967, 0.0027900427]\n","Batch:130\n","d_loss:0.629081979393959\n","g_loss:[0.33122855, 0.3262685, 0.0024800245]\n","Batch:131\n","d_loss:0.6070189028978348\n","g_loss:[0.33483112, 0.33063602, 0.002097553]\n","Batch:132\n","d_loss:0.5985222309827805\n","g_loss:[0.34381926, 0.33994293, 0.00193816]\n","Batch:133\n","d_loss:0.5988606512546539\n","g_loss:[0.36445826, 0.36104318, 0.0017075324]\n","Batch:134\n","d_loss:0.6216144859790802\n","g_loss:[0.3591699, 0.35501808, 0.002075918]\n","Batch:135\n","d_loss:0.6047761142253876\n","g_loss:[0.3474348, 0.34378645, 0.0018241673]\n","Batch:136\n","d_loss:0.6271751075983047\n","g_loss:[0.3375937, 0.33403957, 0.0017770617]\n","Batch:137\n","d_loss:0.5909149646759033\n","g_loss:[0.33097088, 0.32840425, 0.0012833198]\n","Batch:138\n","d_loss:0.5927414447069168\n","g_loss:[0.3306954, 0.32785475, 0.0014203195]\n","Batch:139\n","d_loss:0.5988958030939102\n","g_loss:[0.32963115, 0.32713214, 0.0012495061]\n","Batch:140\n","d_loss:0.623818501830101\n","g_loss:[0.3310892, 0.3273741, 0.0018575522]\n","Batch:141\n","d_loss:0.603292390704155\n","g_loss:[0.33249477, 0.32737628, 0.0025592418]\n","Batch:142\n","d_loss:0.5699170976877213\n","g_loss:[0.33438936, 0.33012158, 0.0021338975]\n","Batch:143\n","d_loss:0.6273613125085831\n","g_loss:[0.3323036, 0.32853606, 0.0018837759]\n","Batch:144\n","d_loss:0.6144406944513321\n","g_loss:[0.33189452, 0.32809973, 0.001897392]\n","Batch:145\n","d_loss:0.6003367602825165\n","g_loss:[0.33261016, 0.32917964, 0.0017152588]\n","Batch:146\n","d_loss:0.6117106378078461\n","g_loss:[0.33107102, 0.32768625, 0.0016923838]\n","Batch:147\n","d_loss:0.606184795498848\n","g_loss:[0.33084202, 0.3279274, 0.0014573021]\n","Batch:148\n","d_loss:0.5720284134149551\n","g_loss:[0.33431885, 0.33136845, 0.0014752061]\n","Batch:149\n","d_loss:0.6396710723638535\n","g_loss:[0.33188686, 0.32771975, 0.002083555]\n","Batch:150\n","d_loss:0.5890741497278214\n","g_loss:[0.33211386, 0.3276676, 0.0022231385]\n","Batch:151\n","d_loss:0.6087369471788406\n","g_loss:[0.32970005, 0.32679653, 0.0014517547]\n","Batch:152\n","d_loss:0.593193918466568\n","g_loss:[0.33130077, 0.32766858, 0.0018160867]\n","Batch:153\n","d_loss:0.6067162454128265\n","g_loss:[0.32954007, 0.32618323, 0.0016784235]\n","Batch:154\n","d_loss:0.6044249832630157\n","g_loss:[0.33140168, 0.32841152, 0.0014950822]\n","Batch:155\n","d_loss:0.5971437990665436\n","g_loss:[0.33154598, 0.32850617, 0.0015198966]\n","Batch:156\n","d_loss:0.6089831739664078\n","g_loss:[0.33139896, 0.32764232, 0.0018783261]\n","Batch:157\n","d_loss:0.5941507518291473\n","g_loss:[0.33325022, 0.32994592, 0.0016521534]\n","Batch:158\n","d_loss:0.6003165543079376\n","g_loss:[0.33130252, 0.32724482, 0.002028852]\n","Batch:159\n","d_loss:0.5911034643650055\n","g_loss:[0.3304832, 0.3268522, 0.0018154972]\n","Batch:160\n","d_loss:0.6077651828527451\n","g_loss:[0.33000538, 0.32634568, 0.0018298429]\n","Batch:161\n","d_loss:0.5816676318645477\n","g_loss:[0.33449036, 0.33041388, 0.00203824]\n","Batch:162\n","d_loss:0.5887560546398163\n","g_loss:[0.33021402, 0.32662725, 0.0017933844]\n","Batch:163\n","d_loss:0.6064383536577225\n","g_loss:[0.32874104, 0.32589167, 0.0014246787]\n","Batch:164\n","d_loss:0.6155063360929489\n","g_loss:[0.33318737, 0.32969737, 0.0017450057]\n","Batch:165\n","d_loss:0.5915867984294891\n","g_loss:[0.33758596, 0.33463076, 0.0014776054]\n","Batch:166\n","d_loss:0.5814025849103928\n","g_loss:[0.33215567, 0.32911134, 0.0015221725]\n","Batch:167\n","d_loss:0.586750864982605\n","g_loss:[0.3318527, 0.3278259, 0.0020133962]\n","Batch:168\n","d_loss:0.5732700079679489\n","g_loss:[0.3312582, 0.32790053, 0.0016788364]\n","Batch:169\n","d_loss:0.5866904705762863\n","g_loss:[0.33287916, 0.32934448, 0.0017673312]\n","Batch:170\n","d_loss:0.5809901058673859\n","g_loss:[0.32895288, 0.326175, 0.0013889421]\n","Batch:171\n","d_loss:0.5900410860776901\n","g_loss:[0.33055088, 0.32803345, 0.0012587139]\n","Batch:172\n","d_loss:0.5818538218736649\n","g_loss:[0.32904506, 0.32621598, 0.0014145374]\n","Batch:173\n","d_loss:0.6357023268938065\n","g_loss:[0.33251423, 0.32950026, 0.001506984]\n","Batch:174\n","d_loss:0.5851067900657654\n","g_loss:[0.33077535, 0.3268187, 0.001978327]\n","Batch:175\n","d_loss:0.5938801467418671\n","g_loss:[0.3297895, 0.3263918, 0.0016988575]\n","Batch:176\n","d_loss:0.6013200879096985\n","g_loss:[0.33292758, 0.32977515, 0.0015762097]\n","Batch:177\n","d_loss:0.5745921432971954\n","g_loss:[0.33235684, 0.32829833, 0.0020292532]\n","Batch:178\n","d_loss:0.6432162821292877\n","g_loss:[0.33140767, 0.3278259, 0.0017908821]\n","Batch:179\n","d_loss:0.6003354638814926\n","g_loss:[0.35230157, 0.3489876, 0.0016569734]\n","Batch:180\n","d_loss:0.5852456539869308\n","g_loss:[0.5562859, 0.5526391, 0.0018233943]\n","Batch:181\n","d_loss:0.5995681285858154\n","g_loss:[1.5699042, 1.5672381, 0.0013330393]\n","Batch:182\n","d_loss:0.620515763759613\n","g_loss:[1.0784668, 1.0756283, 0.0014192732]\n","Batch:183\n","d_loss:0.6827631890773773\n","g_loss:[0.94251436, 0.9373508, 0.0025817745]\n","Batch:184\n","d_loss:0.6869405657052994\n","g_loss:[0.7280084, 0.72320956, 0.0023994218]\n","Batch:185\n","d_loss:0.6423993110656738\n","g_loss:[0.6054878, 0.6004523, 0.00251775]\n","Batch:186\n","d_loss:0.6279622912406921\n","g_loss:[0.56812906, 0.5613022, 0.0034134397]\n","Batch:187\n","d_loss:0.6044782996177673\n","g_loss:[0.5089284, 0.50260353, 0.0031624525]\n","Batch:188\n","d_loss:0.613819420337677\n","g_loss:[0.5004702, 0.49645925, 0.0020054844]\n","Batch:189\n","d_loss:0.600376769900322\n","g_loss:[0.49900046, 0.49590257, 0.0015489479]\n","Batch:190\n","d_loss:0.5766976326704025\n","g_loss:[0.4983287, 0.4952247, 0.0015519814]\n","Batch:191\n","d_loss:0.5828135162591934\n","g_loss:[0.50734735, 0.5034615, 0.0019429359]\n","Batch:192\n","d_loss:0.5800832957029343\n","g_loss:[0.49370542, 0.48994815, 0.0018786395]\n","Batch:193\n","d_loss:0.5956930965185165\n","g_loss:[0.47933233, 0.47487038, 0.0022309755]\n","Batch:194\n","d_loss:0.5886087119579315\n","g_loss:[0.46980733, 0.46601433, 0.0018964957]\n","Batch:195\n","d_loss:0.5818180739879608\n","g_loss:[0.5001747, 0.49608508, 0.0020448263]\n","Batch:196\n","d_loss:0.604055181145668\n","g_loss:[0.4766225, 0.47245625, 0.0020831162]\n","Batch:197\n","d_loss:0.6202361583709717\n","g_loss:[0.45655537, 0.45280358, 0.0018758855]\n","Batch:198\n","d_loss:0.574771448969841\n","g_loss:[0.47173348, 0.46770704, 0.0020132153]\n","Batch:199\n","d_loss:0.5810211598873138\n","g_loss:[0.4253651, 0.42112762, 0.002118743]\n","Batch:200\n","d_loss:0.5827004611492157\n","g_loss:[0.36292955, 0.35975915, 0.0015852078]\n","Batch:201\n","d_loss:0.6005821824073792\n","g_loss:[0.35257727, 0.34945628, 0.0015604935]\n","Batch:202\n","d_loss:0.5941920578479767\n","g_loss:[0.3695957, 0.36645386, 0.0015709314]\n","Batch:203\n","d_loss:0.5844736844301224\n","g_loss:[0.39263257, 0.38935924, 0.0016366639]\n","Batch:204\n","d_loss:0.6065040677785873\n","g_loss:[0.34794137, 0.34541997, 0.0012607006]\n","Batch:205\n","d_loss:0.626544401049614\n","g_loss:[0.35342097, 0.35028094, 0.0015700095]\n","Batch:206\n","d_loss:0.6025726944208145\n","g_loss:[0.36632702, 0.36336708, 0.0014799756]\n","Batch:207\n","d_loss:0.6072516590356827\n","g_loss:[0.3773745, 0.3741567, 0.0016088984]\n","Batch:208\n","d_loss:0.6071575582027435\n","g_loss:[0.3902273, 0.38723958, 0.00149385]\n","Batch:209\n","d_loss:0.6052066087722778\n","g_loss:[0.40903398, 0.40663648, 0.0011987515]\n","Batch:210\n","d_loss:0.6109094470739365\n","g_loss:[0.3611291, 0.35897443, 0.001077336]\n","Batch:211\n","d_loss:0.6083279699087143\n","g_loss:[0.4400275, 0.43763575, 0.0011958821]\n","Batch:212\n","d_loss:0.5903787463903427\n","g_loss:[0.4132564, 0.4104079, 0.0014242518]\n","Batch:213\n","d_loss:0.6114978790283203\n","g_loss:[0.3720303, 0.36841443, 0.0018079267]\n","Batch:214\n","d_loss:0.6007811576128006\n","g_loss:[0.37548697, 0.37253246, 0.0014772528]\n","Batch:215\n","d_loss:0.627111405134201\n","g_loss:[0.37182033, 0.3695287, 0.0011458166]\n","Batch:216\n","d_loss:0.5781543850898743\n","g_loss:[0.36094138, 0.3583106, 0.0013153835]\n","Batch:217\n","d_loss:0.6073319166898727\n","g_loss:[0.3771969, 0.3741317, 0.0015325923]\n","Batch:218\n","d_loss:0.6103705316781998\n","g_loss:[0.35568878, 0.3528688, 0.0014099921]\n","Batch:219\n","d_loss:0.5950488448143005\n","g_loss:[0.34747913, 0.34478953, 0.0013447979]\n","Batch:220\n","d_loss:0.6183407157659531\n","g_loss:[0.39037988, 0.38681298, 0.001783445]\n","Batch:221\n","d_loss:0.5865444391965866\n","g_loss:[0.37246826, 0.3690719, 0.001698175]\n","Batch:222\n","d_loss:0.5968978554010391\n","g_loss:[0.40580404, 0.40221715, 0.001793451]\n","Batch:223\n","d_loss:0.5937716960906982\n","g_loss:[0.41141897, 0.4075487, 0.0019351384]\n","Batch:224\n","d_loss:0.6378026753664017\n","g_loss:[0.42161912, 0.41772082, 0.0019491416]\n","Batch:225\n","d_loss:0.6287471354007721\n","g_loss:[0.3794056, 0.3758076, 0.0017989883]\n","Batch:226\n","d_loss:0.6314593255519867\n","g_loss:[0.38468525, 0.38138467, 0.0016502899]\n","Batch:227\n","d_loss:0.6041557192802429\n","g_loss:[0.3934057, 0.3905666, 0.0014195661]\n","Batch:228\n","d_loss:0.6022614240646362\n","g_loss:[0.37065122, 0.36705393, 0.0017986511]\n","Batch:229\n","d_loss:0.6007669866085052\n","g_loss:[0.35411724, 0.35061264, 0.0017522983]\n","Batch:230\n","d_loss:0.5735755115747452\n","g_loss:[0.37957078, 0.37594205, 0.0018143661]\n","Batch:231\n","d_loss:0.6225439459085464\n","g_loss:[0.37158346, 0.3678525, 0.0018654775]\n","Batch:232\n","d_loss:0.6718648374080658\n","g_loss:[0.37180692, 0.36816883, 0.0018190389]\n","Batch:233\n","d_loss:0.6056828945875168\n","g_loss:[0.3573287, 0.35449725, 0.0014157254]\n","Batch:234\n","d_loss:0.6042665094137192\n","g_loss:[0.34676003, 0.344454, 0.0011530251]\n","Batch:235\n","d_loss:0.6281943172216415\n","g_loss:[0.34846917, 0.34520954, 0.0016298105]\n","Batch:236\n","d_loss:0.5938254445791245\n","g_loss:[0.3479609, 0.34564406, 0.0011584226]\n","Batch:237\n","d_loss:0.6023663878440857\n","g_loss:[0.34096402, 0.33834583, 0.0013090946]\n","Batch:238\n","d_loss:0.5772696733474731\n","g_loss:[0.3629533, 0.36002576, 0.0014637737]\n","Batch:239\n","d_loss:0.6042367219924927\n","g_loss:[0.4423021, 0.4389245, 0.0016888136]\n","Batch:240\n","d_loss:0.5917729437351227\n","g_loss:[0.46590212, 0.46281344, 0.0015443461]\n","Batch:241\n","d_loss:0.6160966902971268\n","g_loss:[0.34461564, 0.3417642, 0.0014257072]\n","Batch:242\n","d_loss:0.6025941520929337\n","g_loss:[0.3560004, 0.35235634, 0.0018220303]\n","Batch:243\n","d_loss:0.5961886048316956\n","g_loss:[0.36112627, 0.358599, 0.0012636387]\n","Batch:244\n","d_loss:0.6216524839401245\n","g_loss:[0.36628845, 0.36380363, 0.0012424195]\n","Batch:245\n","d_loss:0.5973333716392517\n","g_loss:[0.34121597, 0.33841944, 0.0013982654]\n","Batch:246\n","d_loss:0.6203454881906509\n","g_loss:[0.33807102, 0.33603495, 0.0010180289]\n","Batch:247\n","d_loss:0.5942061990499496\n","g_loss:[0.3599126, 0.35746855, 0.0012220342]\n","Batch:248\n","d_loss:0.5996428430080414\n","g_loss:[0.33577532, 0.33352658, 0.0011243646]\n","Batch:249\n","d_loss:0.5849824249744415\n","g_loss:[0.34152, 0.33852598, 0.0014970154]\n","Batch:250\n","d_loss:0.5889259427785873\n","g_loss:[0.3355406, 0.33266866, 0.0014359656]\n","Batch:251\n","d_loss:0.6172074228525162\n","g_loss:[0.3415598, 0.3387376, 0.0014111012]\n","Batch:252\n","d_loss:0.5941056162118912\n","g_loss:[0.3417899, 0.33900437, 0.0013927608]\n","Batch:253\n","d_loss:0.6074080914258957\n","g_loss:[0.57053655, 0.5680729, 0.0012318182]\n","Batch:254\n","d_loss:0.6817552894353867\n","g_loss:[0.9918484, 0.9885854, 0.0016315121]\n","Batch:255\n","d_loss:0.6228818744421005\n","g_loss:[0.7620097, 0.7585218, 0.0017439312]\n","Batch:256\n","d_loss:0.6538533568382263\n","g_loss:[0.8474415, 0.84366703, 0.0018872467]\n","Batch:257\n","d_loss:0.6193518340587616\n","g_loss:[0.6551697, 0.65167975, 0.0017449873]\n","Batch:258\n","d_loss:0.6283610761165619\n","g_loss:[0.4895844, 0.48552406, 0.0020301705]\n","Batch:259\n","d_loss:0.5864177942276001\n","g_loss:[0.4445154, 0.4417974, 0.001359003]\n","Batch:260\n","d_loss:0.5994401723146439\n","g_loss:[0.4610134, 0.4581253, 0.0014440565]\n","Batch:261\n","d_loss:0.6123405992984772\n","g_loss:[0.41636235, 0.41334367, 0.0015093435]\n","Batch:262\n","d_loss:0.6027776747941971\n","g_loss:[0.42189828, 0.4188296, 0.0015343487]\n","Batch:263\n","d_loss:0.5816502422094345\n","g_loss:[0.40567863, 0.40326306, 0.0012077764]\n","Batch:264\n","d_loss:0.5927222520112991\n","g_loss:[0.38282934, 0.38084996, 0.0009896833]\n","Batch:265\n","d_loss:0.5933697074651718\n","g_loss:[0.35772943, 0.3556623, 0.0010335747]\n","Batch:266\n","d_loss:0.5749353319406509\n","g_loss:[0.3399478, 0.3376272, 0.0011602868]\n","Batch:267\n","d_loss:0.5734533071517944\n","g_loss:[0.34012625, 0.33765614, 0.0012350531]\n","Batch:268\n","d_loss:0.5778988897800446\n","g_loss:[0.345212, 0.3428199, 0.0011960517]\n","Batch:269\n","d_loss:0.5935987830162048\n","g_loss:[0.34472412, 0.34249005, 0.0011170347]\n","Batch:270\n","d_loss:0.5809329450130463\n","g_loss:[0.33632082, 0.33436477, 0.000978026]\n","Batch:271\n","d_loss:0.571698933839798\n","g_loss:[0.35397944, 0.35184765, 0.0010658888]\n","Batch:272\n","d_loss:0.5946023166179657\n","g_loss:[0.35519814, 0.3532297, 0.0009842289]\n","Batch:273\n","d_loss:0.5846971273422241\n","g_loss:[0.35334152, 0.35147193, 0.0009347907]\n","Batch:274\n","d_loss:0.5768111348152161\n","g_loss:[0.35181904, 0.35003483, 0.00089209643]\n","Batch:275\n","d_loss:0.5856135487556458\n","g_loss:[0.3388528, 0.33672196, 0.0010654116]\n","Batch:276\n","d_loss:0.5840362012386322\n","g_loss:[0.3391703, 0.33657062, 0.0012998452]\n"],"name":"stdout"},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["========================================\n","Epoch is: 1\n","Number of batches:276\n","Batch:1\n","d_loss:0.5700252205133438\n","g_loss:[0.33971837, 0.3372423, 0.0012380327]\n","Batch:2\n","d_loss:0.5870311707258224\n","g_loss:[0.33610025, 0.33395848, 0.0010708929]\n","Batch:3\n","d_loss:0.5838400423526764\n","g_loss:[0.33240095, 0.3301862, 0.0011073791]\n","Batch:4\n","d_loss:0.5942967981100082\n","g_loss:[0.33384475, 0.33119792, 0.0013234112]\n","Batch:5\n","d_loss:0.5970092266798019\n","g_loss:[0.3352159, 0.33279997, 0.0012079615]\n","Batch:6\n","d_loss:0.578727975487709\n","g_loss:[0.34273225, 0.3401934, 0.0012694318]\n","Batch:7\n","d_loss:0.6141554713249207\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-be9aff93f5cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m             \"\"\"\n\u001b[1;32m    132\u001b[0m             g_loss = adversarial_model.train_on_batch([embedding_batch, z_noise, compressed_embedding],\n\u001b[0;32m--> 133\u001b[0;31m                                                                 [K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"g_loss:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}