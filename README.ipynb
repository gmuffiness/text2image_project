{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4조StackGAN_PT.ipynb","provenance":[],"collapsed_sections":["6J2bb5kUjFlO"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dKB_cti8mnrr","colab_type":"text"},"source":["# **Team4      권성수, 문대정, 이재철, 조은경**"]},{"cell_type":"markdown","metadata":{"id":"9fwxRLZgg2_a","colab_type":"text"},"source":["<h1>Text2Image implementation with StackGAN</h1>"]},{"cell_type":"markdown","metadata":{"id":"pM9x5B-5t9x2","colab_type":"text"},"source":["# Motivation for the project and an explanation of the problem statement"]},{"cell_type":"markdown","metadata":{"id":"PT4rZUXlLS__","colab_type":"text"},"source":["<h2>The power of visualization</h2>"]},{"cell_type":"markdown","metadata":{"id":"27wczALILlXQ","colab_type":"text"},"source":["* 인간은 시각화된 자료를 쉽게 인지할 수 있다고 함.\n","* 오감 중에서 시각이 70~80%의 정보를 습득하는 수용체\n","* 시각화가 가능해지면 \"Read\"가 아닌, \"See\"의 방법으로 더 빨리 정보를 받아들일 수 있게 됨."]},{"cell_type":"markdown","metadata":{"id":"CimENp_Ntshn","colab_type":"text"},"source":["![대체 텍스트](https://i.ibb.co/CMwcnZb/num-dummy.png)"]},{"cell_type":"markdown","metadata":{"id":"hc0cdmOuL7Sf","colab_type":"text"},"source":["<h2>Text2Image? => StackGAN</h2>"]},{"cell_type":"markdown","metadata":{"id":"E3Qb5DS6MSwr","colab_type":"text"},"source":["![대체 텍스트](https://miro.medium.com/max/3356/1*g-0onhpbu6dU0aZbpfEUeA.jpeg)"]},{"cell_type":"markdown","metadata":{"id":"DmMMPJz3uBRe","colab_type":"text"},"source":["#A description of the data"]},{"cell_type":"markdown","metadata":{"id":"DaiGEhKHulzC","colab_type":"text"},"source":["![대체 텍스트](https://vision.cornell.edu/se3/wp-content/uploads/2017/04/Screenshot-from-2017-04-29-16-36-17-705x456.png)"]},{"cell_type":"markdown","metadata":{"id":"XAqfWnIAJJ0V","colab_type":"text"},"source":["* Caltech-UCSD Birds 200\n","* 총 200개의 다른 종으로 이루어진 11,788장의 이미지\n","* 각 이미지는 새를 segmentation하고, 속성이 어떠한지 카테고리별로 설명되어있음.\n","* ex) 부위별 색상은 어떤 색인지, 사이즈는 small/medium/large인지, 부위별 패턴은 어떤지 영단어로 표시되어 있음."]},{"cell_type":"markdown","metadata":{"id":"D79QOniOuxC2","colab_type":"text"},"source":["# Hyperparameter and architecture choices that were explored"]},{"cell_type":"markdown","metadata":{"id":"MEV2aVXQw_wI","colab_type":"text"},"source":["<h2>Idea</h2>"]},{"cell_type":"markdown","metadata":{"id":"yV3SGSyXxDkl","colab_type":"text"},"source":["* 기존의 Text to image : 주어진 문장을 기반으로 하나의 GAN을 통해 이미지를 생성함\n","* Text to image는 어려운 문제이므로 **두가지 sub problem으로 나누어 보다 고해상도의 이미지를 생성하자!**"]},{"cell_type":"markdown","metadata":{"id":"ZJSi8ckuys-a","colab_type":"text"},"source":["<h2>StackGAN Model Architecture</h2>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kyDa_3EegN33","colab_type":"text"},"source":["![대체 텍스트](https://miro.medium.com/max/4756/1*NwDrP1Zi6xj1bGN62wrf3g.jpeg)"]},{"cell_type":"markdown","metadata":{"id":"m9AprKuDyQks","colab_type":"text"},"source":["<h2>StackGAN Model - Conditioning Augmentation</h2>\n","\n","* 한정적인 데이터가 고차원의 text embedding 공간에서 불연속성을 야기하는 문제점을 해결하기 위해 도입한 구조.\n","\n","* $N(\\mu(\\rho_t)),\\sigma(\\rho_t))$ 에서 샘플링한 conditioning variable을 text feature로 사용하여 generator의 input에 추가시킴.\n","\n","* Gaussian distribution에서 샘플링한 text embedding을 사용하므로 randomness가 더해져 동일한 문장에 대해서도 다양한 이미지를 생성할 수 있는 효과가 있음.\n","\n","* 해당 방법은 generator의 loss에 $D_K$$_L(N(\\mu(\\rho_t)),\\sigma(\\rho_t))||N(0,1))$를 추가함으로서 가능함.\n","\n","\n","![대체 텍스트](https://i.ibb.co/vH2D052/1.pnghttps://)"]},{"cell_type":"markdown","metadata":{"id":"dMuLGixi2Gxl","colab_type":"text"},"source":["<h2>StackGAN Model - Stage1</h2>\n","\n","* GAN을 이용해 text에 대한 초기 shape과 color를 나타내는 저화질 이미지를 1차적으로 생성하는 단계\n","\n","* Generator : conditioning variable과 noise로부터 저화질의 초기 이미지를 생성함.\n","\n","* Discriminator : image와 text feature를 기반으로 real / fake 판단.\n","\n","\n","![대체 텍스트](https://i.ibb.co/R3t6hLK/2.png)"]},{"cell_type":"markdown","metadata":{"id":"V-bSYENN49Tl","colab_type":"text"},"source":["<h2>StackGAN Model - Stage2</h2>\n","\n","* Stage1에서 생성한 이미지를 수정하고 추가적으로 디테일한 부분을 생성하는 단계.\n","\n","* Generator : conditioning variable과 stage1의 결과로부터 보다 자세한 고화질 이미지 생성.\n","\n","* Discriminator : image와 text feature를 기반으로 real / fake 판단.\n","\n","\n","![대체 텍스트](https://i.ibb.co/C5pqPcP/4.png)"]},{"cell_type":"markdown","metadata":{"id":"XKhAeg4Ir92N","colab_type":"text"},"source":["# 구현 방법\n","1. Batch normalization X + Adam optimizer 사용\n","2. Batch normalization X + RMSprop optimzer 사용\n","3. Batch normalization X + RMSprop optimizer + Wassertein Loss 변경\n","4. Batch normalization O + Adam optimizer 사용 \n","5. Batch normalization O + Adam optimizer + Wassertein Loss 변경 + Dropout 추가\n","6. Learning rate 기존의 10배로 학습 진행 \n","\n"]},{"cell_type":"markdown","metadata":{"id":"FRZ_pW51iqjZ","colab_type":"text"},"source":["# Code implementation"]},{"cell_type":"markdown","metadata":{"id":"_WDHPr98ufO0","colab_type":"text"},"source":["> ## Mount / Extract"]},{"cell_type":"code","metadata":{"id":"PiloYx6yuepo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1592656585240,"user_tz":-540,"elapsed":17251,"user":{"displayName":"문대정","photoUrl":"","userId":"12686300660117063511"}},"outputId":"6a2da32b-e9e4-4fac-9694-d8471591e4bf"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jvr720CLuu_4","colab_type":"code","colab":{}},"source":["import os\n","import tarfile\n","\n","fname = '/content/gdrive/My Drive/dl_teamproject_folder/CUB_200_2011/CUB_200_2011.tgz'  # 압축 파일을 지정해주고   \n","ap = tarfile.open(fname)      # 열어줍니다. \n","\n","ap.extractall('/content/gdrive/My Drive/dl_teamproject_folder/CUB_200_2011')         # 그리고는 압축을 풀어줍니다. \n","# () 안에는 풀고 싶은 경로를 넣어주면 되요. 비워둘 경우 현재 경로에 압축 풉니다. \n"," \n","ap.close()  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z1aTf9Z_iwCu","colab_type":"text"},"source":["> ## Importing Libraries"]},{"cell_type":"code","metadata":{"id":"aR7xUawQjLCg","colab_type":"code","colab":{}},"source":["import os\n","import pickle\n","import random\n","import time\n","\n","import PIL\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from PIL import Image\n","from keras import Input, Model\n","from keras import backend as K\n","from keras.callbacks import TensorBoard\n","from keras.layers import Dense, LeakyReLU, BatchNormalization, ReLU, Reshape, UpSampling2D, Conv2D, Activation, \\\n","    concatenate, Flatten, Lambda, Concatenate, ZeroPadding2D, Dropout\n","from keras.optimizers import Adam\n","from matplotlib import pyplot as plt\n","from keras.layers import add"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6J2bb5kUjFlO","colab_type":"text"},"source":["> ## Loading of Dataset"]},{"cell_type":"code","metadata":{"id":"xZe324PhjOAp","colab_type":"code","colab":{}},"source":["#pickle(텍스트가 아닌 객체 자체인 파일 ex.list) 불러오기 위한 함수 (labels저장된 파일)\n","def load_class_ids(class_info_file_path):\n","    with open(class_info_file_path, 'rb') as f:\n","        class_ids = pickle.load(f, encoding='latin1')\n","        return class_ids\n","       \n","#임베딩     \n","def load_embeddings(embeddings_file_path):\n","    with open(embeddings_file_path, 'rb') as f:\n","        embeddings = pickle.load(f, encoding='latin1')\n","        embeddings = np.array(embeddings)\n","        print('embeddings: ', embeddings.shape)\n","    return embeddings\n","\n","#pickle 파일이름 불러오기\n","def load_filenames(filenames_file_path):\n","    with open(filenames_file_path, 'rb') as f:\n","        filenames = pickle.load(f, encoding='latin1')\n","    return filenames\n","\n","#image detection위한 bounding box(바운딩 박스와 일치하는 파일의 dictionary 불러오기) \n","def load_bounding_boxes(dataset_dir):\n","    # Paths\n","    bounding_boxes_path = os.path.join(dataset_dir, 'bounding_boxes.txt')\n","    file_paths_path = os.path.join(dataset_dir, 'images.txt')\n","\n","    # Read bounding_boxes.txt and images.txt file\n","    df_bounding_boxes = pd.read_csv(bounding_boxes_path,\n","                                    delim_whitespace=True, header=None).astype(int) #delim_whitespace : 공백으로 구분된 값 파일 읽기\n","    df_file_names = pd.read_csv(file_paths_path, delim_whitespace=True, header=None)\n","\n","    # Create a list of file names\n","    file_names = df_file_names[1].tolist()\n","\n","    # Create a dictionary of file_names and bounding boxes\n","    filename_boundingbox_dict = {img_file[:-4]: [] for img_file in file_names[:2]}\n","\n","    # Assign a bounding box to the corresponding image\n","    for i in range(0, len(file_names)):\n","        # Get the bounding box\n","        bounding_box = df_bounding_boxes.iloc[i][1:].tolist()\n","        key = file_names[i][:-4]\n","        filename_boundingbox_dict[key] = bounding_box\n","\n","    return filename_boundingbox_dict\n","\n","#image 바운딩 박스로 자르고, 주어진 사이즈로 이미지 resize\n","def get_img(img_path, bbox, image_size):\n","    img = Image.open(img_path).convert('RGB')\n","    width, height = img.size\n","    if bbox is not None:\n","        R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n","        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n","        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n","        y1 = np.maximum(0, center_y - R)\n","        y2 = np.minimum(height, center_y + R)\n","        x1 = np.maximum(0, center_x - R)\n","        x2 = np.minimum(width, center_x + R)\n","        img = img.crop([x1, y1, x2, y2])\n","    img = img.resize(image_size, PIL.Image.BILINEAR)\n","    return img\n"," \n"," #트레이닝하기 위한 데이터셋 로드 : image, labels, 일치하는 embedding return\n","def load_dataset(filenames_file_path, class_info_file_path, cub_dataset_dir, embeddings_file_path, image_size):\n","    filenames = load_filenames(filenames_file_path)\n","    class_ids = load_class_ids(class_info_file_path)\n","    bounding_boxes = load_bounding_boxes(cub_dataset_dir)\n","    all_embeddings = load_embeddings(embeddings_file_path)\n","\n","    X, y, embeddings = [], [], []\n","    print(\"Embeddings shape:\", all_embeddings.shape)\n","\n","    for index, filename in enumerate(filenames):\n","        bounding_box = bounding_boxes[filename]\n","        try:\n","            img_name = '{}/images/{}.jpg'.format(cub_dataset_dir, filename)\n","            img = get_img(img_name, bounding_box, image_size)\n","\n","            all_embeddings1 = all_embeddings[index, :, :]\n","\n","            embedding_ix = random.randint(0, all_embeddings1.shape[0] - 1)#0과 임베딩크기 사이 정수 랜덤반환\n","            embedding = all_embeddings1[embedding_ix, :]\n","\n","            X.append(np.array(img))\n","            y.append(class_ids[index])\n","            embeddings.append(embedding)\n","        except Exception as e:\n","            print(e)\n","\n","    X = np.array(X)\n","    y = np.array(y)\n","    embeddings = np.array(embeddings)\n","    return X, y, embeddings"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wW2sqGF0oEup","colab_type":"text"},"source":["> ## Model Creation"]},{"cell_type":"code","metadata":{"id":"rIjKSa-8oacp","colab_type":"code","colab":{}},"source":["def generate_c(x):\n","    mean = x[:, :128] #(batch,128)dims의 tensor생성\n","    log_sigma = x[:, 128:]\n","    stddev = K.exp(log_sigma) #from keras import backend as K | exp = exponential\n","    epsilon = K.random_normal(shape=K.constant((mean.shape[1],), dtype='int32'))  # random normal vector with mean=0 and std=1.0\n","    c = stddev * epsilon + mean #text conditioning variable 계산 | 모델 아키텍쳐 그림 중에서 c0 햇 부분\n","    return c\n","\n","#conditioning augmentation: text embedding vector를 conditioning latent variables로 변환  \n","def build_ca_model():\n","    input_layer = Input(shape=(1024,))\n","    x = Dense(256)(input_layer)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    model = Model(inputs=[input_layer], outputs=[x])\n","    return model  # Takes an embedding of shape (1024,) and returns a tensor of shape (256,)\n","  \n","def build_embedding_compressor_model():\n","    input_layer = Input(shape=(1024,))\n","    x = Dense(128)(input_layer)\n","    x = ReLU()(x)\n","    model = Model(inputs=[input_layer], outputs=[x])\n","    return model\n","\n","\n","def build_stage1_generator():\n","    input_layer = Input(shape=(1024,)) #noise variable\n","    x = Dense(256)(input_layer)\n","    mean_logsigma = LeakyReLU(alpha=0.2)(x)\n","\n","    c = Lambda(generate_c)(mean_logsigma)\n","\n","    input_layer2 = Input(shape=(100,))\n","\n","    gen_input = Concatenate(axis=1)([c, input_layer2]) #text-conditioning variable/noise variable\n","\n","    x = Dense(128 * 8 * 4 * 4, use_bias=False)(gen_input)\n","    x = ReLU()(x)\n","\n","    x = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(x) #2d tensor->4d tensor로 변환\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x) #bn 사용 - > bias=False\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = Activation(activation='tanh')(x) #저해상도 이미지 생성할 generator\n","\n","    stage1_gen = Model(inputs=[input_layer, input_layer2], outputs=[x, mean_logsigma])\n","    stage1_gen.summary()\n","\n","    checkpoint_path =  '/content/gdrive/My Drive/dl_teamproject_folder/filepath/gen/model.{epoch:02d}.hdf5'\n","    checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, verbose=1, save_weights_only=True,   period=5)\n","\n","    return stage1_gen"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-qt6dJSHox1N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592655578497,"user_tz":-540,"elapsed":1401,"user":{"displayName":"문대정","photoUrl":"","userId":"12686300660117063511"}},"outputId":"79847619-8b5c-4d0f-b5e2-b10dbce96bdd"},"source":["stage1_generator = build_stage1_generator()\n","stage1_generator.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 1024)         0                                            \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          262400      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 128)          0           leaky_re_lu_1[0][0]              \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, 100)          0                                            \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 228)          0           lambda_1[0][0]                   \n","                                                                 input_2[0][0]                    \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 16384)        3735552     concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","re_lu_1 (ReLU)                  (None, 16384)        0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","reshape_1 (Reshape)             (None, 4, 4, 1024)   0           re_lu_1[0][0]                    \n","__________________________________________________________________________________________________\n","up_sampling2d_1 (UpSampling2D)  (None, 8, 8, 1024)   0           reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 8, 8, 512)    4718592     up_sampling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 8, 8, 512)    2048        conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","re_lu_2 (ReLU)                  (None, 8, 8, 512)    0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","up_sampling2d_2 (UpSampling2D)  (None, 16, 16, 512)  0           re_lu_2[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 16, 16, 256)  1179648     up_sampling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 16, 16, 256)  1024        conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","re_lu_3 (ReLU)                  (None, 16, 16, 256)  0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","up_sampling2d_3 (UpSampling2D)  (None, 32, 32, 256)  0           re_lu_3[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 32, 32, 128)  294912      up_sampling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 32, 32, 128)  512         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","re_lu_4 (ReLU)                  (None, 32, 32, 128)  0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","up_sampling2d_4 (UpSampling2D)  (None, 64, 64, 128)  0           re_lu_4[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 64, 64, 64)   73728       up_sampling2d_4[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","re_lu_5 (ReLU)                  (None, 64, 64, 64)   0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 64, 64, 3)    1728        re_lu_5[0][0]                    \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 64, 64, 3)    0           conv2d_5[0][0]                   \n","==================================================================================================\n","Total params: 10,270,400\n","Trainable params: 10,268,480\n","Non-trainable params: 1,920\n","__________________________________________________________________________________________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 1024)         0                                            \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          262400      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 128)          0           leaky_re_lu_1[0][0]              \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, 100)          0                                            \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 228)          0           lambda_1[0][0]                   \n","                                                                 input_2[0][0]                    \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 16384)        3735552     concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","re_lu_1 (ReLU)                  (None, 16384)        0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","reshape_1 (Reshape)             (None, 4, 4, 1024)   0           re_lu_1[0][0]                    \n","__________________________________________________________________________________________________\n","up_sampling2d_1 (UpSampling2D)  (None, 8, 8, 1024)   0           reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 8, 8, 512)    4718592     up_sampling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 8, 8, 512)    2048        conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","re_lu_2 (ReLU)                  (None, 8, 8, 512)    0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","up_sampling2d_2 (UpSampling2D)  (None, 16, 16, 512)  0           re_lu_2[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 16, 16, 256)  1179648     up_sampling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 16, 16, 256)  1024        conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","re_lu_3 (ReLU)                  (None, 16, 16, 256)  0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","up_sampling2d_3 (UpSampling2D)  (None, 32, 32, 256)  0           re_lu_3[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 32, 32, 128)  294912      up_sampling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 32, 32, 128)  512         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","re_lu_4 (ReLU)                  (None, 32, 32, 128)  0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","up_sampling2d_4 (UpSampling2D)  (None, 64, 64, 128)  0           re_lu_4[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 64, 64, 64)   73728       up_sampling2d_4[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","re_lu_5 (ReLU)                  (None, 64, 64, 64)   0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 64, 64, 3)    1728        re_lu_5[0][0]                    \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 64, 64, 3)    0           conv2d_5[0][0]                   \n","==================================================================================================\n","Total params: 10,270,400\n","Trainable params: 10,268,480\n","Non-trainable params: 1,920\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RNG4l15DpyK7","colab_type":"code","colab":{}},"source":["def build_stage1_discriminator():\n","    \"\"\"\n","    discriminator는 모델 아키텍쳐 그림에서처럼 2개의 input을 받는다 \n","    1) generator거쳐서 upsampling된 네트워크를 다시 downsampling해서 만든 3차원의 4x4x512의 네트워크\n","    2) 3번에서 concatenate하기 위해 embedding layer를 같은 shape으로 만들어준다. 4x4x128 \n","    3. Concatenate 시키고, 마지막 로짓값(0~1)을 얻기 위해 마지막 모듈(merged_input ~ x2)로 넣어준다.\n","    \"\"\"\n","    input_layer = Input(shape=(64, 64, 3))\n","\n","    x = Conv2D(64, (4, 4),\n","               padding='same', strides=2,\n","               input_shape=(64, 64, 3), use_bias=False)(input_layer)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Dropout(0.3)(x)\n","\n","    x = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Dropout(0.3)(x)\n","\n","    x = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Dropout(0.3)(x)    \n","    \n","\n","    x = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Dropout(0.3)(x)    \n","\n","    input_layer2 = Input(shape=(4, 4, 128))\n","\n","    merged_input = concatenate([x, input_layer2])\n","\n","    x2 = Conv2D(64 * 8, kernel_size=1,\n","                padding=\"same\", strides=1)(merged_input)\n","    x2 = BatchNormalization()(x2)\n","    x2 = LeakyReLU(alpha=0.2)(x2)\n","    x2 = Flatten()(x2)\n","    x2 = Dense(1)(x2)\n","    x2 = Activation('sigmoid')(x2)\n","\n","    stage1_dis = Model(inputs=[input_layer, input_layer2], outputs=[x2])\n","    checkpoint_path2 =  '/content/gdrive/My Drive/dl_teamproject_folder/filepath/dis/model.{epoch:02d}.hdf5'\n","    checkpoint_dir2 = os.path.dirname(checkpoint_path2)\n","\n","    cp_callback2 = tf.keras.callbacks.ModelCheckpoint(checkpoint_path2, verbose=1, save_weights_only=True,   period=5)\n","    return stage1_dis"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cW9do9Zqp-ZI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592655664662,"user_tz":-540,"elapsed":910,"user":{"displayName":"문대정","photoUrl":"","userId":"12686300660117063511"}},"outputId":"82198c56-0310-44b4-8574-8d6d6a5537fe"},"source":["stage1_discriminator = build_stage1_discriminator()\n","stage1_discriminator.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Model: \"model_2\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_4 (InputLayer)            (None, 64, 64, 3)    0                                            \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 32, 32, 64)   3072        input_4[0][0]                    \n","__________________________________________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)       (None, 32, 32, 64)   0           conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 32, 32, 64)   0           leaky_re_lu_3[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 16, 16, 128)  131072      dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 16, 16, 128)  512         conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)       (None, 16, 16, 128)  0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 16, 16, 128)  0           leaky_re_lu_4[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 8, 8, 256)    524288      dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 8, 8, 256)    1024        conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_5 (LeakyReLU)       (None, 8, 8, 256)    0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 8, 8, 256)    0           leaky_re_lu_5[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 4, 4, 512)    2097152     dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 4, 4, 512)    2048        conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_6 (LeakyReLU)       (None, 4, 4, 512)    0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 4, 4, 512)    0           leaky_re_lu_6[0][0]              \n","__________________________________________________________________________________________________\n","input_5 (InputLayer)            (None, 4, 4, 128)    0                                            \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 4, 4, 640)    0           dropout_4[0][0]                  \n","                                                                 input_5[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 4, 4, 512)    328192      concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 4, 4, 512)    2048        conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_7 (LeakyReLU)       (None, 4, 4, 512)    0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 8192)         0           leaky_re_lu_7[0][0]              \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 1)            8193        flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 1)            0           dense_3[0][0]                    \n","==================================================================================================\n","Total params: 3,097,601\n","Trainable params: 3,094,785\n","Non-trainable params: 2,816\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"856MlJdtp_V0","colab_type":"code","colab":{}},"source":["def build_adversarial_model(gen_model, dis_model):\n","    input_layer = Input(shape=(1024,)) # 1024 = stage1_generator에 들어갈 input size\n","    input_layer2 = Input(shape=(100,)) # 100 = noise 변수의 input size\n","    input_layer3 = Input(shape=(4, 4, 128)) \n","\n","    x, mean_logsigma = gen_model([input_layer, input_layer2]) # stage1_gen 처럼 나온 output\n","\n","    dis_model.trainable = False\n","    valid = dis_model([x, input_layer3]) # stage1_gen 처럼 나온 output과 임베딩 logit값?\n","\n","    model = Model(inputs=[input_layer, input_layer2, input_layer3], outputs=[valid, mean_logsigma])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3zyCR8IwUCnC","colab_type":"code","colab":{}},"source":["def residual_block(input):\n","    \"\"\"\n","    그래디언트가 잘 흐를 수 있도록 일종의 지름길(shortcut, skip connection)을 만들어 주자는 생각\n","    \"\"\"\n","    x = Conv2D(128 * 4, kernel_size=(3, 3), padding='same', strides=1)(input)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = Conv2D(128 * 4, kernel_size=(3, 3), strides=1, padding='same')(x)\n","    x = BatchNormalization()(x)\n","\n","    x = add([x, input])\n","    x = ReLU()(x)\n","\n","    return x\n","\n","def joint_block(inputs): # 임베딩한 결과와 CA를 합침\n","    c = inputs[0]\n","    x = inputs[1]\n","\n","    c = K.expand_dims(c, axis=1)\n","    c = K.expand_dims(c, axis=1)\n","    c = K.tile(c, [1, 16, 16, 1])\n","    return K.concatenate([c, x], axis=3)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zNxCJiZxTsBw","colab_type":"code","colab":{}},"source":["def build_stage2_generator():\n","    \"\"\"\n","    CA 네트워크를 포함한 stage 2 generator 생성\n","    \"\"\"\n","\n","    # 1. CA Augmentation Network\n","    input_layer = Input(shape=(1024,))\n","    input_lr_images = Input(shape=(64, 64, 3))\n","\n","    ca = Dense(256)(input_layer)\n","    mean_logsigma = LeakyReLU(alpha=0.2)(ca)\n","    c = Lambda(generate_c)(mean_logsigma)\n","\n","    # 2. Image Encoder\n","    x = ZeroPadding2D(padding=(1, 1))(input_lr_images)\n","    x = Conv2D(128, kernel_size=(3, 3), strides=1, use_bias=False)(x)\n","    x = ReLU()(x)\n","\n","    x = ZeroPadding2D(padding=(1, 1))(x)\n","    x = Conv2D(256, kernel_size=(4, 4), strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = ZeroPadding2D(padding=(1, 1))(x)\n","    x = Conv2D(512, kernel_size=(4, 4), strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    # 3. Joint\n","    c_code = Lambda(joint_block)([c, x])\n","\n","    x = ZeroPadding2D(padding=(1, 1))(c_code)\n","    x = Conv2D(512, kernel_size=(3, 3), strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    # 4. Residual blocks\n","    x = residual_block(x)\n","    x = residual_block(x)\n","    x = residual_block(x)\n","    x = residual_block(x)\n","\n","    # 5. Upsampling blocks\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = Activation('tanh')(x)\n","\n","    model = Model(inputs=[input_layer, input_lr_images], outputs=[x, mean_logsigma])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sefu7nJjT_qY","colab_type":"code","colab":{}},"source":["def build_stage2_discriminator():\n","    \"\"\"\n","    stage 2 discriminator 모델 만들기\n","    \"\"\"\n","    input_layer = Input(shape=(256, 256, 3))\n","\n","    x = Conv2D(64, (4, 4), padding='same', strides=2, input_shape=(256, 256, 3), use_bias=False)(input_layer)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Dropout(0.3)(x)    \n","\n","\n","    x = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Dropout(0.3)(x)    \n","\n","    x = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Dropout(0.3)(x)    \n","\n","    x = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Dropout(0.3)(x)    \n","\n","    x = Conv2D(1024, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x) \n","    x = Dropout(0.3)(x)    \n","\n","\n","    x = Conv2D(2048, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Dropout(0.3)(x)    \n","\n","    x = Conv2D(1024, (1, 1), padding='same', strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Dropout(0.3)(x)    \n","\n","    x = Conv2D(512, (1, 1), padding='same', strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.3)(x)    \n","\n","    x2 = Conv2D(128, (1, 1), padding='same', strides=1, use_bias=False)(x)\n","    x2 = BatchNormalization()(x2)\n","    x2 = LeakyReLU(alpha=0.2)(x2)\n","    x2 = Dropout(0.3)(x)    \n","\n","    x2 = Conv2D(128, (3, 3), padding='same', strides=1, use_bias=False)(x2)\n","    x2 = BatchNormalization()(x2)\n","    x2 = LeakyReLU(alpha=0.2)(x2)\n","    x2 = Dropout(0.3)(x)    \n","\n","    x2 = Conv2D(512, (3, 3), padding='same', strides=1, use_bias=False)(x2)\n","    x2 = BatchNormalization()(x2)\n","    x2 = Dropout(0.3)(x)    \n","\n","    added_x = add([x, x2])\n","    added_x = LeakyReLU(alpha=0.2)(added_x)\n","\n","    input_layer2 = Input(shape=(4, 4, 128))\n","\n","    merged_input = concatenate([added_x, input_layer2])\n","\n","    x3 = Conv2D(64 * 8, kernel_size=1, padding=\"same\", strides=1)(merged_input)\n","    x3 = BatchNormalization()(x3)\n","    x3 = LeakyReLU(alpha=0.2)(x3)\n","    x3 = Flatten()(x3)\n","    x3 = Dense(1)(x3)\n","    x3 = Activation('sigmoid')(x3)\n","\n","    stage2_dis = Model(inputs=[input_layer, input_layer2], outputs=[x3])\n","    return stage2_dis"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1VjU7ZEqQfVT","colab_type":"code","colab":{}},"source":["def build_adversarial_model(gen_model2, dis_model, gen_model1):\n","    \"\"\"\n","    adversarial 모델 만들기\n","    \"\"\"\n","    embeddings_input_layer = Input(shape=(1024, ))\n","    noise_input_layer = Input(shape=(100, ))\n","    compressed_embedding_input_layer = Input(shape=(4, 4, 128))\n","\n","    gen_model1.trainable = False\n","    dis_model.trainable = False\n","\n","    lr_images, mean_logsigma1 = gen_model1([embeddings_input_layer, noise_input_layer])\n","    hr_images, mean_logsigma2 = gen_model2([embeddings_input_layer, lr_images])\n","    valid = dis_model([hr_images, compressed_embedding_input_layer])\n","\n","    model = Model(inputs=[embeddings_input_layer, noise_input_layer, compressed_embedding_input_layer], outputs=[valid, mean_logsigma2])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-UK6apmCoKna","colab_type":"text"},"source":["> ## Defining Loss"]},{"cell_type":"code","metadata":{"id":"4tXK8paoqHbp","colab_type":"code","colab":{}},"source":["def KL_loss(y_true, y_pred):\n","    mean = y_pred[:, :128]\n","    logsigma = y_pred[:, :128]\n","    loss = -logsigma + .5 * (-1 + K.exp(2. * logsigma) + K.square(mean))\n","    loss = K.mean(loss)\n","    return loss\n","\n","def wasserstein_loss(y_true, y_pred):\n","\treturn K.mean(y_true * y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fHnAfkFdL8ot","colab_type":"text"},"source":["* Wasserstein_loss의 수식적 당위성 설명 : https://ahjeong.tistory.com/7"]},{"cell_type":"markdown","metadata":{"id":"tEiWRBCYDvUA","colab_type":"text"},"source":["> ## Inception score measure"]},{"cell_type":"markdown","metadata":{"id":"Ue0-bpRpAb0P","colab_type":"text"},"source":["![대체 텍스트](https://i.ibb.co/Dw20ssm/gan.png)"]},{"cell_type":"markdown","metadata":{"id":"8CQ1l4MIEv2q","colab_type":"text"},"source":["* 잘 생성된 이미지 10개 대상으로 성능 측정해보기"]},{"cell_type":"code","metadata":{"id":"hLFdnyKLDsdK","colab_type":"code","colab":{}},"source":["from math import floor\n","from numpy import ones\n","from numpy import expand_dims\n","from numpy import log\n","from numpy import mean\n","from numpy import std\n","from numpy import exp\n","from numpy.random import shuffle\n","from keras.applications.inception_v3 import InceptionV3\n","from keras.applications.inception_v3 import preprocess_input\n","from keras.datasets import cifar10\n","from skimage.transform import resize\n","from numpy import asarray\n"," \n","# scale an array of images to a new size\n","def scale_images(images, new_shape):\n","\timages_list = list()\n","\tfor image in images:\n","\t\t# resize with nearest neighbor interpolation\n","\t\tnew_image = resize(image, new_shape, 0)\n","\t\t# store\n","\t\timages_list.append(new_image)\n","\treturn asarray(images_list)\n"," \n","# assumes images have any shape and pixels in [0,255]\n","def calculate_inception_score(images, n_split=10, eps=1E-16):\n","\t# load inception v3 model\n","\tmodel = InceptionV3()\n","\t# enumerate splits of images/predictions\n","\tscores = list()\n","\tn_part = floor(images.shape[0] / n_split)\n","\tfor i in range(n_split):\n","\t\t# retrieve images\n","\t\tix_start, ix_end = i * n_part, (i+1) * n_part\n","\t\tsubset = images[ix_start:ix_end]\n","\t\t# convert from uint8 to float32\n","\t\tsubset = subset.astype('float32')\n","\t\t# scale images to the required size\n","\t\tsubset = scale_images(subset, (299,299,3))\n","\t\t# pre-process images, scale to [-1,1]\n","\t\tsubset = preprocess_input(subset)\n","\t\t# predict p(y|x)\n","\t\tp_yx = model.predict(subset)\n","\t\t# calculate p(y)\n","\t\tp_y = expand_dims(p_yx.mean(axis=0), 0)\n","\t\t# calculate KL divergence using log probabilities\n","\t\tkl_d = p_yx * (log(p_yx + eps) - log(p_y + eps))\n","\t\t# sum over classes\n","\t\tsum_kl_d = kl_d.sum(axis=1)\n","\t\t# average over images\n","\t\tavg_kl_d = mean(sum_kl_d)\n","\t\t# undo the log\n","\t\tis_score = exp(avg_kl_d)\n","\t\t# store\n","\t\tscores.append(is_score)\n","\t# average across images\n","\tis_avg, is_std = mean(scores), std(scores)\n","\treturn is_avg, is_std"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fYJ3hF_IEBQg","colab_type":"code","colab":{}},"source":["image1 = Image.open('stage1_results/gen_590_0.png')\n","image2 = Image.open('stage1_results/gen_590_3.png')\n","image3 = Image.open('stage1_results/gen_590_5.png')\n","image4 = Image.open('stage1_results/gen_590_6.png')\n","image5 = Image.open('stage1_results/gen_590_8.png')\n","image6 = Image.open('stage1_results/gen_590_9.png')\n","image7 = Image.open('stage1_results/gen_592_1.png')\n","image8 = Image.open('stage1_results/gen_592_7.png')\n","image9 = Image.open('stage1_results/gen_556_6.png')\n","image10 = Image.open('stage1_results/gen_556_9.png')\n","\n","data1 = np.asarray(image1)\n","data2 = np.asarray(image2)\n","data3 = np.asarray(image3)\n","data4 = np.asarray(image4)\n","data5 = np.asarray(image5)\n","data6 = np.asarray(image6)\n","data7 = np.asarray(image7)\n","data8 = np.asarray(image8)\n","data9 = np.asarray(image9)\n","data10 = np.asarray(image10)\n","print(test_data.shape)\n","\n","dataset1 = [data1,data2,data3,data4,data5,data6,data7,data8,data9,data10]\n","dataset1 = np.asarray(dataset1)\n","print(dataset1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eljg9hG8EJ7a","colab_type":"code","colab":{}},"source":["calculate_inception_score(dataset1, n_split=1, eps=1E-16)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W_GlmJrbIbR3","colab_type":"text"},"source":["* 흰색 배경 제거한 이미지로 다시 score 측정"]},{"cell_type":"code","metadata":{"id":"yquJXq-YGbjC","colab_type":"code","colab":{}},"source":["image1 = Image.open('image/gen_592_9.png')\n","image2 = Image.open('image/gen_590_2.png')\n","image3 = Image.open('image/gen_458_8.png')\n","\n","data1 = np.asarray(image1)\n","data2 = np.asarray(image2)\n","data3 = np.asarray(image3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pq7mZD-SIpIz","colab_type":"code","colab":{}},"source":["calculate_inception_score(data1, n_split=1, eps=1E-16)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LazoAqT6I91i","colab_type":"code","colab":{}},"source":["calculate_inception_score(data2, n_split=1, eps=1E-16)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2ebG8IYI-Mh","colab_type":"code","colab":{}},"source":["calculate_inception_score(data3, n_split=1, eps=1E-16)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nAbBVl3CoN0B","colab_type":"text"},"source":["> ## Main File(Train)"]},{"cell_type":"markdown","metadata":{"id":"vRne9G5RNPxv","colab_type":"text"},"source":["* (Rewind) GAN의 학습과정\n","  * 판별자 네트워크 학습\n","   1. 랜덤 노이즈 m 개를 생성하여, 생성자 네트워크에 전달하고 변환된 데이터 m 개를 얻음.\n","   2. 학습 데이터셋에서 진짜 데이터 m 개를 선택.\n","   3. 2m 개의 데이터(진짜 m개 + 가짜 m개)를 이용해 판별자 네트워크의 정확도를 최대화하는 방향으로 학습.\n","  * 생성자 네트워크 학습\n","   1. 랜덤 노이즈 m 개를 다시 생성.\n","   2. 랜덤 노이즈 m 개를 이용해 생성자가 판별자의 정확도를 최소화하도록 학습."]},{"cell_type":"code","metadata":{"id":"v4baFP2_qIVf","colab_type":"code","colab":{}},"source":["if __name__ == '__main__':\n","\n"," \n","    '''\n","    filepath = '/content/gdrive/My Drive/dl_teamproject_folder/filepath/gen/model.{epoch:02d}.hdf5'\n","    filepath2 = '/content/gdrive/My Drive/dl_teamproject_folder/filepath/dis/model.{epoch:02d}.hdf5'\n","    modelckpt = ModelCheckpoint(filepath=filepath)\n","    modelckpt2 = ModelCheckpoint(filepath=filepath2)\n","    '''\n","    # 폴더 경로 설정\n","    data_dir = \"/content/gdrive/My Drive/dl_teamproject_folder/birds/birds\"\n","    train_dir = data_dir + \"/train\"\n","    test_dir = data_dir + \"/test\"\n","\n","    # 하이퍼 파라미터 설정\n","    image_size = 64\n","    batch_size = 64\n","    z_dim = 100\n","    stage1_generator_lr = 0.0002\n","    stage1_discriminator_lr = 0.0002\n","    stage1_lr_decay_step = 600\n","    epochs = 600\n","    condition_dim = 128\n","\n","    #폴더 경로 설정\n","    embeddings_file_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n","    embeddings_file_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n","\n","    filenames_file_path_train = train_dir + \"/filenames.pickle\"\n","    filenames_file_path_test = test_dir + \"/filenames.pickle\"\n","\n","    class_info_file_path_train = train_dir + \"/class_info.pickle\"\n","    class_info_file_path_test = test_dir + \"/class_info.pickle\"\n","\n","    cub_dataset_dir = \"/content/gdrive/My Drive/dl_teamproject_folder/CUB_200_2011/CUB_200_2011\"\n","    \n","    # optimizer 정의\n","    dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n","    gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n","\n","    \"\"\"\"\n","    데이터 불러오기\n","    \"\"\"\n","    X_train, y_train, embeddings_train = load_dataset(filenames_file_path=filenames_file_path_train,\n","                                                      class_info_file_path=class_info_file_path_train,\n","                                                      cub_dataset_dir=cub_dataset_dir,\n","                                                      embeddings_file_path=embeddings_file_path_train,\n","                                                      image_size=(64, 64))\n","\n","    X_test, y_test, embeddings_test = load_dataset(filenames_file_path=filenames_file_path_test,\n","                                                   class_info_file_path=class_info_file_path_test,\n","                                                   cub_dataset_dir=cub_dataset_dir,\n","                                                   embeddings_file_path=embeddings_file_path_test,\n","                                                   image_size=(64, 64))\n","\n","    \"\"\"\n","    네트워크 만들고 컴파일\n","    \"\"\"\n","    ca_model = build_ca_model()\n","    ca_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n","\n","    stage1_dis = build_stage1_discriminator()\n","    stage1_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n","\n","    stage1_gen = build_stage1_generator()\n","    stage1_gen.compile(loss=\"mse\", optimizer=gen_optimizer)\n","\n","    embedding_compressor_model = build_embedding_compressor_model()\n","    embedding_compressor_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n","\n","    adversarial_model = build_adversarial_model(gen_model=stage1_gen, dis_model=stage1_dis)\n","    adversarial_model.compile(loss=['binary_crossentropy',wasserstein_loss], loss_weights=[1, 2.0],\n","                              optimizer=gen_optimizer, metrics=None)\n","\n","    tensorboard = TensorBoard(log_dir=\"/content/gdrive/My Drive/dl_teamproject_folder/logs/\".format(time.time()))\n","    tensorboard.set_model(stage1_gen)\n","    tensorboard.set_model(stage1_dis)\n","    tensorboard.set_model(ca_model)\n","    tensorboard.set_model(embedding_compressor_model)\n","\n","    # 진짜와 가짜 값들이 담긴 배열 생성\n","    # label smoothing 적용 (discriminator가 부드러운 형태로 확률을 예측하도록 하기 위해 실제 데이터에 대한 target 값을 1보다 약간 작은 값, 이를테면 0.9로 해준다는 것)\n","    # 정확한 건 (https://kangbk0120.github.io/articles/2017-08/tips-from-goodfellow)\n","    real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n","    fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n","\n","    for epoch in range(epochs):\n","        print(\"========================================\")\n","        print(\"Epoch is:\", epoch+1)\n","        print(\"Number of batches\", int(X_train.shape[0] / batch_size))\n","\n","        gen_losses = []\n","        dis_losses = []\n","\n","        # 데이터와 train 모델 불러오기\n","        number_of_batches = int(X_train.shape[0] / batch_size)\n","        for index in range(number_of_batches):\n","            print(\"Batch:{}\".format(index+1))\n","            \n","            \"\"\"\n","            Discriminator network 학습\n","            \"\"\"\n","            # 배치 사이즈 만큼의 데이터를 샘플링한다 Sample a batch of data\n","            z_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n","            image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n","            embedding_batch = embeddings_train[index * batch_size:(index + 1) * batch_size]\n","            image_batch = (image_batch - 127.5) / 127.5\n","\n","            # 가짜 이미지 생성\n","            fake_images, _ = stage1_gen.predict([embedding_batch, z_noise], verbose=3)\n","\n","            # compressed된 embedding 생성\n","            compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n","            compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n","            # 배열을 반복하면서 새로운 축(axis)을 추가하는 np.tile\n","            compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n","\n","            # 진짜 이미지에 진짜라는 라벨(1)을 주고 discriminator 학습시킨 loss \n","            dis_loss_real = stage1_dis.train_on_batch([image_batch, compressed_embedding],\n","                                                      np.reshape(real_labels, (batch_size, 1)))\n","            # generator가 생성한 가짜 이미지에 가짜라는 라벨(0)을 주고 discriminator 학습시킨 loss \n","            dis_loss_fake = stage1_dis.train_on_batch([fake_images, compressed_embedding],\n","                                                      np.reshape(fake_labels, (batch_size, 1)))\n","            # 진짜 이미지에 가짜라는 라벨(0)주고 discriminator 학습시킨 loss \n","            dis_loss_wrong = stage1_dis.train_on_batch([image_batch[:(batch_size - 1)], compressed_embedding[1:]],\n","                                                       np.reshape(fake_labels[1:], (batch_size-1, 1)))\n","            # 총 discriminator의 loss = 0.5 * (loss_real + 0.5 * (loss_wrong + loss_fake)) \n","            d_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong, dis_loss_fake))\n","\n","            print(\"d_loss_real:{}\".format(dis_loss_real))\n","            print(\"d_loss_fake:{}\".format(dis_loss_fake))\n","            print(\"d_loss_wrong:{}\".format(dis_loss_wrong))\n","            print(\"d_loss:{}\".format(d_loss))\n","\n","            \"\"\"\n","            Generator network 학습\n","            \"\"\"\n","            g_loss = adversarial_model.train_on_batch([embedding_batch, z_noise, compressed_embedding],[K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n","            print(\"g_loss:{}\".format(g_loss))\n","\n","            dis_losses.append(d_loss)\n","            gen_losses.append(g_loss)\n","\n","         #   stage1_gen.save_weights(\"stage1_gen.h5\")\n","         #   stage1_dis.save_weights(\"stage1_dis.h5\")\n","\n","        \"\"\"\n","        각 에폭 끝나고 tensorboard에 loss값들 저장하는 부분\n","        \"\"\"\n","        write_log(tensorboard, 'discriminator_loss', np.mean(dis_losses), epoch)\n","        write_log(tensorboard, 'generator_loss', np.mean(gen_losses[0]), epoch)\n","        \n","        # 짝수번째 에폭 학습 끝날때마다 이미지 생성하고 저장하는 부분\n","        if epoch % 2 == 0:\n","            # z_noise2 = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n","            z_noise2 = np.random.normal(0, 1, size=(batch_size, z_dim))\n","            embedding_batch = embeddings_test[0:batch_size]\n","            fake_images, _ = stage1_gen.predict_on_batch([embedding_batch, z_noise2])\n","            stage1_gen.save_weights(\"/content/gdrive/My Drive/dl_teamproject_folder/filepath/stage1_gen2.h5\")\n","            stage1_dis.save_weights(\"/content/gdrive/My Drive/dl_teamproject_folder/filepath/stage1_dis2.h5\")\n","\n","            # Save images\n","            for i, img in enumerate(fake_images[:10]):\n","              save_rgb_img(img, \"/content/gdrive/My Drive/dl_teamproject_folder/results/gen_{}_{}.png\".format(epoch, i))\n","\n","    # Save models\n","    stage1_gen.save_weights(\"/content/gdrive/My Drive/dl_teamproject_folder/stage1_gen.h5\")\n","    stage1_dis.save_weights(\"/content/gdrive/My Drive/dl_teamproject_folder/stage1_dis.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ybug8bjeQ_qU","colab_type":"code","colab":{}},"source":["# Stage 2\n","if __name__ == '__main__':\n","    # 경로 설정\n","    data_dir =\"/content/birds/birds\"\n","    train_dir = data_dir + \"/train\"\n","    test_dir = data_dir + \"/test\"\n","\n","    # 하이퍼파라미터 설정\n","    hr_image_size = (256, 256)\n","    lr_image_size = (64, 64)\n","    batch_size = 32\n","    z_dim = 100\n","    stage1_generator_lr = 0.0002\n","    stage1_discriminator_lr = 0.0002\n","    stage1_lr_decay_step = 600\n","    epochs = 600\n","    condition_dim = 128\n","\n","    # 경로 설정\n","    embeddings_file_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n","    embeddings_file_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n","\n","    filenames_file_path_train = train_dir + \"/filenames.pickle\"\n","    filenames_file_path_test = test_dir + \"/filenames.pickle\"\n","\n","    class_info_file_path_train = train_dir + \"/class_info.pickle\"\n","    class_info_file_path_test = test_dir + \"/class_info.pickle\"\n","\n","    cub_dataset_dir = \"/content/CUB_200_2011/CUB_200_2011\"\n","\n","    # 옵티마이저 설정\n","    dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n","    gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n","\n","    \"\"\"\n","   데이터 불러오기\n","    \"\"\"\n","    X_hr_train, y_hr_train, embeddings_train = load_dataset(filenames_file_path=filenames_file_path_train,\n","                                                            class_info_file_path=class_info_file_path_train,\n","                                                            cub_dataset_dir=cub_dataset_dir,\n","                                                            embeddings_file_path=embeddings_file_path_train,\n","                                                            image_size=(256, 256))\n","\n","    X_hr_test, y_hr_test, embeddings_test = load_dataset(filenames_file_path=filenames_file_path_test,\n","                                                         class_info_file_path=class_info_file_path_test,\n","                                                         cub_dataset_dir=cub_dataset_dir,\n","                                                         embeddings_file_path=embeddings_file_path_test,\n","                                                         image_size=(256, 256))\n","\n","    X_lr_train, y_lr_train, _ = load_dataset(filenames_file_path=filenames_file_path_train,\n","                                             class_info_file_path=class_info_file_path_train,\n","                                             cub_dataset_dir=cub_dataset_dir,\n","                                             embeddings_file_path=embeddings_file_path_train,\n","                                             image_size=(64, 64))\n","\n","    X_lr_test, y_lr_test, _ = load_dataset(filenames_file_path=filenames_file_path_test,\n","                                           class_info_file_path=class_info_file_path_test,\n","                                           cub_dataset_dir=cub_dataset_dir,\n","                                           embeddings_file_path=embeddings_file_path_test,\n","                                           image_size=(64, 64))\n","\n","    \"\"\"\n","    모델을 만들고 컴파일\n","    \"\"\"\n","    stage2_dis = build_stage2_discriminator()\n","    stage2_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n","\n","    stage1_gen = build_stage1_generator()\n","    stage1_gen.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n","\n","    stage1_gen.load_weights(\"/content/gdrive/My Drive/딥러닝/results2/stage1_gen2.h5\")\n","\n","    stage2_gen = build_stage2_generator()\n","    stage2_gen.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n","\n","    embedding_compressor_model = build_embedding_compressor_model()\n","    embedding_compressor_model.compile(loss='binary_crossentropy', optimizer='adam')\n","\n","    adversarial_model = build_adversarial_model(stage2_gen, stage2_dis, stage1_gen)\n","    adversarial_model.compile(loss=['binary_crossentropy',wasserstein_loss], loss_weights=[1.0, 2.0],\n","                              optimizer=gen_optimizer, metrics=None)\n","\n","    tensorboard = TensorBoard(log_dir=\"/content/gdrive/My Drive/딥러닝/logs/\".format(time.time()))\n","    tensorboard.set_model(stage2_gen)\n","    tensorboard.set_model(stage2_dis)\n","\n","    # 진짜와 가짜 값들이 담긴 배열 생성\n","    # label smoothing 적용 (discriminator가 부드러운 형태로 확률을 예측하도록 하기 위해 실제 데이터에 대한 target 값을 1보다 약간 작은 값, 이를테면 0.9로 해준다는 것)\n","    # 정확한 건 (https://kangbk0120.github.io/articles/2017-08/tips-from-goodfellow)\n","    real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n","    fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n","\n","    for epoch in range(epochs):\n","        print(\"========================================\")\n","        print(\"Epoch is:\", epoch)\n","\n","        gen_losses = []\n","        dis_losses = []\n","\n","        # 데이터 불러오고 학습\n","        number_of_batches = int(X_hr_train.shape[0] / batch_size)\n","        print(\"Number of batches:{}\".format(number_of_batches))\n","        for index in range(number_of_batches):\n","            print(\"Batch:{}\".format(index+1))\n","\n","            # Create a noise vector\n","            z_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n","            X_hr_train_batch = X_hr_train[index * batch_size:(index + 1) * batch_size]\n","            embedding_batch = embeddings_train[index * batch_size:(index + 1) * batch_size]\n","            X_hr_train_batch = (X_hr_train_batch - 127.5) / 127.5\n","\n","            # 가짜 이미지 생성\n","            lr_fake_images, _ = stage1_gen.predict([embedding_batch, z_noise], verbose=3)\n","            hr_fake_images, _ = stage2_gen.predict([embedding_batch, lr_fake_images], verbose=3)\n","\n","            \"\"\"\n","            4. Generate compressed embeddings\n","            \"\"\"\n","            compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n","            compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n","            compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n","\n","            \"\"\"\n","            5. Train the discriminator model\n","            \"\"\"\n","            dis_loss_real = stage2_dis.train_on_batch([X_hr_train_batch, compressed_embedding],\n","                                                      np.reshape(real_labels, (batch_size, 1)))\n","            dis_loss_fake = stage2_dis.train_on_batch([hr_fake_images, compressed_embedding],\n","                                                      np.reshape(fake_labels, (batch_size, 1)))\n","            dis_loss_wrong = stage2_dis.train_on_batch([X_hr_train_batch[:(batch_size - 1)], compressed_embedding[1:]],\n","                                                       np.reshape(fake_labels[1:], (batch_size-1, 1)))\n","            d_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong,  dis_loss_fake))\n","            print(\"d_loss:{}\".format(d_loss))\n","\n","            \"\"\"\n","            Train the adversarial model\n","            \"\"\"\n","            g_loss = adversarial_model.train_on_batch([embedding_batch, z_noise, compressed_embedding],\n","                                                                [K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n","\n","            print(\"g_loss:{}\".format(g_loss))\n","\n","            dis_losses.append(d_loss)\n","            gen_losses.append(g_loss)\n","            adversarial_model.save('/content/gdrive/My Drive/딥러닝/results2/my_stackganmodel2.h5')    \n","            stage2_gen.save('/content/gdrive/My Drive/딥러닝/results2/my_genmodel2.h5')    \n","            stage2_dis.save('/content/gdrive/My Drive/딥러닝/results2/my_dismodel2.h5')    \n","            \"\"\"\n","            각 에폭 끝나고 tensorboard에 loss값들 저장하는 부분\n","            \"\"\"\n","            write_log(tensorboard, 'discriminator_loss', np.mean(dis_losses), epoch)\n","            write_log(tensorboard, 'generator_loss', np.mean(gen_losses[0]), epoch)\n","\n","\n","        # 2 에폭마다 이미지 생성\n","        if epoch % 2 == 0:\n","            z_noise2 = np.random.normal(0, 1, size=(batch_size, z_dim))\n","            embedding_batch = embeddings_test[0:batch_size]\n","\n","            lr_fake_images, _ = stage1_gen.predict([embedding_batch, z_noise2], verbose=3)\n","            hr_fake_images, _ = stage2_gen.predict([embedding_batch, lr_fake_images], verbose=3)\n","            stage2_gen.save_weights(\"/content/gdrive/My Drive/딥러닝/results2/stage2_gen.h5\")\n","            stage2_dis.save_weights(\"/content/gdrive/My Drive/딥러닝/results2/stage2_dis.h5\")\n","            # Save images\n","            for i, img in enumerate(lr_fake_images[:10]):\n","                save_rgb_img(img, \"/content/gdrive/My Drive/딥러닝/results2/gen_{}_{}.png\".format(epoch, i))\n","            for i, img in enumerate(hr_fake_images[:10]):\n","                save_rgb_img(img, \"/content/gdrive/My Drive/딥러닝/results2/gen_{}_{}.png\".format(epoch, i))\n","\n","    adversarial_model.save('/content/gdrive/My Drive/딥러닝/results2/my_stackganmodel.h5')    \n","    stage2_gen.save('/content/gdrive/My Drive/딥러닝/results2/my_genmodel.h5')    \n","    stage2_dis.save('/content/gdrive/My Drive/딥러닝/results2/my_dismodel.h5')  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VC6WuhK8M-Jx","colab_type":"text"},"source":["![대체 텍스트](https://i.ibb.co/9pqLpWD/embedding-shape.pngs://ibb.co/sgFGgqf)\n","\n","![대체 텍스트](https://i.ibb.co/nzYYZyb/600-epoch.png://)"]},{"cell_type":"markdown","metadata":{"id":"tZBsKWqvMc6W","colab_type":"text"},"source":["* LeakyReLU 층 사용 \n","\n",": 음수의 활성화 값을 조금 허용함으로써 희소한 그래디언트가 훈련 방해하는 것을 방지"]},{"cell_type":"markdown","metadata":{"id":"stRhAcf2RY21","colab_type":"text"},"source":["  * Residual Block 사용\n","  \n","    <img src=\"https://drive.google.com/uc?id=1ZqD1rYEg3wd6XA8LjRZo-JYsbIgQ1ro9\" width=\"500\">\n","\n","    그래디언트가 잘 흐를 수 있도록 일종의 지름길(shortcut, skip connection)을 만들어 주자는 생각"]},{"cell_type":"markdown","metadata":{"id":"_uPGevXV_L1o","colab_type":"text"},"source":["**<모델 별 변경/ 추가 부분>**\n","\n","*   LearningRate 변경 \n","  *   기존 lr의 10배로 변경  \n","  *  학습시간 단축위해 시행\n","\n","*   Batch Normalization 사용 : 빠른 학습을 위해\n","      <img src=\"https://drive.google.com/uc?id=1BZw_m1lVnfTaNK7blmmvzSgw1svwMXNN\" width=\"400\">\n","  \n","      <img src=\"https://drive.google.com/uc?id=1BVRTwGj3mNzdMOnKOG6_dd-gYn6Qoem2\" width=\"400\">\n","\n","\n","\n","*   Optimizer 변경 \n","  *   Batch Normalization 여부에 따른 최적의 Optimizer 상이\n","\n","        <img src=\"  https://1.bp.blogspot.com/-_fMUvxuThaw/WOD-7GuqZ0I/AAAAAAAABgo/fEN29-ukRJ0XKKVnJ5I_R-JVAFSwqO88QCK4B/s1600/lsgan_12.PNG\" width=\"400\">\n","\n","\n","*   Wassertein Loss 사용\n","  * discriminator와 generator간의 balance 맞추기\n","  * mode collapse(G returns the same looking samples for different input signals) 피하기 위해  \n","  <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FcRqbfT%2Fbtqu2Ia2BK7%2FQEinXUjVkZoWx2jVVXaho0%2Fimg.png\" width=\"400\">\n","  * http://dl-ai.blogspot.com/2017/08/gan-problems.html\n","\n","*   DropOut 사용 \n","  *   모델 견고하게 만들기 위해 무작위성 주입\n","    *  GAN은 동적 평형 특성상 여러 방식으로 갇힐 가능성 높음\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9F6_rfuOu4q0","colab_type":"text"},"source":["# Presentation of results"]},{"cell_type":"markdown","metadata":{"id":"ywNfQN6XG5g1","colab_type":"text"},"source":["* MODEL : Batch normalization X + Adam optimizer 사용\n","\n","  <img src=https://i.ibb.co/Q7VqfcG/black.png, width=150><img src=https://i.ibb.co/4PfNyKG/black2.png, width=150><img src=https://i.ibb.co/4PfNyKG/black2.png, width=150><img src=https://i.ibb.co/4PfNyKG/black2.png, width=150>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CY-I721PolYE","colab_type":"text"},"source":["* MODEL : Learning rate 기존의 10배로 학습 진행\n","\n","  <img src=https://i.ibb.co/hydFCT2/legend.png, width=450>\n"]},{"cell_type":"markdown","metadata":{"id":"M5DQHnhrIzJD","colab_type":"text"},"source":["* MODEL : Batch normalization X + RMSprop optimzer 사용\n","\n","  <img src=https://i.ibb.co/c3W0KLT/NBN.png, width=850>"]},{"cell_type":"markdown","metadata":{"id":"SUmj17oZJfux","colab_type":"text"},"source":["* MODEL :Batch normalization X + RMSprop optimizer + Wassertein Loss 변경\n"," <img src=https://i.ibb.co/qYw6nc3/wgan-rmsprop.png>"]},{"cell_type":"markdown","metadata":{"id":"hrJAqFsy90rQ","colab_type":"text"},"source":["* MODEL : Batch normalization O + Adam optimizer 사용 -> Mode Collapse 발생\n","\n","  <img src=https://i.ibb.co/8cGhWbN/original-epoch.png, width=800>"]},{"cell_type":"markdown","metadata":{"id":"bYdQGsUqwYsM","colab_type":"text"},"source":["* MODEL : Batch normalization O + Adam optimizer + Wassertein Loss 변경 + Dropout 추가\n","\n","  * STAGE1\n","  ![대체 텍스트](https://i.ibb.co/ZTrjXtb/ungyeong.png)\n","\n","  * STAGE2\n","  <img src=https://i.ibb.co/gMH1YSr/image.jpg, width=600>\n"]},{"cell_type":"markdown","metadata":{"id":"KvBuKF2Iu9YE","colab_type":"text"},"source":["# Analysis of results"]},{"cell_type":"markdown","metadata":{"id":"1EVyzg6uCwUq","colab_type":"text"},"source":["* 모델에 따른 학습시간\n","  * learning rate 기존의 10배로 학습 진행\n"," \n","    -> STAGE 1 : 94epochs, 약 12시간 소요 / 88epoch 이후로 loss 값에 NAN 값만 출력 / 높은 Learning rate가 문제인 것으로 보임.\n","  *  Batch normalization X + RMSprop optimzer 사용\n","  \n","    -> STAGE 1 : 150epochs, 약 7시간 소요 / 150epoch 이후 런타임 끊김 현상. \n","  * Batch normalization X + RMSprop optimizer + Wassertein Loss 변경\n"," \n","    -> STAGE 1 : 180epochs, 약 4시간 소요 / 180epoch에서 세션 끊김 현상 지속적 발생\n","\n","  *  Batch normalization O + Adam optimizer 사용  \n","    -> STAGE 1 : case1 = 138epochs, 약11시간 소요 / case2 = 600epochs, 약 8시간 소요        \n","     \n","  * Batch normalization O + Adam optimizer + Wassertein Loss 변경 + Dropout 추가\n"," \n","    -> STAGE 1 : 600epochs, 약 4-5시간 소요\n"," \n","    -> STAGE 2 : 8epochs, 24시간 소요"]},{"cell_type":"markdown","metadata":{"id":"VautTs7Qxhlu","colab_type":"text"},"source":["* 학습 시간이 너무 오래 걸려 learning rate를 0.002도 작은 learning rate라고 생각해서 0.0002에서 0.002로 높여봤음. \n","  * 이는 학습에 별로 도움이 되지 못하고 오히려 loss값에 nan값을 띄게 됨. loss가 증가하다가 무한대로 수렴했기 때문일 것.\n","\n","   ![대체 텍스트](https://i.ibb.co/P6hJ0jK/nan.png)\n","  * 출처 : https://stackoverflow.com/questions/52211665/why-do-i-get-nan-loss-value-in-training-discriminator-and-generator-of-gan\n","\n","  "]},{"cell_type":"markdown","metadata":{"id":"ZjqcnFr-F_yx","colab_type":"text"},"source":["* GAN 변형 모델인 StackGAN의 성능을 높이기 위해서도 Wasserstein loss가 효과가 있었음.\n"," 다른 StyleGAN이나 CycleGAN에도 Wasserstein loss로 학습하면 더 좋은 학습이 가능해 질 수도 있다고 생각."]},{"cell_type":"markdown","metadata":{"id":"jfAyF2zTDdS8","colab_type":"text"},"source":["* inception score 측정\n","\n","  ![대체 텍스트](https://i.ibb.co/1Lb0dMj/inception-1.png)\n","  * Stage1밖에 학습을 진행하지 못했기 때문에 기존의 StackGAN의 Inception score에 달하는 score 얻지 못함.\n","  * data를 뜯어서 봐보니 GAN이 생성한 이미지가 주변에 흰색 배경의 패딩이 있었음.\n","\n","  ![대체 텍스트](https://i.ibb.co/4Rxm13S/inception-3.png)\n","  * 이를 해결하면 더 좋은 inception score를 얻을 수 있다고 생각함.\n","\n","  ![대체 텍스트](https://i.ibb.co/yBVR6Y9/inception-2.png)\n","  * 겉의 흰 배경을 제거했더니 대체적으로 더 좋은 score를 받는 것 확인."]},{"cell_type":"markdown","metadata":{"id":"q62fxiMNu_p-","colab_type":"text"},"source":["# Insights and discussions relevant to the project"]},{"cell_type":"markdown","metadata":{"id":"_2NlZ3NvxjTE","colab_type":"text"},"source":["* GAN은 학습이 굉장히 어려움\n","  * 학습이 잘 되기 위해서는 서로 비슷한 수준의 생성자와 구분자가 함께 조금씩 발전해야 힘 한쪽이 너무 급격하게 강력해지면 이 관계가 깨져서 학습이 이루어지지 않음\n","\n","\n","*   DCGAN, WGAN,EBGAN, BEGAN,CycleGAN, DiscoGAN 등 성능향상 및 모델 안정화 위해 다양한 모델 출현\n","  *   출처 : https://dreamgonfly.github.io/2018/03/17/gan-explained.html\n","\n","\n","* GAN은 상당히 오랜 시간 학습 필요, 장시간 학습 중 런타임 연결 끊김 계속 발생\n","  * 개발자 도구(F12)에서 console에 명령어 입력으로 해결\n","  * ```\n"," function ClickConnect() { \n","   var buttons = document.querySelectorAll(\"colab-dialog.yes-no-dialog paper-button#cancel\"); \n","   buttons.forEach(function(btn) { btn.click(); }); \n","   console.log(\"1분마다 자동 재연결\"); \n","   document.querySelector(\"#top-toolbar > colab-connect-button\").click();\n","    } \n","   setInterval(ClickConnect,1000*60);\n","```\n","  * 출처: https://bryan7.tistory.com/1077 [민서네집]\n","\n","* GAN관련 모델을 학습시키고 싶으시다면,,, GTX 1080Ti graphic card 추천..!\n","\n","   ![대체 텍스트](https://i.ibb.co/BVGjN0V/gtx1080.pnghttps://)\n","   * 출처 : https://stackoverflow.com/questions/58595157/colab-gpu-vs-gtx-1080\n","\n","* github 코드들 활용하려 했지만, 다른 python, tensorflow 버전에서 작성되어있어서 가상환경에서 다운그레이드를 해서 다시 시도.\n"," *  그래도 원하는대로 잘 안됐음.. 버전 상 호환 안되는 부분이 많기 때문인 듯. 클론 해보고 싶은 코드가 있다면 먼저 라이브러리 버전 확인을 해보시기를..!\n"]},{"cell_type":"markdown","metadata":{"id":"vznFCbhEkVWF","colab_type":"text"},"source":["# References"]},{"cell_type":"markdown","metadata":{"id":"a-9TQBSNvFVT","colab_type":"text"},"source":["* 웹사이트\n","  * StackGAN 전반적인 구조 및 구현 메인 코드\n","   - https://medium.com/@mrgarg.rajat/implementing-stackgan-using-keras-a0a1b381125e\n","\n","  * StackGAN Inception score 산출\n","   - https://github.com/hanzhanggit/StackGAN-inception-model\n","   - https://machinelearningmastery.com/how-to-implement-the-inception-score-from-scratch-for-evaluating-generated-images/\n","\n","  * StackGAN 구조 이해\n","   - https://www.youtube.com/watch?v=G2_8Jc0IwYk\n","\n","\n","* 논문\n","\n","  * “Generative Adversarial Network”(2014,Ian J. Goodfellow외) \n","\n","  * “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks” (2016, Alec Radford외)\n","\n","  * “StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks”(2017,Han Zhang 외)\n","\n","  * \"Stacked Generative Adversarial Networks\"(Xun Huang et al)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9tvd9lSSwda6","colab_type":"text"},"source":["# Member's constribution statement"]},{"cell_type":"markdown","metadata":{"id":"iTFIf3KFOl0K","colab_type":"text"},"source":["* 권성수 : 기여도 (100%) 기여내용 : Stage2 설계 / <3,6>모델 실행\n","* 문대정 : 기여도 (100%) 기여내용 : Stage1 설계 / <2,3>모델 실행 / 디버깅 /  inception score 측정 / 발표파일 작성/ 발표 \n","* 이재철 : 기여도 (100%) 기여내용 : Stage2 설계 / <4,5>모델 실행 / 디버깅 /데이터셋 구조 조사\n","* 조은경 : 기여도 (100%) 기여내용 : Stage1 설계 /<1,4,5>모델 실행 / 디버깅 / 모델변경 방향 조사,제시/ 발표파일 작성\n","\n","\n","1. Batch normalization X + Adam optimizer 사용\n","2. Batch normalization X + RMSprop optimzer 사용\n","3. Batch normalization X + RMSprop optimizer + Wassertein Loss 변경\n","4. Batch normalization O + Adam optimizer 사용 \n","5. Batch normalization O + Adam optimizer + Wassertein Loss 변경 + Dropout 추가\n","6. Learning rate 기존의 10배로 학습 진행 \n"]}]}
