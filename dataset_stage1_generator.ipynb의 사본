{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataset/stage1_generator.ipynb의 사본","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"0KV9OIEe9DXn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":126},"executionInfo":{"status":"ok","timestamp":1592381898801,"user_tz":-540,"elapsed":61750,"user":{"displayName":"권성수","photoUrl":"","userId":"16677952698498715458"}},"outputId":"91d3c001-19d7-49ab-bc93-c101a3cf22a4"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4aUhv6uv91aB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":389},"executionInfo":{"status":"error","timestamp":1592382107377,"user_tz":-540,"elapsed":762,"user":{"displayName":"권성수","photoUrl":"","userId":"16677952698498715458"}},"outputId":"4e358ec5-18bb-4add-a65a-54ceb9c1430f"},"source":["import os\n","import tarfile\n","\n","fname = '/content/gdrive/My Drive/dl_teamproject_folder/CUB_200_2011/CUB_200_2011.tgz'  # 압축 파일을 지정해주고   \n","ap = tarfile.open(fname)      # 열어줍니다. \n","\n","ap.extractall('/content/drive/My Drive/dl_teamproject_folder/CUB_200_2011/')         # 그리고는 압축을 풀어줍니다. \n","# () 안에는 풀고 싶은 경로를 넣어주면 되요. 비워둘 경우 현재 경로에 압축 풉니다. \n"," \n","ap.close()  \n","\n","  "],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-6c574be24c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/Shared with me/dl_teamproject_folder/CUB_200_2011/CUB_200_2011.tgz'\u001b[0m  \u001b[0;31m# 압축 파일을 지정해주고\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# 열어줍니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/dl_teamproject_folder/CUB_200_2011/'\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# 그리고는 압축을 풀어줍니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1569\u001b[0m                     \u001b[0msaved_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mReadError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompressionError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mgzopen\u001b[0;34m(cls, name, mode, fileobj, compresslevel, **kwargs)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1636\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1637\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/Shared with me/dl_teamproject_folder/CUB_200_2011/CUB_200_2011.tgz'"]}]},{"cell_type":"markdown","metadata":{"id":"KIegGWV5aON0","colab_type":"text"},"source":["# Importing Libraries"]},{"cell_type":"code","metadata":{"id":"0TjLKTwFcHZ1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592308343097,"user_tz":-540,"elapsed":3501,"user":{"displayName":"문대정","photoUrl":"","userId":"12686300660117063511"}},"outputId":"80b0977b-9b97-415a-df28-c7a10eecc6b1"},"source":["import os\n","import pickle\n","import random\n","import time\n","\n","import PIL\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from PIL import Image\n","from keras import Input, Model\n","from keras import backend as K\n","from keras.callbacks import TensorBoard\n","from keras.layers import Dense, LeakyReLU, BatchNormalization, ReLU, Reshape, UpSampling2D, Conv2D, Activation, \\\n","    concatenate, Flatten, Lambda, Concatenate\n","from keras.optimizers import Adam\n","from matplotlib import pyplot as plt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"7pycIMXgaTtW","colab_type":"text"},"source":["# Loading of Dataset"]},{"cell_type":"code","metadata":{"id":"PHs4UNm2eTWA","colab_type":"code","colab":{}},"source":["#pickle(텍스트가 아닌 객체 자체인 파일 ex.list) 불러오기 위한 함수 (labels저장된 파일)\n","def load_class_ids(class_info_file_path):\n","    with open(class_info_file_path, 'rb') as f:\n","        class_ids = pickle.load(f, encoding='latin1')\n","        return class_ids\n","       \n","#임베딩     \n","def load_embeddings(embeddings_file_path):\n","    with open(embeddings_file_path, 'rb') as f:\n","        embeddings = pickle.load(f, encoding='latin1')\n","        embeddings = np.array(embeddings)\n","        print('embeddings: ', embeddings.shape)\n","    return embeddings\n","\n","#pickle 파일이름 불러오기\n","def load_filenames(filenames_file_path):\n","    with open(filenames_file_path, 'rb') as f:\n","        filenames = pickle.load(f, encoding='latin1')\n","    return filenames\n","\n","#image detection위한 bounding box(바운딩 박스와 일치하는 파일의 dictionary 불러오기) \n","def load_bounding_boxes(dataset_dir):\n","    # Paths\n","    bounding_boxes_path = os.path.join(dataset_dir, 'bounding_boxes.txt')\n","    file_paths_path = os.path.join(dataset_dir, 'images.txt')\n","\n","    # Read bounding_boxes.txt and images.txt file\n","    df_bounding_boxes = pd.read_csv(bounding_boxes_path,\n","                                    delim_whitespace=True, header=None).astype(int) #delim_whitespace : 공백으로 구분된 값 파일 읽기\n","    df_file_names = pd.read_csv(file_paths_path, delim_whitespace=True, header=None)\n","\n","    # Create a list of file names\n","    file_names = df_file_names[1].tolist()\n","\n","    # Create a dictionary of file_names and bounding boxes\n","    filename_boundingbox_dict = {img_file[:-4]: [] for img_file in file_names[:2]}\n","\n","    # Assign a bounding box to the corresponding image\n","    for i in range(0, len(file_names)):\n","        # Get the bounding box\n","        bounding_box = df_bounding_boxes.iloc[i][1:].tolist()\n","        key = file_names[i][:-4]\n","        filename_boundingbox_dict[key] = bounding_box\n","\n","    return filename_boundingbox_dict\n","\n","#image 바운딩 박스로 자르고, 주어진 사이즈로 이미지 resize\n","def get_img(img_path, bbox, image_size):\n","    img = Image.open(img_path).convert('RGB')\n","    width, height = img.size\n","    if bbox is not None:\n","        R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n","        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n","        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n","        y1 = np.maximum(0, center_y - R)\n","        y2 = np.minimum(height, center_y + R)\n","        x1 = np.maximum(0, center_x - R)\n","        x2 = np.minimum(width, center_x + R)\n","        img = img.crop([x1, y1, x2, y2])\n","    img = img.resize(image_size, PIL.Image.BILINEAR)\n","    return img\n"," \n"," #트레이닝하기 위한 데이터셋 로드 : image, labels, 일치하는 embedding return\n","def load_dataset(filenames_file_path, class_info_file_path, cub_dataset_dir, embeddings_file_path, image_size):\n","    filenames = load_filenames(filenames_file_path)\n","    class_ids = load_class_ids(class_info_file_path)\n","    bounding_boxes = load_bounding_boxes(cub_dataset_dir)\n","    all_embeddings = load_embeddings(embeddings_file_path)\n","\n","    X, y, embeddings = [], [], []\n","    print(\"Embeddings shape:\", all_embeddings.shape)\n","\n","    for index, filename in enumerate(filenames):\n","        bounding_box = bounding_boxes[filename]\n","        try:\n","            img_name = '{}/images/{}.jpg'.format(cub_dataset_dir, filename)\n","            img = get_img(img_name, bounding_box, image_size)\n","\n","            all_embeddings1 = all_embeddings[index, :, :]\n","\n","            embedding_ix = random.randint(0, all_embeddings1.shape[0] - 1)#0과 임베딩크기 사이 정수 랜덤반환\n","            embedding = all_embeddings1[embedding_ix, :]\n","\n","            X.append(np.array(img))\n","            y.append(class_ids[index])\n","            embeddings.append(embedding)\n","        except Exception as e:\n","            print(e)\n","\n","    X = np.array(X)\n","    y = np.array(y)\n","    embeddings = np.array(embeddings)\n","    return X, y, embeddings"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SHG799svaZA9","colab_type":"text"},"source":["#  Model Creation"]},{"cell_type":"code","metadata":{"id":"Fnamu5fjeXv8","colab_type":"code","colab":{}},"source":["def generate_c(x):\n","    mean = x[:, :128] #(batch,128)dims의 tensor생성\n","    log_sigma = x[:, 128:]\n","    stddev = K.exp(log_sigma) #from keras import backend as K | exp = exponential\n","    epsilon = K.random_normal(shape=K.constant((mean.shape[1],), dtype='int32'))  # random normal vector with mean=0 and std=1.0\n","    c = stddev * epsilon + mean #text conditioning variable 계산 | 모델 아키텍쳐 그림 중에서 c0 햇 부분\n","    return c\n","\n","#conditioning augmentation: text embedding vector를 conditioning latent variables로 변환  \n","def build_ca_model():\n","    input_layer = Input(shape=(1024,))\n","    x = Dense(256)(input_layer)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    model = Model(inputs=[input_layer], outputs=[x])\n","    return model  # Takes an embedding of shape (1024,) and returns a tensor of shape (256,)\n","  \n","def build_embedding_compressor_model():\n","    input_layer = Input(shape=(1024,))\n","    x = Dense(128)(input_layer)\n","    x = ReLU()(x)\n","    model = Model(inputs=[input_layer], outputs=[x])\n","    return model\n","\n","\n","def build_stage1_generator():\n","    input_layer = Input(shape=(1024,)) #noise variable\n","    x = Dense(256)(input_layer)\n","    mean_logsigma = LeakyReLU(alpha=0.2)(x)\n","\n","    c = Lambda(generate_c)(mean_logsigma)\n","\n","    input_layer2 = Input(shape=(100,))\n","\n","    gen_input = Concatenate(axis=1)([c, input_layer2]) #text-conditioning variable/noise variable\n","\n","    x = Dense(128 * 8 * 4 * 4, use_bias=False)(gen_input)\n","    x = ReLU()(x)\n","\n","    x = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(x) #2d tensor->4d tensor로 변환\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x) #bn 사용 - > bias=False\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = Activation(activation='tanh')(x) #저해상도 이미지 생성할 generator\n","\n","    stage1_gen = Model(inputs=[input_layer, input_layer2], outputs=[x, mean_logsigma])\n","    stage1_gen.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iHvK2VzuebDk","colab_type":"code","colab":{}},"source":["# 이미지 저장\n","def save_rgb_img(img, path):\n","    fig = plt.figure()\n","    ax = fig.add_subplot(1, 1, 1)\n","    ax.imshow(img)\n","    ax.axis(\"off\")\n","    ax.set_title(\"Image\")\n","\n","    plt.savefig(path)\n","    plt.close()\n","\n","#텐서보드에 summary기록   \n","def write_log(callback, names, logs, batch_no):\n","    for name, value in zip(names, logs):\n","        summary = tf.Summary()\n","        summary_value = summary.value.add()\n","        summary_value.simple_value = value\n","        summary_value.tag = name\n","        callback.writer.add_summary(summary, batch_no)\n","        callback.writer.flush()    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X6wFBKBgefMD","colab_type":"code","colab":{}},"source":["def build_stage1_discriminator():\n","    \"\"\"\n","    discriminator는 모델 아키텍쳐 그림에서처럼 2개의 input을 받는다\n","    1. generator거쳐서 upsampling된 네트워크를 다시 downsampling해서 만든 3차원의 4x4x512의 네트워크 / 를 fully connected layer에 연결한 총 8193개의 뉴런들\n","    2. embedding layer를 \n","    3. Concatenate along the axis dimension and feed it to the last module which produces final logits\n","    \"\"\"\n","    input_layer = Input(shape=(64, 64, 3))\n","\n","    x = Conv2D(64, (4, 4),\n","               padding='same', strides=2,\n","               input_shape=(64, 64, 3), use_bias=False)(input_layer)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    input_layer2 = Input(shape=(4, 4, 128))\n","\n","    merged_input = concatenate([x, input_layer2])\n","\n","    x2 = Conv2D(64 * 8, kernel_size=1,\n","                padding=\"same\", strides=1)(merged_input)\n","    x2 = BatchNormalization()(x2)\n","    x2 = LeakyReLU(alpha=0.2)(x2)\n","    x2 = Flatten()(x2)\n","    x2 = Dense(1)(x2)\n","    x2 = Activation('sigmoid')(x2)\n","\n","    stage1_dis = Model(inputs=[input_layer, input_layer2], outputs=[x2])\n","    return stage1_dis"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3JUeRDH4RXfN","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}